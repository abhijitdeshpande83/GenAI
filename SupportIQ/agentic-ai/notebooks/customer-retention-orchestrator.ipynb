{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6d275839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "from typing import Annotated, TypedDict, List, Sequence\n",
    "from langchain_groq import ChatGroq \n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import BaseMessage,HumanMessage,SystemMessage,AIMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.prebuilt import InjectedState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0357fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b887cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "1cbc24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisorState(MessagesState):\n",
    "    \"\"\"\n",
    "        State for multi-agent system\n",
    "    \"\"\"   \n",
    "    next_agent: str\n",
    "    user_input: str\n",
    "    user_intent: str\n",
    "    complaint: List[dict]\n",
    "    question:  List[dict]\n",
    "\n",
    "# Supervisor Node\n",
    "\n",
    "def supervisor_node(state:SupervisorState)->SupervisorState:\n",
    " \n",
    "    system_prompt = \"\"\"\n",
    "    You are a supervisor. Classify the user query into one of: inquiry, complaint, retention.\n",
    "\n",
    "    Rules:\n",
    "    - If the user mentions cancelling, switching, leaving, or expresses strong frustration about continuing the service, classify as retention.\n",
    "    - If the user complains but does not express intent to leave, classify as complaint.\n",
    "    - If the user is asking for info or guidance, classify as inquiry.\n",
    "\n",
    "    Examples:\n",
    "    - \"My laptop stopped working\" -> complaint\n",
    "    - \"Show me more info on my TCL TV\" -> inquiry\n",
    "    - \"I want to cancel my Netflix subscription\" -> retention\n",
    "    - \"My smartphone battery is not charging\" -> complaint\n",
    "    - \"I want to know the specs of my AirPods\" -> inquiry\n",
    "    - \"I want to cancel my gym membership\" -> retention\n",
    "    - \"My last order was delayed and I am frustrated, considering switching\" -> retention\n",
    "    - \"The app crashes often, this is annoying\" -> complaint\n",
    "    - \"Where can I see my last invoice?\" -> inquiry\n",
    "\n",
    "    Return **only** the intent.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        {\"role\":\"system\", \"content\":system_prompt},\n",
    "        {\"role\":\"user\", \"content\":state[\"user_input\"]}\n",
    "        ])\n",
    "    \n",
    "    intent=response.content.strip().lower()\n",
    "    print(\"Intent is:\",intent)\n",
    "    if intent==\"inquiry\":\n",
    "        return Command(goto=\"Inquiry Agent\")\n",
    "    elif intent==\"complaint\":\n",
    "        return Command(goto=\"Complaint Agent\")\n",
    "    elif intent==\"retention\":\n",
    "        return Command(goto=\"Retention Agent\")\n",
    "    else:\n",
    "        return Command(goto=\"Fallback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "7c2eb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker nodes\n",
    "\n",
    "def inquiry_node(state:SupervisorState)->SupervisorState:\n",
    "    return {'messages':[f\"Inquiry Handled: {state['user_input']}\"]}\n",
    "\n",
    "def fallback(state:SupervisorState)->SupervisorState:\n",
    "    return {'messages':[f\"Sorry, I couldn't understand your message\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "f8413a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def nlu(user_input:str)->dict:\n",
    "    \"\"\"It extracts the entities from the user input\"\"\"\n",
    "\n",
    "    system_prompt=\"\"\"\n",
    "    Extract the following from the customer message:\n",
    "    - product\n",
    "    - issue_type\n",
    "    - purchase_date\n",
    "\n",
    "    Respond ONLY with a valid JSON object in this exact format:\n",
    "    {\n",
    "        \"product\": string or null,\n",
    "        \"issue_type\": string or null,\n",
    "        \"purchase_date\": string or null\n",
    "    }\"\"\"\n",
    "    response = llm.invoke([\n",
    "        {'role':'system','content':system_prompt},\n",
    "        {'role':'user','content':user_input}\n",
    "    ])\n",
    "    complaint = json.loads(response.content) \n",
    "    return {'complaint':complaint}\n",
    "\n",
    "@tool\n",
    "def ask_missing_info(missing_values:List[str])->dict:\n",
    "    \"\"\"\n",
    "    It will ask the missing information to user to get the \n",
    "    remaining details missing from the user input\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt=f\"\"\"You are a helpful assistant.\n",
    "        Ask the user for the missing information in a clear and concise way, \n",
    "        one piece at a time, so we can complete the complaint record.\n",
    "        Missing values are: {missing_values}\n",
    "        \"\"\"\n",
    "    response = llm.invoke([{'role':'system','content':system_prompt}])\n",
    "    return {'question':response.content}\n",
    "\n",
    "@tool\n",
    "def create_ticket(complaint: dict)->dict:\n",
    "    \"\"\"\n",
    "    Tool to create a complaint ticket.\n",
    "    Generates a ticket ID like IG408C90.\n",
    "    \"\"\"\n",
    "    prefix = ''.join(random.choices(string.ascii_uppercase,k=2))\n",
    "    number1 = random.randint(100,1000)\n",
    "    mid = ''.join(random.choices(string.ascii_uppercase))\n",
    "    number2 = random.randint(10,100)\n",
    "    ticket_id = f\"{prefix}{number1}{mid}{number2}\"\n",
    "\n",
    "    ticket={\n",
    "        \"ticket_id\":ticket_id,\n",
    "        \"status\":\"created\",\n",
    "        \"details\":complaint\n",
    "    }\n",
    "    \n",
    "    return ticket\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "fc104327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complaint_node(state:SupervisorState)->SupervisorState:\n",
    "   \"\"\"\n",
    "      This node handles complaint and helps to resolve complaint.  \n",
    "   \"\"\"\n",
    "   # system_prompt = \"\"\"\n",
    "   # You are an assistant. Use nlu tool to extract product, issue_type, purchase_date\n",
    "   # from customer messages whenever needed.\n",
    "   # \"\"\" \n",
    "   # llm_tool = llm.bind_tools([nlu])\n",
    "   # response = llm_tool.invoke([{'role':'system','content':system_prompt},\n",
    "   #                            {'role':'user','content':state['user_input']}])\n",
    "\n",
    "   \n",
    "   extracted=nlu(state['user_input'])\n",
    "   print(extracted)\n",
    "   missing_values=[k for k,v in extracted['complaint'].items() if v is None]\n",
    "   print(\"Missing Values\", missing_values)\n",
    "\n",
    "   question=ask_missing_info({'missing_values':missing_values})\n",
    "   print(question)\n",
    "   state['question']=question\n",
    "\n",
    "   print(\"creating ticket\")\n",
    "   ticket=create_ticket(extracted)\n",
    "   print(ticket)\n",
    "\n",
    "   return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "1ca3c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(SupervisorState)\n",
    "graph.add_node(\"Supervisor\",supervisor_node)\n",
    "graph.add_node(\"Inquiry Agent\",inquiry_node)\n",
    "graph.add_node(\"Complaint Agent\",complaint_node)\n",
    "graph.add_node(\"Retention Agent\",retention_node)\n",
    "graph.add_node(\"Fallback\", fallback)\n",
    "\n",
    "graph.add_edge(START, \"Supervisor\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "20918bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 22:44:27,403 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent is: retention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 22:44:27,811 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-09-30 22:44:28,426 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n",
      "\"I'm so sorry to hear that our app hasn't met your expectations. I can imagine how frustrating that must be for you. Before you go, I want to thank you for giving us a try and for being a valued customer. As a small gesture of appreciation, I'd like to offer you a 25% discount on your next month's subscription. If you're willing, I'd love to help you get the most out of our service before you leave. What's the main issue you're experiencing, and how can we make it right?\"\n"
     ]
    }
   ],
   "source": [
    "response = app.invoke({\"user_input\":\"I want to cancel the service, this is worst app ever I have used.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76adc2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "e097527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def churn_score(user_input:str)->int:\n",
    "    \"\"\"\n",
    "        Calculate a churn risk score from user input.\n",
    "        Returns category: high, medium, or low.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\" \n",
    "    You are a churn detection assistant.\n",
    "    Based on the user input, classifiy their churn risk into:\n",
    "    - high: user explicitly wants to cancel, switch, or sounds very frustrated.\n",
    "    - medium: user shows dissatisfaction but hasn't decided to cancel yet.\n",
    "    - low: user just asking questions or mild complaints.\n",
    "\n",
    "    Examples:\n",
    "    User: \"I'm cancelling this useless service today.\"\n",
    "    Churn risk: high\n",
    "\n",
    "    User: \"Your prices keep going up, I don't know if it's worth it anymore.\"\n",
    "    Churn risk: medium\n",
    "\n",
    "    User: \"How do I cancel if I ever need to in the future?\"\n",
    "    Churn risk: low\n",
    "\n",
    "    Respond with only one of: high, medium, low.\n",
    "    \"\"\"\n",
    "    score = llm.invoke([\n",
    "        {'role':'system', 'content':system_prompt},\n",
    "        {'role':'user', 'content':user_input}\n",
    "    ]).content\n",
    "\n",
    "    return str(score)\n",
    "\n",
    "@tool\n",
    "def loyalty_score(score:str)->str:\n",
    "    \"\"\" \n",
    "        Provide rewards to user based on churn score. \n",
    "    \"\"\"\n",
    "    reward_weights = {'high': 1.0, \"medium\": 0.6, \"low\": 0.2}\n",
    "    clv_values = {\"high\": 1000, \"medium\": 500, \"low\": 200}\n",
    "    clv_tier=random.choices(\n",
    "        ['high','medium','low'],\n",
    "        weights=[0.25,0.45,0.3],\n",
    "        k=1\n",
    "    )[0]\n",
    "\n",
    "    reward_score = reward_weights[score]*clv_values[clv_tier]\n",
    "\n",
    "    return reward_score\n",
    "\n",
    "@tool\n",
    "def loyalty_rewards(user_input:str, reward_score:float, churn_score:str)->str:\n",
    "    \"\"\"\n",
    "    Generate a personalized retention message.\n",
    "    \"\"\"\n",
    "    system_prompt=\"\"\" \n",
    "    You are a customer retention assistant. \n",
    "    Generate a polite, empathetic message based on the following:\n",
    "    - The user's message\n",
    "    - Their churn risk (high, medium, low)\n",
    "    - Their loyalty score (40-1000)\n",
    "\n",
    "    Rules:\n",
    "    - If loyalty score >= 800 → emphasize strong appreciation and give a high reward (e.g., big discount, free premium month).\n",
    "    - If 500-799 → show gratitude and offer a medium reward (e.g., discount or perk).\n",
    "    - If 200-499 → acknowledge their value and give a small reward (e.g., loyalty points or small discount).\n",
    "    - If < 200 → do not give a reward, just apologize and promise to improve.\n",
    "    - If churn risk = high → always start by apologizing and showing empathy before mentioning any reward.\n",
    "    - Keep the message short, friendly, concise and natural. Do not include technical terms or scores.\n",
    "    \n",
    "    Respond with only the final concise message.\n",
    "    \"\"\"\n",
    "\n",
    "    user_context = f\"\"\"\n",
    "    User message: {user_input}\n",
    "    Churn risk: {churn_score}\n",
    "    Loyalty score: {reward_score}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([{'role':'system', 'content':system_prompt},\n",
    "                           {'role':'user','content':user_context}]).content\n",
    "    \n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "4590802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retention_node(state:SupervisorState)->SupervisorState:\n",
    "    score = churn_score(state['user_input'])\n",
    "    rewards = loyalty_score(score)\n",
    "    loyalty=loyalty_rewards({'user_input':state['user_input'],'reward_score':rewards, 'churn_score':score})\n",
    "    print(rewards)\n",
    "    print(loyalty)\n",
    "    return {'messages':[f\"Retention Handled: {state['user_input']}\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acc0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddab765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80734c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7034adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854d2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdd5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f808c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adff76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e54004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dcb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe763e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
