{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6d275839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "from typing import Annotated, TypedDict, List, Sequence, Optional\n",
    "from langchain_groq import ChatGroq \n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import BaseMessage,HumanMessage,SystemMessage,AIMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode, InjectedState\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0357fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b887cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1cbc24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisorState(MessagesState):\n",
    "    \"\"\"\n",
    "        State for multi-agent system\n",
    "    \"\"\"   \n",
    "    next_agent: str\n",
    "    user_input: str\n",
    "    user_intent: str\n",
    "    complaint: str\n",
    "    missing_info:  List[dict]\n",
    "    complaint_in_progress: bool = False\n",
    "\n",
    "# Supervisor Node\n",
    "\n",
    "def supervisor_node(state:SupervisorState)->SupervisorState:\n",
    "\n",
    "    if state.get('complaint_in_progress'):\n",
    "        return Command(goto=\"Complaint Agent\")\n",
    " \n",
    "    system_prompt = \"\"\"\n",
    "    You are a supervisor. Classify the user query into one of: inquiry, complaint, retention.\n",
    "\n",
    "    Rules:\n",
    "    - If the user mentions cancelling, switching, leaving, or expresses strong frustration about continuing the service, classify as retention.\n",
    "    - If the user complains but does not express intent to leave, classify as complaint.\n",
    "    - If the user is asking for info or guidance, classify as inquiry.\n",
    "\n",
    "    Examples:\n",
    "    - \"My laptop stopped working\" -> complaint\n",
    "    - \"Show me more info on my TCL TV\" -> inquiry\n",
    "    - \"I want to cancel my Netflix subscription\" -> retention\n",
    "    - \"My smartphone battery is not charging\" -> complaint\n",
    "    - \"I want to know the specs of my AirPods\" -> inquiry\n",
    "    - \"I want to cancel my gym membership\" -> retention\n",
    "    - \"My last order was delayed and I am frustrated, considering switching\" -> retention\n",
    "    - \"The app crashes often, this is annoying\" -> complaint\n",
    "    - \"Where can I see my last invoice?\" -> inquiry\n",
    "\n",
    "    Return **only** the intent.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([\n",
    "        {\"role\":\"system\", \"content\":system_prompt},\n",
    "        {\"role\":\"user\", \"content\":state[\"user_input\"]}\n",
    "        ])\n",
    "    \n",
    "    intent=response.content.strip().lower()\n",
    "    state['user_intent']=intent\n",
    "\n",
    "    if intent==\"inquiry\":\n",
    "        return Command(goto=\"Inquiry Agent\")\n",
    "    elif intent==\"complaint\":\n",
    "        return Command(goto=\"Complaint Agent\")\n",
    "    elif intent==\"retention\":\n",
    "        return Command(goto=\"Retention Agent\")\n",
    "    else:\n",
    "        return Command(goto=\"Fallback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7c2eb31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worker nodes\n",
    "\n",
    "def inquiry_node(state:SupervisorState)->SupervisorState:\n",
    "    return {'messages':[f\"Inquiry Handled: {state['user_input']}\"]}\n",
    "\n",
    "def fallback(state:SupervisorState)->SupervisorState:\n",
    "    return {'messages':[f\"Sorry, I couldn't understand your message\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f8413a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def nlu(user_input:str, complaint:Optional[str]=\"\")->dict:\n",
    "    \"\"\"It extracts the entities from the user input\"\"\"\n",
    "\n",
    "    # complaint=state.complaint if state.complaint else ''\n",
    "    system_prompt=f\"\"\"\n",
    "    Extract the following from the user message:\n",
    "    - product\n",
    "    - issue_type\n",
    "    - purchase_date\n",
    "    For you information: {complaint} already been captured.\n",
    "    Respond ONLY with a valid JSON object in this exact format:\n",
    "    {{\n",
    "        \"product\": string or null,\n",
    "        \"issue_type\": string or null,\n",
    "        \"purchase_date\": string or null\n",
    "    }}\n",
    "    \"\"\"\n",
    "    response = llm.invoke([\n",
    "        {'role':'system','content':system_prompt},\n",
    "        {'role':'user','content':user_input}\n",
    "    ])\n",
    "    complaint = json.loads(response.content) \n",
    "    return {'complaint':complaint}\n",
    "\n",
    "@tool\n",
    "def ask_missing_info(missing_value:str)->dict:\n",
    "    \"\"\"\n",
    "    It will ask the missing information to user to get the \n",
    "    remaining details missing from the user input\n",
    "    \"\"\"\n",
    "   \n",
    "    question = f\"Could you please provide {missing_value}.\"\n",
    "\n",
    "    return {\"missing_info\":question}\n",
    "\n",
    "@tool\n",
    "def create_ticket(complaint: dict)->dict:\n",
    "    \"\"\"\n",
    "    Tool to create a complaint ticket.\n",
    "    Generates a ticket ID like IG408C90.\n",
    "    \"\"\"\n",
    "    prefix = ''.join(random.choices(string.ascii_uppercase,k=2))\n",
    "    number1 = random.randint(100,1000)\n",
    "    mid = ''.join(random.choices(string.ascii_uppercase))\n",
    "    number2 = random.randint(10,100)\n",
    "    ticket_id = f\"{prefix}{number1}{mid}{number2}\"\n",
    "\n",
    "    ticket={\n",
    "        \"ticket_id\":ticket_id,\n",
    "        \"status\":\"created\",\n",
    "        \"details\":complaint\n",
    "    }\n",
    "    \n",
    "    return ticket\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc104327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complaint_node(state:SupervisorState)->SupervisorState:\n",
    "   \"\"\"\n",
    "      This node handles complaint and helps to resolve complaint.  \n",
    "   \"\"\"\n",
    "   extracted=nlu({'user_input':state['user_input']})\n",
    "   print(extracted)\n",
    "   missing_values=[k for k,v in extracted['complaint'].items() if v is None]\n",
    "   print(\"Missing Values\", missing_values)\n",
    "   for missing_value in missing_values:\n",
    "      missing_info = ask_missing_info({'missing_value':missing_value})\n",
    "      \n",
    "      print(missing_info)\n",
    "   state['missing_info']=missing_info\n",
    "   state['complaint_in_progress']=True\n",
    "\n",
    "   print(\"creating ticket\")\n",
    "   ticket=create_ticket(extracted)\n",
    "   print(ticket)\n",
    "\n",
    "   return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1ca3c58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(SupervisorState)\n",
    "graph.add_node(\"Supervisor\",supervisor_node)\n",
    "graph.add_node(\"Inquiry Agent\",inquiry_node)\n",
    "graph.add_node(\"Complaint Agent\",complaint_node)\n",
    "graph.add_node(\"Retention Agent\",retention_node)\n",
    "graph.add_node(\"Fallback\", fallback)\n",
    "\n",
    "graph.add_edge(START, \"Supervisor\")\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "20918bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'complaint': {'product': 'mouse', 'issue_type': 'not working', 'purchase_date': None}}\n",
      "Missing Values ['purchase_date']\n",
      "{'missing_info': 'Could you please provide purchase_date.'}\n",
      "creating ticket\n",
      "{'ticket_id': 'UY682C83', 'status': 'created', 'details': {'product': 'mouse', 'issue_type': 'not working', 'purchase_date': None}}\n"
     ]
    }
   ],
   "source": [
    "response = app.invoke({\"user_input\":\"My mouse is not working.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70f14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76adc2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Inquiry Handled: Today.', additional_kwargs={}, response_metadata={}, id='c78f8213-7881-4c20-a510-39f35e6cd737')],\n",
       " 'user_input': 'Today.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = app.invoke({\"user_input\":\"Today.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e097527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def churn_score(user_input:str)->int:\n",
    "    \"\"\"\n",
    "        Calculate a churn risk score from user input.\n",
    "        Returns category: high, medium, or low.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = \"\"\" \n",
    "    You are a churn detection assistant.\n",
    "    Based on the user input, classifiy their churn risk into:\n",
    "    - high: user explicitly wants to cancel, switch, or sounds very frustrated.\n",
    "    - medium: user shows dissatisfaction but hasn't decided to cancel yet.\n",
    "    - low: user just asking questions or mild complaints.\n",
    "\n",
    "    Examples:\n",
    "    User: \"I'm cancelling this useless service today.\"\n",
    "    Churn risk: high\n",
    "\n",
    "    User: \"Your prices keep going up, I don't know if it's worth it anymore.\"\n",
    "    Churn risk: medium\n",
    "\n",
    "    User: \"How do I cancel if I ever need to in the future?\"\n",
    "    Churn risk: low\n",
    "\n",
    "    Respond with only one of: high, medium, low.\n",
    "    \"\"\"\n",
    "    score = llm.invoke([\n",
    "        {'role':'system', 'content':system_prompt},\n",
    "        {'role':'user', 'content':user_input}\n",
    "    ]).content\n",
    "\n",
    "    return str(score)\n",
    "\n",
    "@tool\n",
    "def loyalty_score(score:str)->str:\n",
    "    \"\"\" \n",
    "        Provide rewards to user based on churn score. \n",
    "    \"\"\"\n",
    "    reward_weights = {'high': 1.0, \"medium\": 0.6, \"low\": 0.2}\n",
    "    clv_values = {\"high\": 1000, \"medium\": 500, \"low\": 200}\n",
    "    clv_tier=random.choices(\n",
    "        ['high','medium','low'],\n",
    "        weights=[0.25,0.45,0.3],\n",
    "        k=1\n",
    "    )[0]\n",
    "\n",
    "    reward_score = reward_weights[score]*clv_values[clv_tier]\n",
    "\n",
    "    return reward_score\n",
    "\n",
    "@tool\n",
    "def loyalty_rewards(user_input:str, reward_score:float, churn_score:str)->str:\n",
    "    \"\"\"\n",
    "    Generate a personalized retention message.\n",
    "    \"\"\"\n",
    "    system_prompt=\"\"\" \n",
    "    You are a customer retention assistant. \n",
    "    Generate a polite, empathetic message based on the following:\n",
    "    - The user's message\n",
    "    - Their churn risk (high, medium, low)\n",
    "    - Their loyalty score (40-1000)\n",
    "\n",
    "    Rules:\n",
    "    - If loyalty score >= 800 → emphasize strong appreciation and give a high reward (e.g., big discount, free premium month).\n",
    "    - If 500-799 → show gratitude and offer a medium reward (e.g., discount or perk).\n",
    "    - If 200-499 → acknowledge their value and give a small reward (e.g., loyalty points or small discount).\n",
    "    - If < 200 → do not give a reward, just apologize and promise to improve.\n",
    "    - If churn risk = high → always start by apologizing and showing empathy before mentioning any reward.\n",
    "    - Keep the message short, friendly, concise and natural. Do not include technical terms or scores.\n",
    "    \n",
    "    Respond with only the final concise message.\n",
    "    \"\"\"\n",
    "\n",
    "    user_context = f\"\"\"\n",
    "    User message: {user_input}\n",
    "    Churn risk: {churn_score}\n",
    "    Loyalty score: {reward_score}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([{'role':'system', 'content':system_prompt},\n",
    "                           {'role':'user','content':user_context}]).content\n",
    "    \n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4590802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retention_node(state:SupervisorState)->SupervisorState:\n",
    "    score = churn_score(state['user_input'])\n",
    "    rewards = loyalty_score(score)\n",
    "    loyalty=loyalty_rewards({'user_input':state['user_input'],'reward_score':rewards, 'churn_score':score})\n",
    "    print(rewards)\n",
    "    print(loyalty)\n",
    "    return {'messages':[f\"Retention Handled: {state['user_input']}\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3acc0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddab765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80734c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7034adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4854d2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cdd5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f808c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72adff76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e54004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688dcb9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fe763e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
