{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "463276f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/abhijitdeshpande/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88409d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLE='arn:aws:iam::720332985926:role/service-role/AmazonSageMaker-ExecutionRole-20240705T185967'\n",
    "SOURCE_DIR='/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48b593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21841050",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = 's3://sagemaker-us-east-1-720332985926/huggingface-pytorch-training-2025-06-19-23-24-05-737/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2571b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_model = HuggingFaceModel(\n",
    "    entry_point='inference.py',\n",
    "    model_data=model_uri,\n",
    "    source_dir=SOURCE_DIR,\n",
    "    role=ROLE,\n",
    "    transformers_version='4.49.0',\n",
    "    py_version='py312',\n",
    "    pytorch_version='2.6.0',\n",
    "    sagemaker_session=sagemaker_session\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee54904",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict({\"inputs\": \"Classify the intent:  Can I cacnel my ticket?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75e69170",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    strategy=\"SingleRecord\",\n",
    "    output_path='s3://sagemaker-us-east-1-720332985926/transformer/output/',\n",
    "    accept=\"application/json\",\n",
    "    assemble_with=\"Line\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32238463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: huggingface-pytorch-inference-2025-06-22-14-25-07-216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................Warning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "Warning: MMS is using non-default JVM parameters: -XX:-UseContainerSupport\n",
      "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "2025-06-22T14:29:51,013 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "MMS Home: /opt/conda/lib/python3.12/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /tmp\n",
      "Number of GPUs: 0\n",
      "Number of CPUs: 4\n",
      "Max heap size: 3934 M\n",
      "Python executable: /opt/conda/bin/python3.12\n",
      "Config file: /etc/sagemaker-mms.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Model Store: /\n",
      "Initial Models: model=/opt/ml/model\n",
      "Log dir: null\n",
      "2025-06-22T14:29:51,013 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "MMS Home: /opt/conda/lib/python3.12/site-packages\n",
      "Current directory: /\n",
      "Temp directory: /tmp\n",
      "Number of GPUs: 0\n",
      "Number of CPUs: 4\n",
      "Max heap size: 3934 M\n",
      "Python executable: /opt/conda/bin/python3.12\n",
      "Config file: /etc/sagemaker-mms.properties\n",
      "Inference address: http://0.0.0.0:8080\n",
      "Management address: http://0.0.0.0:8080\n",
      "Model Store: /\n",
      "Initial Models: model=/opt/ml/model\n",
      "Log dir: null\n",
      "Metrics dir: null\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 4\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Preload model: false\n",
      "Prefer direct buffer: false\n",
      "2025-06-22T14:29:51,018 [INFO ] main com.amazonaws.ml.mms.ModelServer - Loading initial models: /opt/ml/model preload_model: false\n",
      "2025-06-22T14:29:51,068 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "2025-06-22T14:29:51,129 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /opt/ml/model --model-name model --preload-model false --tmp-dir /tmp\n",
      "2025-06-22T14:29:51,132 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,132 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 45\n",
      "2025-06-22T14:29:51,133 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "2025-06-22T14:29:51,133 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.12.8\n",
      "2025-06-22T14:29:51,133 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "2025-06-22T14:29:51,138 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,193 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "2025-06-22T14:29:51,200 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "2025-06-22T14:29:51,207 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "2025-06-22T14:29:51,208 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "Model server started.\n",
      "2025-06-22T14:29:51,221 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "Metrics dir: null\n",
      "Netty threads: 0\n",
      "Netty client threads: 0\n",
      "Default workers per model: 4\n",
      "Blacklist Regex: N/A\n",
      "Maximum Response Size: 6553500\n",
      "Maximum Request Size: 6553500\n",
      "Preload model: false\n",
      "Prefer direct buffer: false\n",
      "2025-06-22T14:29:51,018 [INFO ] main com.amazonaws.ml.mms.ModelServer - Loading initial models: /opt/ml/model preload_model: false\n",
      "2025-06-22T14:29:51,068 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "2025-06-22T14:29:51,129 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /tmp/.mms.sock.9000 --handler sagemaker_huggingface_inference_toolkit.handler_service --model-path /opt/ml/model --model-name model --preload-model false --tmp-dir /tmp\n",
      "2025-06-22T14:29:51,132 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,132 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 45\n",
      "2025-06-22T14:29:51,133 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "2025-06-22T14:29:51,133 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.12.8\n",
      "2025-06-22T14:29:51,133 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "2025-06-22T14:29:51,138 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,145 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /tmp/.mms.sock.9000\n",
      "2025-06-22T14:29:51,193 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "2025-06-22T14:29:51,200 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "2025-06-22T14:29:51,207 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "2025-06-22T14:29:51,208 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "Model server started.\n",
      "2025-06-22T14:29:51,221 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /tmp/.mms.sock.9000.\n",
      "2025-06-22T14:29:51,249 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "2025-06-22T14:29:51,413 [INFO ] pool-2-thread-6 ACCESS_LOG - /169.254.255.130:51594 \"GET /ping HTTP/1.1\" 200 28\n",
      "2025-06-22T14:29:51,249 [WARN ] pool-3-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "2025-06-22T14:29:51,413 [INFO ] pool-2-thread-6 ACCESS_LOG - /169.254.255.130:51594 \"GET /ping HTTP/1.1\" 200 28\n",
      "2025-06-22T14:29:52,451 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:51600 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "2025-06-22T14:29:52,451 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:51600 \"GET /execution-parameters HTTP/1.1\" 404 1\n",
      "2025-06-22T14:29:52.461:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=SINGLE_RECORD\n",
      "2025-06-22T14:29:58,358 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,359 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,360 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,361 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,358 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,359 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,360 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,361 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,362 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,362 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,365 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,366 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,367 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,367 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,368 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,368 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,394 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,395 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,396 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,396 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,397 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,397 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,419 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,421 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,422 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,423 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,424 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,424 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,362 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,362 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,365 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,366 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,367 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,367 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,368 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,368 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,394 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,395 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,396 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,396 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,397 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,397 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,419 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Inference script implementation found at `inference`.\n",
      "2025-06-22T14:29:58,421 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `model_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,422 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `input_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,423 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - `predict_fn` implementation found. It will be used in place of the default one.\n",
      "2025-06-22T14:29:58,424 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `output_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:58,424 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - No `transform_fn` implementation was found. The default one from the handler service will be used.\n",
      "2025-06-22T14:29:59,875 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000003-65a1dcd0c40babb2-f6bf94b9\n",
      "2025-06-22T14:29:59,875 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000003-65a1dcd0c40babb2-f6bf94b9\n",
      "2025-06-22T14:29:59,877 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000002-9df548d0c40babb0-cf97dff5\n",
      "2025-06-22T14:29:59,877 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8642\n",
      "2025-06-22T14:29:59,878 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8643\n",
      "2025-06-22T14:29:59,878 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "2025-06-22T14:29:59,878 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "2025-06-22T14:29:59,879 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8617\n",
      "2025-06-22T14:29:59,879 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\n",
      "2025-06-22T14:29:59,879 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000000-ae5e48d0c40babb0-c19ca50a\n",
      "2025-06-22T14:29:59,884 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000004-0d5fdcd0c40babb2-7fed04e0\n",
      "2025-06-22T14:29:59,884 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Prediction error\n",
      "2025-06-22T14:29:59,885 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8643\n",
      "2025-06-22T14:29:59,885 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\n",
      "2025-06-22T14:29:59,884 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2025-06-22T14:29:59,885 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 206, in decode\n",
      "2025-06-22T14:29:59,885 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoder = _decoder_map[content_type]\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -               ~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - KeyError: 'application/jsonlines'\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,887 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:51610 \"POST /invocations HTTP/1.1\" 400 7374\n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 270, in handle\n",
      "2025-06-22T14:29:59,889 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     response = self.transform_fn(*([self.model, input_data, content_type, accept] + self.transform_extra_arg))\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 219, in transform_fn\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     processed_data = self.preprocess(*([input_data, content_type] + self.preprocess_extra_arg))\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 160, in preprocess\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoded_input_data = decoder_encoder.decode(input_data, content_type)\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 209, in decode\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise errors.UnsupportedFormatError(content_type)\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - sagemaker_inference.errors.UnsupportedFormatError: Content type application/jsonlines is not supported by this framework.\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README.\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/mms/service.py\", line 108, in predict\n",
      "2025-06-22T14:29:59,877 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000002-9df548d0c40babb0-cf97dff5\n",
      "2025-06-22T14:29:59,877 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8642\n",
      "2025-06-22T14:29:59,878 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8643\n",
      "2025-06-22T14:29:59,878 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "2025-06-22T14:29:59,878 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "2025-06-22T14:29:59,879 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8617\n",
      "2025-06-22T14:29:59,879 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-3\n",
      "2025-06-22T14:29:59,879 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000000-ae5e48d0c40babb0-c19ca50a\n",
      "2025-06-22T14:29:59,884 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242a9fffefeff83-00000013-00000004-0d5fdcd0c40babb2-7fed04e0\n",
      "2025-06-22T14:29:59,884 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Prediction error\n",
      "2025-06-22T14:29:59,885 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 8643\n",
      "2025-06-22T14:29:59,885 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-4\n",
      "2025-06-22T14:29:59,884 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2025-06-22T14:29:59,885 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 206, in decode\n",
      "2025-06-22T14:29:59,885 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 3\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoder = _decoder_map[content_type]\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -               ~~~~~~~~~~~~^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - KeyError: 'application/jsonlines'\n",
      "2025-06-22T14:29:59,886 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,887 [INFO ] W-9000-model ACCESS_LOG - /169.254.255.130:51610 \"POST /invocations HTTP/1.1\" 400 7374\n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2025-06-22T14:29:59,887 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 270, in handle\n",
      "2025-06-22T14:29:59,889 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     response = self.transform_fn(*([self.model, input_data, content_type, accept] + self.transform_extra_arg))\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 219, in transform_fn\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     processed_data = self.preprocess(*([input_data, content_type] + self.preprocess_extra_arg))\n",
      "2025-06-22T14:29:59,890 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 160, in preprocess\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     decoded_input_data = decoder_encoder.decode(input_data, content_type)\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/decoder_encoder.py\", line 209, in decode\n",
      "2025-06-22T14:29:59,891 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise errors.UnsupportedFormatError(content_type)\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - sagemaker_inference.errors.UnsupportedFormatError: Content type application/jsonlines is not supported by this framework.\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README.\n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,892 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - During handling of the above exception, another exception occurred:\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Traceback (most recent call last):\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/mms/service.py\", line 108, in predict\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch, self.context)\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 279, in handle\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e), 400)\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: Content type application/jsonlines is not supported by this framework.\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README. : 400\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     ret = self._entry_point(input_batch, self.context)\n",
      "2025-06-22T14:29:59,893 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   File \"/opt/conda/lib/python3.12/site-packages/sagemaker_huggingface_inference_toolkit/handler_service.py\", line 279, in handle\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -     raise PredictionException(str(e), 400)\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - mms.service.PredictionException: Content type application/jsonlines is not supported by this framework.\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             Please implement input_fn to to deserialize the request data or an output_fn to\n",
      "2025-06-22T14:29:59,894 [INFO ] W-model-2-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -             serialize the response. For more information, see the SageMaker Python SDK README. : 400\n",
      "2025-06-22T14:29:59.900:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl: ClientError: 400\n",
      "2025-06-22T14:29:59.900:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl: \n",
      "2025-06-22T14:29:59.900:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl: Message:\n",
      "2025-06-22T14:29:59.900:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl: {\n",
      "2025-06-22T14:29:59.900:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl:   \"code\": 400,\n",
      "2025-06-22T14:29:59.901:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl:   \"type\": \"InternalServerException\",\n",
      "2025-06-22T14:29:59.901:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl:   \"message\": \"Content type application/jsonlines is not supported by this framework.\\n\\n            Please implement input_fn to to deserialize the request data or an output_fn to\\n            serialize the response. For more information, see the SageMaker Python SDK README.\"\n",
      "2025-06-22T14:29:59.901:[sagemaker logs]: gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl: }\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:1                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 <span style=\"font-weight: bold; text-decoration: underline\">transformer.transform(</span>                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">data=</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"s3://gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl\"</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">content_type=</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"application/jsonlines\"</span><span style=\"font-weight: bold; text-decoration: underline\">,</span>                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">split_type=</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">'Line'</span>                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/work</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">flow/</span><span style=\"font-weight: bold\">pipeline_context.py</span>:346 in wrapper                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">343 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                               <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">344 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _StepArguments(retrieve_caller_name(self_instance), run_func, *args,    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">345 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>346 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"font-weight: bold; text-decoration: underline\">run_func(*args, **kwargs)</span>                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">347 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">348 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">349 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/</span><span style=\"font-weight: bold\">tran</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"font-weight: bold\">sformer.py</span>:318 in transform                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">316 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> wait:                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>318 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.latest_transform_job.wait(logs=logs)</span>                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">320 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">transform_with_monitoring</span>(                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">321 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>,                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/</span><span style=\"font-weight: bold\">tran</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"font-weight: bold\">sformer.py</span>:686 in wait                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">683 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">684 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span><span style=\"color: #808080; text-decoration-color: #808080\"> </span><span style=\"color: #00ff00; text-decoration-color: #00ff00\">wait</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, logs=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>):                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">685 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> logs:                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>686 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.sagemaker_session.logs_for_transform_job(</span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.job_name, wait=</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">True</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">687 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">688 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sagemaker_session.wait_for_transform_job(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.job_name)                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">689 </span>                                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/</span><span style=\"font-weight: bold\">sess</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"font-weight: bold\">ion.py</span>:6315 in logs_for_transform_job                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6312 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>state = LogState.JOB_COMPLETE                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6313 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6314 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> wait:                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>6315 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">_check_job_status(job_name, description, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"TransformJobStatus\"</span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6316 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> dot:                                                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6317 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>()                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6318 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/</span><span style=\"font-weight: bold\">sess</span> <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"font-weight: bold\">ion.py</span>:8835 in _check_job_status                                                                 <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8832 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>allowed_statuses=[<span style=\"color: #808000; text-decoration-color: #808000\">\"Completed\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"Stopped\"</span>],                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8833 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>actual_status=status,                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8834 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>8835 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> exceptions.UnexpectedStatusException(</span>                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8836 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">message=message,</span>                                                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8837 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">allowed_statuses=[</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"Completed\"</span><span style=\"font-weight: bold; text-decoration: underline\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"Stopped\"</span><span style=\"font-weight: bold; text-decoration: underline\">],</span>                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8838 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">actual_status=status,</span>                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">UnexpectedStatusException: </span>Error for Transform job huggingface-pytorch-inference-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-06-22-14-25-07-216: Failed. \n",
       "Reason: ClientError: See job logs for more information. Check troubleshooting guide for common errors: \n",
       "<span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:1                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1 \u001b[1;4mtransformer.transform(\u001b[0m                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m2 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mdata=\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33ms3://gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m,\u001b[0m                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4mcontent_type=\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mapplication/jsonlines\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m,\u001b[0m                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;4msplit_type=\u001b[0m\u001b[1;4;33m'\u001b[0m\u001b[1;4;33mLine\u001b[0m\u001b[1;4;33m'\u001b[0m                                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/work\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2mflow/\u001b[0m\u001b[1mpipeline_context.py\u001b[0m:346 in wrapper                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m343 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m344 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _StepArguments(retrieve_caller_name(self_instance), run_func, *args,    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m345 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m346 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4mrun_func(*args, **kwargs)\u001b[0m                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m347 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m348 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m349 \u001b[0m                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/\u001b[0m\u001b[1mtran\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[1msformer.py\u001b[0m:318 in transform                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m316 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m317 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m wait:                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m318 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.latest_transform_job.wait(logs=logs)\u001b[0m                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m319 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m320 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mtransform_with_monitoring\u001b[0m(                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m321 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/\u001b[0m\u001b[1mtran\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[1msformer.py\u001b[0m:686 in wait                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m683 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m684 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m\u001b[90m \u001b[0m\u001b[92mwait\u001b[0m(\u001b[96mself\u001b[0m, logs=\u001b[94mTrue\u001b[0m):                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m685 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m logs:                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m686 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.sagemaker_session.logs_for_transform_job(\u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.job_name, wait=\u001b[0m\u001b[1;4;94mTrue\u001b[0m\u001b[1;4m)\u001b[0m        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m687 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m688 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.sagemaker_session.wait_for_transform_job(\u001b[96mself\u001b[0m.job_name)                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m689 \u001b[0m                                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/\u001b[0m\u001b[1msess\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[1mion.py\u001b[0m:6315 in logs_for_transform_job                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6312 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstate = LogState.JOB_COMPLETE                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6313 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6314 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m wait:                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m6315 \u001b[2m│   │   │   \u001b[0m\u001b[1;4m_check_job_status(job_name, description, \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mTransformJobStatus\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m)\u001b[0m                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6316 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m dot:                                                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6317 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mprint\u001b[0m()                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m6318 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/sagemaker/\u001b[0m\u001b[1msess\u001b[0m \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[1mion.py\u001b[0m:8835 in _check_job_status                                                                 \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m8832 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mallowed_statuses=[\u001b[33m\"\u001b[0m\u001b[33mCompleted\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mStopped\u001b[0m\u001b[33m\"\u001b[0m],                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m8833 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mactual_status=status,                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m8834 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m8835 \u001b[2m│   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m exceptions.UnexpectedStatusException(\u001b[0m                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m8836 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mmessage=message,\u001b[0m                                                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m8837 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mallowed_statuses=[\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mCompleted\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m, \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mStopped\u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4m],\u001b[0m                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m8838 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4mactual_status=status,\u001b[0m                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mUnexpectedStatusException: \u001b[0mError for Transform job huggingface-pytorch-inference-\u001b[1;36m2025\u001b[0m-06-22-14-25-07-216: Failed. \n",
       "Reason: ClientError: See job logs for more information. Check troubleshooting guide for common errors: \n",
       "\u001b[4;38;2;0;105;255mhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-python-sdk-troubleshooting.html\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.transform(\n",
    "    data=\"s3://gen-ai-repository/finetuning/flan-t5/data/flan-t5-test.jsonl\",\n",
    "    content_type=\"application/json\",\n",
    "    split_type='Line',\n",
    "    wait=True\t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6c9c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/output/checkpoint-1888'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fe39dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/output/checkpoint-1888'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64615942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f84d254f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "#load base model & wrap with peft\n",
    "peft_config = PeftConfig.from_pretrained(checkpoint_path)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "096df4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f98b5da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '/Volumes/LaCie/Projects_portfolio/NLP/SupportIQ/output/final_merged_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17e313f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06551855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-720332985926/huggingface-pytorch-training-2025-06-19-23-24-05-737/output/model.tar.gz'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6983ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
