{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7944915254237288,
  "eval_steps": 500,
  "global_step": 1500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005296610169491525,
      "grad_norm": 9.824673652648926,
      "learning_rate": 9.99470338983051e-06,
      "loss": 49.112,
      "step": 1
    },
    {
      "epoch": 0.001059322033898305,
      "grad_norm": 9.114420890808105,
      "learning_rate": 9.989406779661017e-06,
      "loss": 45.4029,
      "step": 2
    },
    {
      "epoch": 0.0015889830508474577,
      "grad_norm": 7.220670700073242,
      "learning_rate": 9.984110169491527e-06,
      "loss": 44.7267,
      "step": 3
    },
    {
      "epoch": 0.00211864406779661,
      "grad_norm": 8.346141815185547,
      "learning_rate": 9.978813559322034e-06,
      "loss": 46.5697,
      "step": 4
    },
    {
      "epoch": 0.0026483050847457626,
      "grad_norm": 8.710979461669922,
      "learning_rate": 9.973516949152543e-06,
      "loss": 46.0647,
      "step": 5
    },
    {
      "epoch": 0.0031779661016949155,
      "grad_norm": 10.219281196594238,
      "learning_rate": 9.968220338983052e-06,
      "loss": 47.7918,
      "step": 6
    },
    {
      "epoch": 0.003707627118644068,
      "grad_norm": 7.452911376953125,
      "learning_rate": 9.96292372881356e-06,
      "loss": 43.4166,
      "step": 7
    },
    {
      "epoch": 0.00423728813559322,
      "grad_norm": 10.418255805969238,
      "learning_rate": 9.957627118644069e-06,
      "loss": 48.958,
      "step": 8
    },
    {
      "epoch": 0.004766949152542373,
      "grad_norm": 7.719862937927246,
      "learning_rate": 9.952330508474576e-06,
      "loss": 45.2572,
      "step": 9
    },
    {
      "epoch": 0.005296610169491525,
      "grad_norm": 8.043261528015137,
      "learning_rate": 9.947033898305085e-06,
      "loss": 46.2131,
      "step": 10
    },
    {
      "epoch": 0.005826271186440678,
      "grad_norm": 10.416719436645508,
      "learning_rate": 9.941737288135594e-06,
      "loss": 46.287,
      "step": 11
    },
    {
      "epoch": 0.006355932203389831,
      "grad_norm": 8.956294059753418,
      "learning_rate": 9.936440677966102e-06,
      "loss": 44.8255,
      "step": 12
    },
    {
      "epoch": 0.006885593220338983,
      "grad_norm": 10.272127151489258,
      "learning_rate": 9.931144067796611e-06,
      "loss": 47.7993,
      "step": 13
    },
    {
      "epoch": 0.007415254237288136,
      "grad_norm": 10.842305183410645,
      "learning_rate": 9.92584745762712e-06,
      "loss": 49.068,
      "step": 14
    },
    {
      "epoch": 0.007944915254237288,
      "grad_norm": 8.35157585144043,
      "learning_rate": 9.920550847457629e-06,
      "loss": 45.1803,
      "step": 15
    },
    {
      "epoch": 0.00847457627118644,
      "grad_norm": 9.607563018798828,
      "learning_rate": 9.915254237288137e-06,
      "loss": 46.5756,
      "step": 16
    },
    {
      "epoch": 0.009004237288135594,
      "grad_norm": 9.83224868774414,
      "learning_rate": 9.909957627118644e-06,
      "loss": 46.5982,
      "step": 17
    },
    {
      "epoch": 0.009533898305084746,
      "grad_norm": 9.431659698486328,
      "learning_rate": 9.904661016949153e-06,
      "loss": 46.8067,
      "step": 18
    },
    {
      "epoch": 0.010063559322033898,
      "grad_norm": 10.952531814575195,
      "learning_rate": 9.899364406779662e-06,
      "loss": 49.3123,
      "step": 19
    },
    {
      "epoch": 0.01059322033898305,
      "grad_norm": 8.882469177246094,
      "learning_rate": 9.89406779661017e-06,
      "loss": 46.7754,
      "step": 20
    },
    {
      "epoch": 0.011122881355932203,
      "grad_norm": 10.91796588897705,
      "learning_rate": 9.88877118644068e-06,
      "loss": 49.2673,
      "step": 21
    },
    {
      "epoch": 0.011652542372881356,
      "grad_norm": 8.245124816894531,
      "learning_rate": 9.883474576271186e-06,
      "loss": 47.0233,
      "step": 22
    },
    {
      "epoch": 0.012182203389830509,
      "grad_norm": 11.488899230957031,
      "learning_rate": 9.878177966101695e-06,
      "loss": 46.9983,
      "step": 23
    },
    {
      "epoch": 0.012711864406779662,
      "grad_norm": 9.0787992477417,
      "learning_rate": 9.872881355932204e-06,
      "loss": 47.0618,
      "step": 24
    },
    {
      "epoch": 0.013241525423728813,
      "grad_norm": 10.228734970092773,
      "learning_rate": 9.867584745762713e-06,
      "loss": 48.6781,
      "step": 25
    },
    {
      "epoch": 0.013771186440677966,
      "grad_norm": 9.935210227966309,
      "learning_rate": 9.862288135593221e-06,
      "loss": 45.1185,
      "step": 26
    },
    {
      "epoch": 0.014300847457627119,
      "grad_norm": 10.917930603027344,
      "learning_rate": 9.856991525423729e-06,
      "loss": 45.1888,
      "step": 27
    },
    {
      "epoch": 0.014830508474576272,
      "grad_norm": 7.623673439025879,
      "learning_rate": 9.851694915254239e-06,
      "loss": 45.5157,
      "step": 28
    },
    {
      "epoch": 0.015360169491525424,
      "grad_norm": 8.786433219909668,
      "learning_rate": 9.846398305084746e-06,
      "loss": 47.8806,
      "step": 29
    },
    {
      "epoch": 0.015889830508474576,
      "grad_norm": 12.938705444335938,
      "learning_rate": 9.841101694915255e-06,
      "loss": 47.764,
      "step": 30
    },
    {
      "epoch": 0.01641949152542373,
      "grad_norm": 10.660422325134277,
      "learning_rate": 9.835805084745764e-06,
      "loss": 46.2601,
      "step": 31
    },
    {
      "epoch": 0.01694915254237288,
      "grad_norm": 9.035418510437012,
      "learning_rate": 9.830508474576272e-06,
      "loss": 44.5004,
      "step": 32
    },
    {
      "epoch": 0.017478813559322032,
      "grad_norm": 10.623464584350586,
      "learning_rate": 9.825211864406781e-06,
      "loss": 45.7476,
      "step": 33
    },
    {
      "epoch": 0.018008474576271187,
      "grad_norm": 9.58043384552002,
      "learning_rate": 9.819915254237288e-06,
      "loss": 47.5666,
      "step": 34
    },
    {
      "epoch": 0.018538135593220338,
      "grad_norm": 8.9461088180542,
      "learning_rate": 9.814618644067797e-06,
      "loss": 46.0464,
      "step": 35
    },
    {
      "epoch": 0.019067796610169493,
      "grad_norm": 9.794960021972656,
      "learning_rate": 9.809322033898306e-06,
      "loss": 47.527,
      "step": 36
    },
    {
      "epoch": 0.019597457627118644,
      "grad_norm": 10.666226387023926,
      "learning_rate": 9.804025423728814e-06,
      "loss": 47.016,
      "step": 37
    },
    {
      "epoch": 0.020127118644067795,
      "grad_norm": 9.922653198242188,
      "learning_rate": 9.798728813559323e-06,
      "loss": 47.1683,
      "step": 38
    },
    {
      "epoch": 0.02065677966101695,
      "grad_norm": 11.202757835388184,
      "learning_rate": 9.793432203389832e-06,
      "loss": 45.9637,
      "step": 39
    },
    {
      "epoch": 0.0211864406779661,
      "grad_norm": 10.992786407470703,
      "learning_rate": 9.788135593220339e-06,
      "loss": 44.7269,
      "step": 40
    },
    {
      "epoch": 0.021716101694915255,
      "grad_norm": 9.525754928588867,
      "learning_rate": 9.78283898305085e-06,
      "loss": 47.6572,
      "step": 41
    },
    {
      "epoch": 0.022245762711864406,
      "grad_norm": 10.104632377624512,
      "learning_rate": 9.777542372881356e-06,
      "loss": 48.5083,
      "step": 42
    },
    {
      "epoch": 0.022775423728813558,
      "grad_norm": 10.875740051269531,
      "learning_rate": 9.772245762711865e-06,
      "loss": 47.3372,
      "step": 43
    },
    {
      "epoch": 0.023305084745762712,
      "grad_norm": 11.132129669189453,
      "learning_rate": 9.766949152542374e-06,
      "loss": 45.3571,
      "step": 44
    },
    {
      "epoch": 0.023834745762711863,
      "grad_norm": 9.954155921936035,
      "learning_rate": 9.761652542372883e-06,
      "loss": 47.5347,
      "step": 45
    },
    {
      "epoch": 0.024364406779661018,
      "grad_norm": 11.31274700164795,
      "learning_rate": 9.756355932203391e-06,
      "loss": 44.5112,
      "step": 46
    },
    {
      "epoch": 0.02489406779661017,
      "grad_norm": 12.590404510498047,
      "learning_rate": 9.751059322033898e-06,
      "loss": 47.6753,
      "step": 47
    },
    {
      "epoch": 0.025423728813559324,
      "grad_norm": 13.069845199584961,
      "learning_rate": 9.745762711864407e-06,
      "loss": 47.4901,
      "step": 48
    },
    {
      "epoch": 0.025953389830508475,
      "grad_norm": 11.670619010925293,
      "learning_rate": 9.740466101694916e-06,
      "loss": 48.0667,
      "step": 49
    },
    {
      "epoch": 0.026483050847457626,
      "grad_norm": 13.390597343444824,
      "learning_rate": 9.735169491525425e-06,
      "loss": 46.8159,
      "step": 50
    },
    {
      "epoch": 0.02701271186440678,
      "grad_norm": 12.925251960754395,
      "learning_rate": 9.729872881355933e-06,
      "loss": 49.3867,
      "step": 51
    },
    {
      "epoch": 0.02754237288135593,
      "grad_norm": 10.754331588745117,
      "learning_rate": 9.72457627118644e-06,
      "loss": 46.4268,
      "step": 52
    },
    {
      "epoch": 0.028072033898305086,
      "grad_norm": 10.62302017211914,
      "learning_rate": 9.719279661016951e-06,
      "loss": 47.3283,
      "step": 53
    },
    {
      "epoch": 0.028601694915254237,
      "grad_norm": 13.34897518157959,
      "learning_rate": 9.713983050847458e-06,
      "loss": 46.3467,
      "step": 54
    },
    {
      "epoch": 0.02913135593220339,
      "grad_norm": 10.99782657623291,
      "learning_rate": 9.708686440677967e-06,
      "loss": 47.2685,
      "step": 55
    },
    {
      "epoch": 0.029661016949152543,
      "grad_norm": 10.190768241882324,
      "learning_rate": 9.703389830508475e-06,
      "loss": 43.3058,
      "step": 56
    },
    {
      "epoch": 0.030190677966101694,
      "grad_norm": 14.833115577697754,
      "learning_rate": 9.698093220338984e-06,
      "loss": 48.7503,
      "step": 57
    },
    {
      "epoch": 0.03072033898305085,
      "grad_norm": 14.009073257446289,
      "learning_rate": 9.692796610169493e-06,
      "loss": 48.5087,
      "step": 58
    },
    {
      "epoch": 0.03125,
      "grad_norm": 9.995011329650879,
      "learning_rate": 9.6875e-06,
      "loss": 43.5146,
      "step": 59
    },
    {
      "epoch": 0.03177966101694915,
      "grad_norm": 11.539137840270996,
      "learning_rate": 9.682203389830509e-06,
      "loss": 45.2129,
      "step": 60
    },
    {
      "epoch": 0.0323093220338983,
      "grad_norm": 11.501319885253906,
      "learning_rate": 9.676906779661017e-06,
      "loss": 48.0151,
      "step": 61
    },
    {
      "epoch": 0.03283898305084746,
      "grad_norm": 12.755085945129395,
      "learning_rate": 9.671610169491526e-06,
      "loss": 47.2875,
      "step": 62
    },
    {
      "epoch": 0.03336864406779661,
      "grad_norm": 12.369650840759277,
      "learning_rate": 9.666313559322035e-06,
      "loss": 49.4209,
      "step": 63
    },
    {
      "epoch": 0.03389830508474576,
      "grad_norm": 12.440911293029785,
      "learning_rate": 9.661016949152544e-06,
      "loss": 45.788,
      "step": 64
    },
    {
      "epoch": 0.034427966101694914,
      "grad_norm": 10.421191215515137,
      "learning_rate": 9.65572033898305e-06,
      "loss": 45.5261,
      "step": 65
    },
    {
      "epoch": 0.034957627118644065,
      "grad_norm": 11.502129554748535,
      "learning_rate": 9.650423728813561e-06,
      "loss": 46.3147,
      "step": 66
    },
    {
      "epoch": 0.03548728813559322,
      "grad_norm": 10.831786155700684,
      "learning_rate": 9.645127118644068e-06,
      "loss": 45.5018,
      "step": 67
    },
    {
      "epoch": 0.036016949152542374,
      "grad_norm": 11.678831100463867,
      "learning_rate": 9.639830508474577e-06,
      "loss": 48.9308,
      "step": 68
    },
    {
      "epoch": 0.036546610169491525,
      "grad_norm": 11.768855094909668,
      "learning_rate": 9.634533898305086e-06,
      "loss": 45.957,
      "step": 69
    },
    {
      "epoch": 0.037076271186440676,
      "grad_norm": 11.676040649414062,
      "learning_rate": 9.629237288135595e-06,
      "loss": 46.3126,
      "step": 70
    },
    {
      "epoch": 0.03760593220338983,
      "grad_norm": 11.123186111450195,
      "learning_rate": 9.623940677966103e-06,
      "loss": 45.2067,
      "step": 71
    },
    {
      "epoch": 0.038135593220338986,
      "grad_norm": 16.158811569213867,
      "learning_rate": 9.61864406779661e-06,
      "loss": 48.8853,
      "step": 72
    },
    {
      "epoch": 0.03866525423728814,
      "grad_norm": 12.688535690307617,
      "learning_rate": 9.613347457627119e-06,
      "loss": 48.8538,
      "step": 73
    },
    {
      "epoch": 0.03919491525423729,
      "grad_norm": 12.348453521728516,
      "learning_rate": 9.608050847457628e-06,
      "loss": 48.2938,
      "step": 74
    },
    {
      "epoch": 0.03972457627118644,
      "grad_norm": 10.481280326843262,
      "learning_rate": 9.602754237288137e-06,
      "loss": 44.2207,
      "step": 75
    },
    {
      "epoch": 0.04025423728813559,
      "grad_norm": 12.226302146911621,
      "learning_rate": 9.597457627118645e-06,
      "loss": 45.945,
      "step": 76
    },
    {
      "epoch": 0.04078389830508475,
      "grad_norm": 12.43635368347168,
      "learning_rate": 9.592161016949152e-06,
      "loss": 47.7524,
      "step": 77
    },
    {
      "epoch": 0.0413135593220339,
      "grad_norm": 10.253814697265625,
      "learning_rate": 9.586864406779663e-06,
      "loss": 45.0262,
      "step": 78
    },
    {
      "epoch": 0.04184322033898305,
      "grad_norm": 13.549278259277344,
      "learning_rate": 9.58156779661017e-06,
      "loss": 47.7906,
      "step": 79
    },
    {
      "epoch": 0.0423728813559322,
      "grad_norm": 11.23200511932373,
      "learning_rate": 9.576271186440679e-06,
      "loss": 43.7926,
      "step": 80
    },
    {
      "epoch": 0.04290254237288135,
      "grad_norm": 13.726470947265625,
      "learning_rate": 9.570974576271187e-06,
      "loss": 44.0771,
      "step": 81
    },
    {
      "epoch": 0.04343220338983051,
      "grad_norm": 12.592391967773438,
      "learning_rate": 9.565677966101694e-06,
      "loss": 47.132,
      "step": 82
    },
    {
      "epoch": 0.04396186440677966,
      "grad_norm": 8.935877799987793,
      "learning_rate": 9.560381355932205e-06,
      "loss": 44.6114,
      "step": 83
    },
    {
      "epoch": 0.04449152542372881,
      "grad_norm": 11.312682151794434,
      "learning_rate": 9.555084745762712e-06,
      "loss": 45.9913,
      "step": 84
    },
    {
      "epoch": 0.045021186440677964,
      "grad_norm": 11.910426139831543,
      "learning_rate": 9.54978813559322e-06,
      "loss": 45.2182,
      "step": 85
    },
    {
      "epoch": 0.045550847457627115,
      "grad_norm": 14.938702583312988,
      "learning_rate": 9.54449152542373e-06,
      "loss": 46.6207,
      "step": 86
    },
    {
      "epoch": 0.04608050847457627,
      "grad_norm": 10.8577880859375,
      "learning_rate": 9.539194915254238e-06,
      "loss": 44.3205,
      "step": 87
    },
    {
      "epoch": 0.046610169491525424,
      "grad_norm": 10.709208488464355,
      "learning_rate": 9.533898305084747e-06,
      "loss": 44.0943,
      "step": 88
    },
    {
      "epoch": 0.047139830508474576,
      "grad_norm": 12.558162689208984,
      "learning_rate": 9.528601694915256e-06,
      "loss": 44.6588,
      "step": 89
    },
    {
      "epoch": 0.04766949152542373,
      "grad_norm": 11.565126419067383,
      "learning_rate": 9.523305084745763e-06,
      "loss": 44.9817,
      "step": 90
    },
    {
      "epoch": 0.048199152542372885,
      "grad_norm": 12.934249877929688,
      "learning_rate": 9.518008474576273e-06,
      "loss": 45.7716,
      "step": 91
    },
    {
      "epoch": 0.048728813559322036,
      "grad_norm": 8.9599609375,
      "learning_rate": 9.51271186440678e-06,
      "loss": 44.4895,
      "step": 92
    },
    {
      "epoch": 0.04925847457627119,
      "grad_norm": 12.892339706420898,
      "learning_rate": 9.507415254237289e-06,
      "loss": 45.6362,
      "step": 93
    },
    {
      "epoch": 0.04978813559322034,
      "grad_norm": 13.398110389709473,
      "learning_rate": 9.502118644067798e-06,
      "loss": 46.4862,
      "step": 94
    },
    {
      "epoch": 0.05031779661016949,
      "grad_norm": 13.660088539123535,
      "learning_rate": 9.496822033898306e-06,
      "loss": 46.34,
      "step": 95
    },
    {
      "epoch": 0.05084745762711865,
      "grad_norm": 16.327390670776367,
      "learning_rate": 9.491525423728815e-06,
      "loss": 48.711,
      "step": 96
    },
    {
      "epoch": 0.0513771186440678,
      "grad_norm": 10.168864250183105,
      "learning_rate": 9.486228813559322e-06,
      "loss": 44.7249,
      "step": 97
    },
    {
      "epoch": 0.05190677966101695,
      "grad_norm": 11.677518844604492,
      "learning_rate": 9.480932203389831e-06,
      "loss": 45.6954,
      "step": 98
    },
    {
      "epoch": 0.0524364406779661,
      "grad_norm": 12.626070022583008,
      "learning_rate": 9.47563559322034e-06,
      "loss": 44.8583,
      "step": 99
    },
    {
      "epoch": 0.05296610169491525,
      "grad_norm": 11.655086517333984,
      "learning_rate": 9.470338983050848e-06,
      "loss": 46.5279,
      "step": 100
    },
    {
      "epoch": 0.05349576271186441,
      "grad_norm": 13.484797477722168,
      "learning_rate": 9.465042372881357e-06,
      "loss": 45.0749,
      "step": 101
    },
    {
      "epoch": 0.05402542372881356,
      "grad_norm": 10.462074279785156,
      "learning_rate": 9.459745762711864e-06,
      "loss": 45.4183,
      "step": 102
    },
    {
      "epoch": 0.05455508474576271,
      "grad_norm": 17.580568313598633,
      "learning_rate": 9.454449152542373e-06,
      "loss": 48.3493,
      "step": 103
    },
    {
      "epoch": 0.05508474576271186,
      "grad_norm": 12.802325248718262,
      "learning_rate": 9.449152542372882e-06,
      "loss": 43.8834,
      "step": 104
    },
    {
      "epoch": 0.055614406779661014,
      "grad_norm": 14.19803524017334,
      "learning_rate": 9.44385593220339e-06,
      "loss": 46.2765,
      "step": 105
    },
    {
      "epoch": 0.05614406779661017,
      "grad_norm": 12.183131217956543,
      "learning_rate": 9.4385593220339e-06,
      "loss": 44.7768,
      "step": 106
    },
    {
      "epoch": 0.056673728813559324,
      "grad_norm": 13.098036766052246,
      "learning_rate": 9.433262711864406e-06,
      "loss": 45.3535,
      "step": 107
    },
    {
      "epoch": 0.057203389830508475,
      "grad_norm": 16.667449951171875,
      "learning_rate": 9.427966101694917e-06,
      "loss": 46.3868,
      "step": 108
    },
    {
      "epoch": 0.057733050847457626,
      "grad_norm": 11.712324142456055,
      "learning_rate": 9.422669491525424e-06,
      "loss": 43.7138,
      "step": 109
    },
    {
      "epoch": 0.05826271186440678,
      "grad_norm": 14.999502182006836,
      "learning_rate": 9.417372881355933e-06,
      "loss": 45.4052,
      "step": 110
    },
    {
      "epoch": 0.058792372881355935,
      "grad_norm": 14.092772483825684,
      "learning_rate": 9.412076271186441e-06,
      "loss": 45.6388,
      "step": 111
    },
    {
      "epoch": 0.059322033898305086,
      "grad_norm": 12.757980346679688,
      "learning_rate": 9.40677966101695e-06,
      "loss": 43.3002,
      "step": 112
    },
    {
      "epoch": 0.05985169491525424,
      "grad_norm": 14.760502815246582,
      "learning_rate": 9.401483050847459e-06,
      "loss": 45.9261,
      "step": 113
    },
    {
      "epoch": 0.06038135593220339,
      "grad_norm": 11.731225967407227,
      "learning_rate": 9.396186440677968e-06,
      "loss": 44.64,
      "step": 114
    },
    {
      "epoch": 0.06091101694915254,
      "grad_norm": 14.624043464660645,
      "learning_rate": 9.390889830508475e-06,
      "loss": 46.0926,
      "step": 115
    },
    {
      "epoch": 0.0614406779661017,
      "grad_norm": 11.877707481384277,
      "learning_rate": 9.385593220338985e-06,
      "loss": 44.3539,
      "step": 116
    },
    {
      "epoch": 0.06197033898305085,
      "grad_norm": 12.445934295654297,
      "learning_rate": 9.380296610169492e-06,
      "loss": 45.185,
      "step": 117
    },
    {
      "epoch": 0.0625,
      "grad_norm": 14.30274772644043,
      "learning_rate": 9.375000000000001e-06,
      "loss": 46.8473,
      "step": 118
    },
    {
      "epoch": 0.06302966101694915,
      "grad_norm": 12.959136009216309,
      "learning_rate": 9.36970338983051e-06,
      "loss": 43.2377,
      "step": 119
    },
    {
      "epoch": 0.0635593220338983,
      "grad_norm": 17.73836326599121,
      "learning_rate": 9.364406779661017e-06,
      "loss": 46.01,
      "step": 120
    },
    {
      "epoch": 0.06408898305084745,
      "grad_norm": 13.67056655883789,
      "learning_rate": 9.359110169491527e-06,
      "loss": 45.9014,
      "step": 121
    },
    {
      "epoch": 0.0646186440677966,
      "grad_norm": 15.143765449523926,
      "learning_rate": 9.353813559322034e-06,
      "loss": 45.6664,
      "step": 122
    },
    {
      "epoch": 0.06514830508474577,
      "grad_norm": 12.251914024353027,
      "learning_rate": 9.348516949152543e-06,
      "loss": 45.8862,
      "step": 123
    },
    {
      "epoch": 0.06567796610169492,
      "grad_norm": 11.660740852355957,
      "learning_rate": 9.343220338983052e-06,
      "loss": 43.7876,
      "step": 124
    },
    {
      "epoch": 0.06620762711864407,
      "grad_norm": 12.778264999389648,
      "learning_rate": 9.33792372881356e-06,
      "loss": 44.4956,
      "step": 125
    },
    {
      "epoch": 0.06673728813559322,
      "grad_norm": 13.168693542480469,
      "learning_rate": 9.33262711864407e-06,
      "loss": 44.3098,
      "step": 126
    },
    {
      "epoch": 0.06726694915254237,
      "grad_norm": 12.793709754943848,
      "learning_rate": 9.327330508474576e-06,
      "loss": 46.5531,
      "step": 127
    },
    {
      "epoch": 0.06779661016949153,
      "grad_norm": 17.414196014404297,
      "learning_rate": 9.322033898305085e-06,
      "loss": 46.2886,
      "step": 128
    },
    {
      "epoch": 0.06832627118644068,
      "grad_norm": 13.372808456420898,
      "learning_rate": 9.316737288135594e-06,
      "loss": 44.8482,
      "step": 129
    },
    {
      "epoch": 0.06885593220338983,
      "grad_norm": 14.688763618469238,
      "learning_rate": 9.311440677966102e-06,
      "loss": 46.9127,
      "step": 130
    },
    {
      "epoch": 0.06938559322033898,
      "grad_norm": 13.017791748046875,
      "learning_rate": 9.306144067796611e-06,
      "loss": 45.6465,
      "step": 131
    },
    {
      "epoch": 0.06991525423728813,
      "grad_norm": 18.00020980834961,
      "learning_rate": 9.300847457627118e-06,
      "loss": 51.6891,
      "step": 132
    },
    {
      "epoch": 0.0704449152542373,
      "grad_norm": 14.6775541305542,
      "learning_rate": 9.295550847457629e-06,
      "loss": 43.979,
      "step": 133
    },
    {
      "epoch": 0.07097457627118645,
      "grad_norm": 14.837903022766113,
      "learning_rate": 9.290254237288136e-06,
      "loss": 46.4973,
      "step": 134
    },
    {
      "epoch": 0.0715042372881356,
      "grad_norm": 13.911921501159668,
      "learning_rate": 9.284957627118645e-06,
      "loss": 44.6305,
      "step": 135
    },
    {
      "epoch": 0.07203389830508475,
      "grad_norm": 14.227741241455078,
      "learning_rate": 9.279661016949153e-06,
      "loss": 44.9326,
      "step": 136
    },
    {
      "epoch": 0.0725635593220339,
      "grad_norm": 13.136384963989258,
      "learning_rate": 9.274364406779662e-06,
      "loss": 44.2295,
      "step": 137
    },
    {
      "epoch": 0.07309322033898305,
      "grad_norm": 12.225574493408203,
      "learning_rate": 9.26906779661017e-06,
      "loss": 45.3111,
      "step": 138
    },
    {
      "epoch": 0.0736228813559322,
      "grad_norm": 14.368593215942383,
      "learning_rate": 9.26377118644068e-06,
      "loss": 43.9123,
      "step": 139
    },
    {
      "epoch": 0.07415254237288135,
      "grad_norm": 13.189476013183594,
      "learning_rate": 9.258474576271187e-06,
      "loss": 44.086,
      "step": 140
    },
    {
      "epoch": 0.0746822033898305,
      "grad_norm": 13.54965877532959,
      "learning_rate": 9.253177966101695e-06,
      "loss": 41.8549,
      "step": 141
    },
    {
      "epoch": 0.07521186440677965,
      "grad_norm": 15.71441650390625,
      "learning_rate": 9.247881355932204e-06,
      "loss": 44.6146,
      "step": 142
    },
    {
      "epoch": 0.07574152542372882,
      "grad_norm": 14.36960506439209,
      "learning_rate": 9.242584745762713e-06,
      "loss": 45.6127,
      "step": 143
    },
    {
      "epoch": 0.07627118644067797,
      "grad_norm": 15.577178955078125,
      "learning_rate": 9.237288135593222e-06,
      "loss": 42.1889,
      "step": 144
    },
    {
      "epoch": 0.07680084745762712,
      "grad_norm": 19.175662994384766,
      "learning_rate": 9.231991525423729e-06,
      "loss": 45.8298,
      "step": 145
    },
    {
      "epoch": 0.07733050847457627,
      "grad_norm": 15.818450927734375,
      "learning_rate": 9.226694915254239e-06,
      "loss": 44.479,
      "step": 146
    },
    {
      "epoch": 0.07786016949152542,
      "grad_norm": 16.356101989746094,
      "learning_rate": 9.221398305084746e-06,
      "loss": 44.9392,
      "step": 147
    },
    {
      "epoch": 0.07838983050847458,
      "grad_norm": 17.910112380981445,
      "learning_rate": 9.216101694915255e-06,
      "loss": 44.5173,
      "step": 148
    },
    {
      "epoch": 0.07891949152542373,
      "grad_norm": 11.76662826538086,
      "learning_rate": 9.210805084745764e-06,
      "loss": 42.7223,
      "step": 149
    },
    {
      "epoch": 0.07944915254237288,
      "grad_norm": 15.474702835083008,
      "learning_rate": 9.205508474576272e-06,
      "loss": 46.411,
      "step": 150
    },
    {
      "epoch": 0.07997881355932203,
      "grad_norm": 12.965540885925293,
      "learning_rate": 9.200211864406781e-06,
      "loss": 43.8627,
      "step": 151
    },
    {
      "epoch": 0.08050847457627118,
      "grad_norm": 16.641456604003906,
      "learning_rate": 9.194915254237288e-06,
      "loss": 44.25,
      "step": 152
    },
    {
      "epoch": 0.08103813559322035,
      "grad_norm": 14.435934066772461,
      "learning_rate": 9.189618644067797e-06,
      "loss": 43.3545,
      "step": 153
    },
    {
      "epoch": 0.0815677966101695,
      "grad_norm": 15.837759017944336,
      "learning_rate": 9.184322033898306e-06,
      "loss": 44.3124,
      "step": 154
    },
    {
      "epoch": 0.08209745762711865,
      "grad_norm": 16.089370727539062,
      "learning_rate": 9.179025423728814e-06,
      "loss": 45.6744,
      "step": 155
    },
    {
      "epoch": 0.0826271186440678,
      "grad_norm": 14.413983345031738,
      "learning_rate": 9.173728813559323e-06,
      "loss": 44.0566,
      "step": 156
    },
    {
      "epoch": 0.08315677966101695,
      "grad_norm": 12.107999801635742,
      "learning_rate": 9.16843220338983e-06,
      "loss": 43.1977,
      "step": 157
    },
    {
      "epoch": 0.0836864406779661,
      "grad_norm": 14.233626365661621,
      "learning_rate": 9.163135593220339e-06,
      "loss": 43.8167,
      "step": 158
    },
    {
      "epoch": 0.08421610169491525,
      "grad_norm": 12.994474411010742,
      "learning_rate": 9.157838983050848e-06,
      "loss": 43.649,
      "step": 159
    },
    {
      "epoch": 0.0847457627118644,
      "grad_norm": 14.202582359313965,
      "learning_rate": 9.152542372881356e-06,
      "loss": 42.5304,
      "step": 160
    },
    {
      "epoch": 0.08527542372881355,
      "grad_norm": 13.463746070861816,
      "learning_rate": 9.147245762711865e-06,
      "loss": 45.5199,
      "step": 161
    },
    {
      "epoch": 0.0858050847457627,
      "grad_norm": 15.107317924499512,
      "learning_rate": 9.141949152542374e-06,
      "loss": 44.0625,
      "step": 162
    },
    {
      "epoch": 0.08633474576271187,
      "grad_norm": 13.02063274383545,
      "learning_rate": 9.136652542372883e-06,
      "loss": 44.7377,
      "step": 163
    },
    {
      "epoch": 0.08686440677966102,
      "grad_norm": 15.182476043701172,
      "learning_rate": 9.131355932203391e-06,
      "loss": 44.689,
      "step": 164
    },
    {
      "epoch": 0.08739406779661017,
      "grad_norm": 14.193438529968262,
      "learning_rate": 9.126059322033898e-06,
      "loss": 43.9248,
      "step": 165
    },
    {
      "epoch": 0.08792372881355932,
      "grad_norm": 12.928723335266113,
      "learning_rate": 9.120762711864407e-06,
      "loss": 42.0982,
      "step": 166
    },
    {
      "epoch": 0.08845338983050847,
      "grad_norm": 17.051420211791992,
      "learning_rate": 9.115466101694916e-06,
      "loss": 45.8974,
      "step": 167
    },
    {
      "epoch": 0.08898305084745763,
      "grad_norm": 14.144514083862305,
      "learning_rate": 9.110169491525425e-06,
      "loss": 43.093,
      "step": 168
    },
    {
      "epoch": 0.08951271186440678,
      "grad_norm": 15.755730628967285,
      "learning_rate": 9.104872881355933e-06,
      "loss": 46.8026,
      "step": 169
    },
    {
      "epoch": 0.09004237288135593,
      "grad_norm": 15.752670288085938,
      "learning_rate": 9.09957627118644e-06,
      "loss": 42.942,
      "step": 170
    },
    {
      "epoch": 0.09057203389830508,
      "grad_norm": 17.557292938232422,
      "learning_rate": 9.094279661016951e-06,
      "loss": 45.1299,
      "step": 171
    },
    {
      "epoch": 0.09110169491525423,
      "grad_norm": 17.129390716552734,
      "learning_rate": 9.088983050847458e-06,
      "loss": 44.5567,
      "step": 172
    },
    {
      "epoch": 0.0916313559322034,
      "grad_norm": 13.517969131469727,
      "learning_rate": 9.083686440677967e-06,
      "loss": 45.0957,
      "step": 173
    },
    {
      "epoch": 0.09216101694915255,
      "grad_norm": 20.450660705566406,
      "learning_rate": 9.078389830508476e-06,
      "loss": 44.2672,
      "step": 174
    },
    {
      "epoch": 0.0926906779661017,
      "grad_norm": 15.442261695861816,
      "learning_rate": 9.073093220338984e-06,
      "loss": 44.9288,
      "step": 175
    },
    {
      "epoch": 0.09322033898305085,
      "grad_norm": 18.71314811706543,
      "learning_rate": 9.067796610169493e-06,
      "loss": 44.1856,
      "step": 176
    },
    {
      "epoch": 0.09375,
      "grad_norm": 14.060361862182617,
      "learning_rate": 9.0625e-06,
      "loss": 43.5869,
      "step": 177
    },
    {
      "epoch": 0.09427966101694915,
      "grad_norm": 16.31285285949707,
      "learning_rate": 9.057203389830509e-06,
      "loss": 42.7866,
      "step": 178
    },
    {
      "epoch": 0.0948093220338983,
      "grad_norm": 15.066601753234863,
      "learning_rate": 9.051906779661018e-06,
      "loss": 44.7566,
      "step": 179
    },
    {
      "epoch": 0.09533898305084745,
      "grad_norm": 13.873777389526367,
      "learning_rate": 9.046610169491526e-06,
      "loss": 43.5197,
      "step": 180
    },
    {
      "epoch": 0.0958686440677966,
      "grad_norm": 16.273666381835938,
      "learning_rate": 9.041313559322035e-06,
      "loss": 41.174,
      "step": 181
    },
    {
      "epoch": 0.09639830508474577,
      "grad_norm": 16.049724578857422,
      "learning_rate": 9.036016949152542e-06,
      "loss": 42.0004,
      "step": 182
    },
    {
      "epoch": 0.09692796610169492,
      "grad_norm": 19.54194450378418,
      "learning_rate": 9.03072033898305e-06,
      "loss": 45.6732,
      "step": 183
    },
    {
      "epoch": 0.09745762711864407,
      "grad_norm": 18.0690860748291,
      "learning_rate": 9.02542372881356e-06,
      "loss": 44.5665,
      "step": 184
    },
    {
      "epoch": 0.09798728813559322,
      "grad_norm": 14.092813491821289,
      "learning_rate": 9.020127118644068e-06,
      "loss": 44.908,
      "step": 185
    },
    {
      "epoch": 0.09851694915254237,
      "grad_norm": 15.447066307067871,
      "learning_rate": 9.014830508474577e-06,
      "loss": 42.7003,
      "step": 186
    },
    {
      "epoch": 0.09904661016949153,
      "grad_norm": 16.527538299560547,
      "learning_rate": 9.009533898305086e-06,
      "loss": 45.2748,
      "step": 187
    },
    {
      "epoch": 0.09957627118644068,
      "grad_norm": 18.911373138427734,
      "learning_rate": 9.004237288135595e-06,
      "loss": 45.126,
      "step": 188
    },
    {
      "epoch": 0.10010593220338983,
      "grad_norm": 15.269401550292969,
      "learning_rate": 8.998940677966103e-06,
      "loss": 43.053,
      "step": 189
    },
    {
      "epoch": 0.10063559322033898,
      "grad_norm": 14.951906204223633,
      "learning_rate": 8.99364406779661e-06,
      "loss": 42.9235,
      "step": 190
    },
    {
      "epoch": 0.10116525423728813,
      "grad_norm": 14.439262390136719,
      "learning_rate": 8.988347457627119e-06,
      "loss": 43.185,
      "step": 191
    },
    {
      "epoch": 0.1016949152542373,
      "grad_norm": 22.62448501586914,
      "learning_rate": 8.983050847457628e-06,
      "loss": 44.5611,
      "step": 192
    },
    {
      "epoch": 0.10222457627118645,
      "grad_norm": 15.886744499206543,
      "learning_rate": 8.977754237288137e-06,
      "loss": 43.0422,
      "step": 193
    },
    {
      "epoch": 0.1027542372881356,
      "grad_norm": 16.342416763305664,
      "learning_rate": 8.972457627118645e-06,
      "loss": 43.4564,
      "step": 194
    },
    {
      "epoch": 0.10328389830508475,
      "grad_norm": 14.021316528320312,
      "learning_rate": 8.967161016949152e-06,
      "loss": 41.3918,
      "step": 195
    },
    {
      "epoch": 0.1038135593220339,
      "grad_norm": 15.82027530670166,
      "learning_rate": 8.961864406779663e-06,
      "loss": 42.5995,
      "step": 196
    },
    {
      "epoch": 0.10434322033898305,
      "grad_norm": 15.34825325012207,
      "learning_rate": 8.95656779661017e-06,
      "loss": 43.771,
      "step": 197
    },
    {
      "epoch": 0.1048728813559322,
      "grad_norm": 15.0744047164917,
      "learning_rate": 8.951271186440679e-06,
      "loss": 42.5319,
      "step": 198
    },
    {
      "epoch": 0.10540254237288135,
      "grad_norm": 17.18386459350586,
      "learning_rate": 8.945974576271187e-06,
      "loss": 43.7582,
      "step": 199
    },
    {
      "epoch": 0.1059322033898305,
      "grad_norm": 15.976750373840332,
      "learning_rate": 8.940677966101694e-06,
      "loss": 43.1739,
      "step": 200
    },
    {
      "epoch": 0.10646186440677965,
      "grad_norm": 13.061029434204102,
      "learning_rate": 8.935381355932205e-06,
      "loss": 41.4326,
      "step": 201
    },
    {
      "epoch": 0.10699152542372882,
      "grad_norm": 13.137149810791016,
      "learning_rate": 8.930084745762712e-06,
      "loss": 40.966,
      "step": 202
    },
    {
      "epoch": 0.10752118644067797,
      "grad_norm": 12.398611068725586,
      "learning_rate": 8.92478813559322e-06,
      "loss": 41.9912,
      "step": 203
    },
    {
      "epoch": 0.10805084745762712,
      "grad_norm": 14.261005401611328,
      "learning_rate": 8.91949152542373e-06,
      "loss": 39.9012,
      "step": 204
    },
    {
      "epoch": 0.10858050847457627,
      "grad_norm": 15.67331600189209,
      "learning_rate": 8.914194915254238e-06,
      "loss": 42.8413,
      "step": 205
    },
    {
      "epoch": 0.10911016949152542,
      "grad_norm": 16.921180725097656,
      "learning_rate": 8.908898305084747e-06,
      "loss": 43.4836,
      "step": 206
    },
    {
      "epoch": 0.10963983050847458,
      "grad_norm": 15.534255981445312,
      "learning_rate": 8.903601694915254e-06,
      "loss": 41.3274,
      "step": 207
    },
    {
      "epoch": 0.11016949152542373,
      "grad_norm": 16.455135345458984,
      "learning_rate": 8.898305084745763e-06,
      "loss": 44.1427,
      "step": 208
    },
    {
      "epoch": 0.11069915254237288,
      "grad_norm": 14.592940330505371,
      "learning_rate": 8.893008474576273e-06,
      "loss": 42.9283,
      "step": 209
    },
    {
      "epoch": 0.11122881355932203,
      "grad_norm": 25.936908721923828,
      "learning_rate": 8.88771186440678e-06,
      "loss": 44.6859,
      "step": 210
    },
    {
      "epoch": 0.11175847457627118,
      "grad_norm": 15.597710609436035,
      "learning_rate": 8.882415254237289e-06,
      "loss": 43.3808,
      "step": 211
    },
    {
      "epoch": 0.11228813559322035,
      "grad_norm": 11.907612800598145,
      "learning_rate": 8.877118644067798e-06,
      "loss": 39.6498,
      "step": 212
    },
    {
      "epoch": 0.1128177966101695,
      "grad_norm": 15.054706573486328,
      "learning_rate": 8.871822033898307e-06,
      "loss": 41.3892,
      "step": 213
    },
    {
      "epoch": 0.11334745762711865,
      "grad_norm": 16.06768226623535,
      "learning_rate": 8.866525423728815e-06,
      "loss": 42.1037,
      "step": 214
    },
    {
      "epoch": 0.1138771186440678,
      "grad_norm": 16.24547576904297,
      "learning_rate": 8.861228813559322e-06,
      "loss": 43.2085,
      "step": 215
    },
    {
      "epoch": 0.11440677966101695,
      "grad_norm": 13.186929702758789,
      "learning_rate": 8.855932203389831e-06,
      "loss": 41.0925,
      "step": 216
    },
    {
      "epoch": 0.1149364406779661,
      "grad_norm": 15.12317180633545,
      "learning_rate": 8.85063559322034e-06,
      "loss": 42.2887,
      "step": 217
    },
    {
      "epoch": 0.11546610169491525,
      "grad_norm": 18.256319046020508,
      "learning_rate": 8.845338983050849e-06,
      "loss": 43.8898,
      "step": 218
    },
    {
      "epoch": 0.1159957627118644,
      "grad_norm": 19.571659088134766,
      "learning_rate": 8.840042372881357e-06,
      "loss": 43.4883,
      "step": 219
    },
    {
      "epoch": 0.11652542372881355,
      "grad_norm": 17.962724685668945,
      "learning_rate": 8.834745762711864e-06,
      "loss": 42.0768,
      "step": 220
    },
    {
      "epoch": 0.1170550847457627,
      "grad_norm": 14.048063278198242,
      "learning_rate": 8.829449152542373e-06,
      "loss": 41.6314,
      "step": 221
    },
    {
      "epoch": 0.11758474576271187,
      "grad_norm": 18.41710662841797,
      "learning_rate": 8.824152542372882e-06,
      "loss": 42.9001,
      "step": 222
    },
    {
      "epoch": 0.11811440677966102,
      "grad_norm": 13.716641426086426,
      "learning_rate": 8.81885593220339e-06,
      "loss": 40.0575,
      "step": 223
    },
    {
      "epoch": 0.11864406779661017,
      "grad_norm": 15.040282249450684,
      "learning_rate": 8.8135593220339e-06,
      "loss": 40.8923,
      "step": 224
    },
    {
      "epoch": 0.11917372881355932,
      "grad_norm": 16.52912139892578,
      "learning_rate": 8.808262711864406e-06,
      "loss": 42.1481,
      "step": 225
    },
    {
      "epoch": 0.11970338983050847,
      "grad_norm": 13.509419441223145,
      "learning_rate": 8.802966101694917e-06,
      "loss": 39.7799,
      "step": 226
    },
    {
      "epoch": 0.12023305084745763,
      "grad_norm": 17.121986389160156,
      "learning_rate": 8.797669491525424e-06,
      "loss": 42.044,
      "step": 227
    },
    {
      "epoch": 0.12076271186440678,
      "grad_norm": 15.027372360229492,
      "learning_rate": 8.792372881355933e-06,
      "loss": 39.8918,
      "step": 228
    },
    {
      "epoch": 0.12129237288135593,
      "grad_norm": 14.092366218566895,
      "learning_rate": 8.787076271186441e-06,
      "loss": 42.3522,
      "step": 229
    },
    {
      "epoch": 0.12182203389830508,
      "grad_norm": 24.92057991027832,
      "learning_rate": 8.78177966101695e-06,
      "loss": 43.036,
      "step": 230
    },
    {
      "epoch": 0.12235169491525423,
      "grad_norm": 15.43857479095459,
      "learning_rate": 8.776483050847459e-06,
      "loss": 43.7397,
      "step": 231
    },
    {
      "epoch": 0.1228813559322034,
      "grad_norm": 19.818382263183594,
      "learning_rate": 8.771186440677966e-06,
      "loss": 43.5315,
      "step": 232
    },
    {
      "epoch": 0.12341101694915255,
      "grad_norm": 14.703324317932129,
      "learning_rate": 8.765889830508475e-06,
      "loss": 41.1778,
      "step": 233
    },
    {
      "epoch": 0.1239406779661017,
      "grad_norm": 14.151469230651855,
      "learning_rate": 8.760593220338985e-06,
      "loss": 39.993,
      "step": 234
    },
    {
      "epoch": 0.12447033898305085,
      "grad_norm": 12.474395751953125,
      "learning_rate": 8.755296610169492e-06,
      "loss": 39.9198,
      "step": 235
    },
    {
      "epoch": 0.125,
      "grad_norm": 16.170940399169922,
      "learning_rate": 8.750000000000001e-06,
      "loss": 44.5931,
      "step": 236
    },
    {
      "epoch": 0.12552966101694915,
      "grad_norm": 14.413511276245117,
      "learning_rate": 8.74470338983051e-06,
      "loss": 41.0592,
      "step": 237
    },
    {
      "epoch": 0.1260593220338983,
      "grad_norm": 14.989477157592773,
      "learning_rate": 8.739406779661017e-06,
      "loss": 40.9657,
      "step": 238
    },
    {
      "epoch": 0.12658898305084745,
      "grad_norm": 16.371431350708008,
      "learning_rate": 8.734110169491527e-06,
      "loss": 41.3418,
      "step": 239
    },
    {
      "epoch": 0.1271186440677966,
      "grad_norm": 14.802279472351074,
      "learning_rate": 8.728813559322034e-06,
      "loss": 42.5882,
      "step": 240
    },
    {
      "epoch": 0.12764830508474576,
      "grad_norm": 20.02827262878418,
      "learning_rate": 8.723516949152543e-06,
      "loss": 42.8217,
      "step": 241
    },
    {
      "epoch": 0.1281779661016949,
      "grad_norm": 11.373907089233398,
      "learning_rate": 8.718220338983052e-06,
      "loss": 39.8341,
      "step": 242
    },
    {
      "epoch": 0.12870762711864406,
      "grad_norm": 14.20452880859375,
      "learning_rate": 8.71292372881356e-06,
      "loss": 40.0912,
      "step": 243
    },
    {
      "epoch": 0.1292372881355932,
      "grad_norm": 14.933534622192383,
      "learning_rate": 8.70762711864407e-06,
      "loss": 40.9392,
      "step": 244
    },
    {
      "epoch": 0.12976694915254236,
      "grad_norm": 14.053047180175781,
      "learning_rate": 8.702330508474576e-06,
      "loss": 39.7796,
      "step": 245
    },
    {
      "epoch": 0.13029661016949154,
      "grad_norm": 17.01775550842285,
      "learning_rate": 8.697033898305085e-06,
      "loss": 41.7197,
      "step": 246
    },
    {
      "epoch": 0.1308262711864407,
      "grad_norm": 18.08636474609375,
      "learning_rate": 8.691737288135594e-06,
      "loss": 42.9922,
      "step": 247
    },
    {
      "epoch": 0.13135593220338984,
      "grad_norm": 17.077959060668945,
      "learning_rate": 8.686440677966103e-06,
      "loss": 40.7556,
      "step": 248
    },
    {
      "epoch": 0.131885593220339,
      "grad_norm": 15.379338264465332,
      "learning_rate": 8.681144067796611e-06,
      "loss": 40.314,
      "step": 249
    },
    {
      "epoch": 0.13241525423728814,
      "grad_norm": 15.305740356445312,
      "learning_rate": 8.675847457627118e-06,
      "loss": 41.3682,
      "step": 250
    },
    {
      "epoch": 0.1329449152542373,
      "grad_norm": 17.093536376953125,
      "learning_rate": 8.670550847457629e-06,
      "loss": 41.907,
      "step": 251
    },
    {
      "epoch": 0.13347457627118645,
      "grad_norm": 16.537490844726562,
      "learning_rate": 8.665254237288136e-06,
      "loss": 41.7812,
      "step": 252
    },
    {
      "epoch": 0.1340042372881356,
      "grad_norm": 16.04497528076172,
      "learning_rate": 8.659957627118645e-06,
      "loss": 40.169,
      "step": 253
    },
    {
      "epoch": 0.13453389830508475,
      "grad_norm": 19.32991600036621,
      "learning_rate": 8.654661016949153e-06,
      "loss": 42.1785,
      "step": 254
    },
    {
      "epoch": 0.1350635593220339,
      "grad_norm": 17.907474517822266,
      "learning_rate": 8.649364406779662e-06,
      "loss": 43.8972,
      "step": 255
    },
    {
      "epoch": 0.13559322033898305,
      "grad_norm": 17.56587028503418,
      "learning_rate": 8.64406779661017e-06,
      "loss": 39.6362,
      "step": 256
    },
    {
      "epoch": 0.1361228813559322,
      "grad_norm": 17.579334259033203,
      "learning_rate": 8.638771186440678e-06,
      "loss": 42.3221,
      "step": 257
    },
    {
      "epoch": 0.13665254237288135,
      "grad_norm": 13.46346664428711,
      "learning_rate": 8.633474576271187e-06,
      "loss": 40.4565,
      "step": 258
    },
    {
      "epoch": 0.1371822033898305,
      "grad_norm": 15.181448936462402,
      "learning_rate": 8.628177966101695e-06,
      "loss": 40.8789,
      "step": 259
    },
    {
      "epoch": 0.13771186440677965,
      "grad_norm": 20.460641860961914,
      "learning_rate": 8.622881355932204e-06,
      "loss": 42.2112,
      "step": 260
    },
    {
      "epoch": 0.1382415254237288,
      "grad_norm": 14.17595386505127,
      "learning_rate": 8.617584745762713e-06,
      "loss": 39.8262,
      "step": 261
    },
    {
      "epoch": 0.13877118644067796,
      "grad_norm": 17.116777420043945,
      "learning_rate": 8.612288135593222e-06,
      "loss": 40.5341,
      "step": 262
    },
    {
      "epoch": 0.1393008474576271,
      "grad_norm": 16.0313720703125,
      "learning_rate": 8.606991525423729e-06,
      "loss": 40.3477,
      "step": 263
    },
    {
      "epoch": 0.13983050847457626,
      "grad_norm": 14.411069869995117,
      "learning_rate": 8.601694915254239e-06,
      "loss": 41.7851,
      "step": 264
    },
    {
      "epoch": 0.1403601694915254,
      "grad_norm": 14.982532501220703,
      "learning_rate": 8.596398305084746e-06,
      "loss": 38.2329,
      "step": 265
    },
    {
      "epoch": 0.1408898305084746,
      "grad_norm": 18.28425407409668,
      "learning_rate": 8.591101694915255e-06,
      "loss": 42.4722,
      "step": 266
    },
    {
      "epoch": 0.14141949152542374,
      "grad_norm": 12.853660583496094,
      "learning_rate": 8.585805084745764e-06,
      "loss": 39.2305,
      "step": 267
    },
    {
      "epoch": 0.1419491525423729,
      "grad_norm": 21.43488121032715,
      "learning_rate": 8.580508474576272e-06,
      "loss": 42.7929,
      "step": 268
    },
    {
      "epoch": 0.14247881355932204,
      "grad_norm": 16.01351547241211,
      "learning_rate": 8.575211864406781e-06,
      "loss": 42.2796,
      "step": 269
    },
    {
      "epoch": 0.1430084745762712,
      "grad_norm": 14.995550155639648,
      "learning_rate": 8.569915254237288e-06,
      "loss": 40.5965,
      "step": 270
    },
    {
      "epoch": 0.14353813559322035,
      "grad_norm": 15.351802825927734,
      "learning_rate": 8.564618644067797e-06,
      "loss": 39.1034,
      "step": 271
    },
    {
      "epoch": 0.1440677966101695,
      "grad_norm": 14.207134246826172,
      "learning_rate": 8.559322033898306e-06,
      "loss": 39.0523,
      "step": 272
    },
    {
      "epoch": 0.14459745762711865,
      "grad_norm": 18.96088981628418,
      "learning_rate": 8.554025423728814e-06,
      "loss": 40.9693,
      "step": 273
    },
    {
      "epoch": 0.1451271186440678,
      "grad_norm": 16.84577178955078,
      "learning_rate": 8.548728813559323e-06,
      "loss": 40.266,
      "step": 274
    },
    {
      "epoch": 0.14565677966101695,
      "grad_norm": 18.967397689819336,
      "learning_rate": 8.54343220338983e-06,
      "loss": 40.45,
      "step": 275
    },
    {
      "epoch": 0.1461864406779661,
      "grad_norm": 15.440354347229004,
      "learning_rate": 8.538135593220339e-06,
      "loss": 41.4072,
      "step": 276
    },
    {
      "epoch": 0.14671610169491525,
      "grad_norm": 15.704645156860352,
      "learning_rate": 8.532838983050848e-06,
      "loss": 40.4637,
      "step": 277
    },
    {
      "epoch": 0.1472457627118644,
      "grad_norm": 15.307123184204102,
      "learning_rate": 8.527542372881356e-06,
      "loss": 39.0186,
      "step": 278
    },
    {
      "epoch": 0.14777542372881355,
      "grad_norm": 12.578743934631348,
      "learning_rate": 8.522245762711865e-06,
      "loss": 40.2655,
      "step": 279
    },
    {
      "epoch": 0.1483050847457627,
      "grad_norm": 16.67361068725586,
      "learning_rate": 8.516949152542372e-06,
      "loss": 40.8645,
      "step": 280
    },
    {
      "epoch": 0.14883474576271186,
      "grad_norm": 17.23623275756836,
      "learning_rate": 8.511652542372883e-06,
      "loss": 41.1118,
      "step": 281
    },
    {
      "epoch": 0.149364406779661,
      "grad_norm": 13.876092910766602,
      "learning_rate": 8.50635593220339e-06,
      "loss": 38.5299,
      "step": 282
    },
    {
      "epoch": 0.14989406779661016,
      "grad_norm": 17.075061798095703,
      "learning_rate": 8.501059322033899e-06,
      "loss": 40.8503,
      "step": 283
    },
    {
      "epoch": 0.1504237288135593,
      "grad_norm": 16.618202209472656,
      "learning_rate": 8.495762711864407e-06,
      "loss": 38.7768,
      "step": 284
    },
    {
      "epoch": 0.15095338983050846,
      "grad_norm": 20.09058380126953,
      "learning_rate": 8.490466101694916e-06,
      "loss": 43.2695,
      "step": 285
    },
    {
      "epoch": 0.15148305084745764,
      "grad_norm": 16.622262954711914,
      "learning_rate": 8.485169491525425e-06,
      "loss": 38.7524,
      "step": 286
    },
    {
      "epoch": 0.1520127118644068,
      "grad_norm": 20.795183181762695,
      "learning_rate": 8.479872881355934e-06,
      "loss": 42.7362,
      "step": 287
    },
    {
      "epoch": 0.15254237288135594,
      "grad_norm": 15.230008125305176,
      "learning_rate": 8.47457627118644e-06,
      "loss": 38.6891,
      "step": 288
    },
    {
      "epoch": 0.1530720338983051,
      "grad_norm": 17.610427856445312,
      "learning_rate": 8.469279661016951e-06,
      "loss": 39.1889,
      "step": 289
    },
    {
      "epoch": 0.15360169491525424,
      "grad_norm": 17.195369720458984,
      "learning_rate": 8.463983050847458e-06,
      "loss": 38.4673,
      "step": 290
    },
    {
      "epoch": 0.1541313559322034,
      "grad_norm": 14.823731422424316,
      "learning_rate": 8.458686440677967e-06,
      "loss": 41.5085,
      "step": 291
    },
    {
      "epoch": 0.15466101694915255,
      "grad_norm": 16.83893585205078,
      "learning_rate": 8.453389830508476e-06,
      "loss": 40.0534,
      "step": 292
    },
    {
      "epoch": 0.1551906779661017,
      "grad_norm": 16.137813568115234,
      "learning_rate": 8.448093220338984e-06,
      "loss": 41.3277,
      "step": 293
    },
    {
      "epoch": 0.15572033898305085,
      "grad_norm": 17.45763397216797,
      "learning_rate": 8.442796610169493e-06,
      "loss": 40.7325,
      "step": 294
    },
    {
      "epoch": 0.15625,
      "grad_norm": 13.673721313476562,
      "learning_rate": 8.4375e-06,
      "loss": 40.7895,
      "step": 295
    },
    {
      "epoch": 0.15677966101694915,
      "grad_norm": 18.841793060302734,
      "learning_rate": 8.432203389830509e-06,
      "loss": 39.9472,
      "step": 296
    },
    {
      "epoch": 0.1573093220338983,
      "grad_norm": 14.98412799835205,
      "learning_rate": 8.426906779661018e-06,
      "loss": 41.7587,
      "step": 297
    },
    {
      "epoch": 0.15783898305084745,
      "grad_norm": 13.28298568725586,
      "learning_rate": 8.421610169491526e-06,
      "loss": 39.1576,
      "step": 298
    },
    {
      "epoch": 0.1583686440677966,
      "grad_norm": 14.122819900512695,
      "learning_rate": 8.416313559322035e-06,
      "loss": 38.6992,
      "step": 299
    },
    {
      "epoch": 0.15889830508474576,
      "grad_norm": 15.145340919494629,
      "learning_rate": 8.411016949152542e-06,
      "loss": 40.0498,
      "step": 300
    },
    {
      "epoch": 0.1594279661016949,
      "grad_norm": 19.84711265563965,
      "learning_rate": 8.405720338983051e-06,
      "loss": 40.457,
      "step": 301
    },
    {
      "epoch": 0.15995762711864406,
      "grad_norm": 13.758794784545898,
      "learning_rate": 8.40042372881356e-06,
      "loss": 40.1351,
      "step": 302
    },
    {
      "epoch": 0.1604872881355932,
      "grad_norm": 13.423484802246094,
      "learning_rate": 8.395127118644068e-06,
      "loss": 38.8103,
      "step": 303
    },
    {
      "epoch": 0.16101694915254236,
      "grad_norm": 15.985000610351562,
      "learning_rate": 8.389830508474577e-06,
      "loss": 40.3776,
      "step": 304
    },
    {
      "epoch": 0.16154661016949154,
      "grad_norm": 19.32046127319336,
      "learning_rate": 8.384533898305084e-06,
      "loss": 39.9468,
      "step": 305
    },
    {
      "epoch": 0.1620762711864407,
      "grad_norm": 15.089706420898438,
      "learning_rate": 8.379237288135595e-06,
      "loss": 38.7383,
      "step": 306
    },
    {
      "epoch": 0.16260593220338984,
      "grad_norm": 17.46820831298828,
      "learning_rate": 8.373940677966103e-06,
      "loss": 39.6276,
      "step": 307
    },
    {
      "epoch": 0.163135593220339,
      "grad_norm": 18.25245475769043,
      "learning_rate": 8.36864406779661e-06,
      "loss": 39.8369,
      "step": 308
    },
    {
      "epoch": 0.16366525423728814,
      "grad_norm": 18.659257888793945,
      "learning_rate": 8.36334745762712e-06,
      "loss": 41.0589,
      "step": 309
    },
    {
      "epoch": 0.1641949152542373,
      "grad_norm": 18.425472259521484,
      "learning_rate": 8.358050847457628e-06,
      "loss": 40.1189,
      "step": 310
    },
    {
      "epoch": 0.16472457627118645,
      "grad_norm": 12.341487884521484,
      "learning_rate": 8.352754237288137e-06,
      "loss": 36.8067,
      "step": 311
    },
    {
      "epoch": 0.1652542372881356,
      "grad_norm": 17.14761734008789,
      "learning_rate": 8.347457627118645e-06,
      "loss": 39.2866,
      "step": 312
    },
    {
      "epoch": 0.16578389830508475,
      "grad_norm": 17.003276824951172,
      "learning_rate": 8.342161016949152e-06,
      "loss": 39.306,
      "step": 313
    },
    {
      "epoch": 0.1663135593220339,
      "grad_norm": 13.784782409667969,
      "learning_rate": 8.336864406779663e-06,
      "loss": 39.112,
      "step": 314
    },
    {
      "epoch": 0.16684322033898305,
      "grad_norm": 20.50088882446289,
      "learning_rate": 8.33156779661017e-06,
      "loss": 40.6695,
      "step": 315
    },
    {
      "epoch": 0.1673728813559322,
      "grad_norm": 16.98780059814453,
      "learning_rate": 8.326271186440679e-06,
      "loss": 38.9496,
      "step": 316
    },
    {
      "epoch": 0.16790254237288135,
      "grad_norm": 17.2318058013916,
      "learning_rate": 8.320974576271187e-06,
      "loss": 38.484,
      "step": 317
    },
    {
      "epoch": 0.1684322033898305,
      "grad_norm": 16.785367965698242,
      "learning_rate": 8.315677966101695e-06,
      "loss": 39.0766,
      "step": 318
    },
    {
      "epoch": 0.16896186440677965,
      "grad_norm": 18.396791458129883,
      "learning_rate": 8.310381355932205e-06,
      "loss": 40.2516,
      "step": 319
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 15.683804512023926,
      "learning_rate": 8.305084745762712e-06,
      "loss": 39.1094,
      "step": 320
    },
    {
      "epoch": 0.17002118644067796,
      "grad_norm": 14.648982048034668,
      "learning_rate": 8.29978813559322e-06,
      "loss": 37.7653,
      "step": 321
    },
    {
      "epoch": 0.1705508474576271,
      "grad_norm": 16.9040584564209,
      "learning_rate": 8.29449152542373e-06,
      "loss": 40.0507,
      "step": 322
    },
    {
      "epoch": 0.17108050847457626,
      "grad_norm": 15.456400871276855,
      "learning_rate": 8.289194915254238e-06,
      "loss": 38.262,
      "step": 323
    },
    {
      "epoch": 0.1716101694915254,
      "grad_norm": 10.87707805633545,
      "learning_rate": 8.283898305084747e-06,
      "loss": 37.0066,
      "step": 324
    },
    {
      "epoch": 0.1721398305084746,
      "grad_norm": 14.249225616455078,
      "learning_rate": 8.278601694915254e-06,
      "loss": 38.8624,
      "step": 325
    },
    {
      "epoch": 0.17266949152542374,
      "grad_norm": 15.313895225524902,
      "learning_rate": 8.273305084745763e-06,
      "loss": 37.758,
      "step": 326
    },
    {
      "epoch": 0.1731991525423729,
      "grad_norm": 16.406631469726562,
      "learning_rate": 8.268008474576272e-06,
      "loss": 38.9996,
      "step": 327
    },
    {
      "epoch": 0.17372881355932204,
      "grad_norm": 15.323809623718262,
      "learning_rate": 8.26271186440678e-06,
      "loss": 36.711,
      "step": 328
    },
    {
      "epoch": 0.1742584745762712,
      "grad_norm": 14.668671607971191,
      "learning_rate": 8.257415254237289e-06,
      "loss": 37.4566,
      "step": 329
    },
    {
      "epoch": 0.17478813559322035,
      "grad_norm": 14.281198501586914,
      "learning_rate": 8.252118644067796e-06,
      "loss": 39.0694,
      "step": 330
    },
    {
      "epoch": 0.1753177966101695,
      "grad_norm": 14.440709114074707,
      "learning_rate": 8.246822033898307e-06,
      "loss": 39.3345,
      "step": 331
    },
    {
      "epoch": 0.17584745762711865,
      "grad_norm": 13.759039878845215,
      "learning_rate": 8.241525423728815e-06,
      "loss": 38.3043,
      "step": 332
    },
    {
      "epoch": 0.1763771186440678,
      "grad_norm": 17.43242835998535,
      "learning_rate": 8.236228813559322e-06,
      "loss": 38.4436,
      "step": 333
    },
    {
      "epoch": 0.17690677966101695,
      "grad_norm": 13.69066333770752,
      "learning_rate": 8.230932203389831e-06,
      "loss": 38.5703,
      "step": 334
    },
    {
      "epoch": 0.1774364406779661,
      "grad_norm": 16.85468101501465,
      "learning_rate": 8.22563559322034e-06,
      "loss": 39.8044,
      "step": 335
    },
    {
      "epoch": 0.17796610169491525,
      "grad_norm": 16.32362174987793,
      "learning_rate": 8.220338983050849e-06,
      "loss": 40.2409,
      "step": 336
    },
    {
      "epoch": 0.1784957627118644,
      "grad_norm": 18.045198440551758,
      "learning_rate": 8.215042372881357e-06,
      "loss": 39.6905,
      "step": 337
    },
    {
      "epoch": 0.17902542372881355,
      "grad_norm": 16.43853759765625,
      "learning_rate": 8.209745762711864e-06,
      "loss": 41.5248,
      "step": 338
    },
    {
      "epoch": 0.1795550847457627,
      "grad_norm": 17.157817840576172,
      "learning_rate": 8.204449152542373e-06,
      "loss": 37.9985,
      "step": 339
    },
    {
      "epoch": 0.18008474576271186,
      "grad_norm": 15.721170425415039,
      "learning_rate": 8.199152542372882e-06,
      "loss": 37.3005,
      "step": 340
    },
    {
      "epoch": 0.180614406779661,
      "grad_norm": 16.326520919799805,
      "learning_rate": 8.19385593220339e-06,
      "loss": 38.3742,
      "step": 341
    },
    {
      "epoch": 0.18114406779661016,
      "grad_norm": 17.589359283447266,
      "learning_rate": 8.1885593220339e-06,
      "loss": 39.0762,
      "step": 342
    },
    {
      "epoch": 0.1816737288135593,
      "grad_norm": 14.245378494262695,
      "learning_rate": 8.183262711864406e-06,
      "loss": 38.5974,
      "step": 343
    },
    {
      "epoch": 0.18220338983050846,
      "grad_norm": 20.359603881835938,
      "learning_rate": 8.177966101694917e-06,
      "loss": 38.755,
      "step": 344
    },
    {
      "epoch": 0.18273305084745764,
      "grad_norm": 15.341033935546875,
      "learning_rate": 8.172669491525424e-06,
      "loss": 38.2094,
      "step": 345
    },
    {
      "epoch": 0.1832627118644068,
      "grad_norm": 10.67183780670166,
      "learning_rate": 8.167372881355933e-06,
      "loss": 36.6774,
      "step": 346
    },
    {
      "epoch": 0.18379237288135594,
      "grad_norm": 16.99692153930664,
      "learning_rate": 8.162076271186441e-06,
      "loss": 38.24,
      "step": 347
    },
    {
      "epoch": 0.1843220338983051,
      "grad_norm": 13.437159538269043,
      "learning_rate": 8.15677966101695e-06,
      "loss": 38.2525,
      "step": 348
    },
    {
      "epoch": 0.18485169491525424,
      "grad_norm": 14.487502098083496,
      "learning_rate": 8.151483050847459e-06,
      "loss": 38.2376,
      "step": 349
    },
    {
      "epoch": 0.1853813559322034,
      "grad_norm": 17.539766311645508,
      "learning_rate": 8.146186440677966e-06,
      "loss": 41.1735,
      "step": 350
    },
    {
      "epoch": 0.18591101694915255,
      "grad_norm": 14.253143310546875,
      "learning_rate": 8.140889830508475e-06,
      "loss": 38.3221,
      "step": 351
    },
    {
      "epoch": 0.1864406779661017,
      "grad_norm": 14.686485290527344,
      "learning_rate": 8.135593220338983e-06,
      "loss": 37.7646,
      "step": 352
    },
    {
      "epoch": 0.18697033898305085,
      "grad_norm": 14.070266723632812,
      "learning_rate": 8.130296610169492e-06,
      "loss": 36.7814,
      "step": 353
    },
    {
      "epoch": 0.1875,
      "grad_norm": 14.59591293334961,
      "learning_rate": 8.125000000000001e-06,
      "loss": 37.3094,
      "step": 354
    },
    {
      "epoch": 0.18802966101694915,
      "grad_norm": 16.729726791381836,
      "learning_rate": 8.119703389830508e-06,
      "loss": 37.6762,
      "step": 355
    },
    {
      "epoch": 0.1885593220338983,
      "grad_norm": 14.66527271270752,
      "learning_rate": 8.114406779661017e-06,
      "loss": 37.127,
      "step": 356
    },
    {
      "epoch": 0.18908898305084745,
      "grad_norm": 14.61306095123291,
      "learning_rate": 8.109110169491527e-06,
      "loss": 38.8526,
      "step": 357
    },
    {
      "epoch": 0.1896186440677966,
      "grad_norm": 15.703363418579102,
      "learning_rate": 8.103813559322034e-06,
      "loss": 38.5697,
      "step": 358
    },
    {
      "epoch": 0.19014830508474576,
      "grad_norm": 15.312939643859863,
      "learning_rate": 8.098516949152543e-06,
      "loss": 37.8277,
      "step": 359
    },
    {
      "epoch": 0.1906779661016949,
      "grad_norm": 13.705245971679688,
      "learning_rate": 8.093220338983052e-06,
      "loss": 37.7011,
      "step": 360
    },
    {
      "epoch": 0.19120762711864406,
      "grad_norm": 17.470590591430664,
      "learning_rate": 8.08792372881356e-06,
      "loss": 37.789,
      "step": 361
    },
    {
      "epoch": 0.1917372881355932,
      "grad_norm": 16.023317337036133,
      "learning_rate": 8.08262711864407e-06,
      "loss": 38.0558,
      "step": 362
    },
    {
      "epoch": 0.19226694915254236,
      "grad_norm": 15.067134857177734,
      "learning_rate": 8.077330508474576e-06,
      "loss": 38.6135,
      "step": 363
    },
    {
      "epoch": 0.19279661016949154,
      "grad_norm": 16.421798706054688,
      "learning_rate": 8.072033898305085e-06,
      "loss": 39.6641,
      "step": 364
    },
    {
      "epoch": 0.1933262711864407,
      "grad_norm": 13.289566993713379,
      "learning_rate": 8.066737288135594e-06,
      "loss": 36.7563,
      "step": 365
    },
    {
      "epoch": 0.19385593220338984,
      "grad_norm": 12.283499717712402,
      "learning_rate": 8.061440677966103e-06,
      "loss": 36.4315,
      "step": 366
    },
    {
      "epoch": 0.194385593220339,
      "grad_norm": 14.71926212310791,
      "learning_rate": 8.056144067796611e-06,
      "loss": 38.8644,
      "step": 367
    },
    {
      "epoch": 0.19491525423728814,
      "grad_norm": 13.94066047668457,
      "learning_rate": 8.050847457627118e-06,
      "loss": 38.6081,
      "step": 368
    },
    {
      "epoch": 0.1954449152542373,
      "grad_norm": 16.23077392578125,
      "learning_rate": 8.045550847457629e-06,
      "loss": 39.2155,
      "step": 369
    },
    {
      "epoch": 0.19597457627118645,
      "grad_norm": 12.926231384277344,
      "learning_rate": 8.040254237288136e-06,
      "loss": 36.7572,
      "step": 370
    },
    {
      "epoch": 0.1965042372881356,
      "grad_norm": 15.421478271484375,
      "learning_rate": 8.034957627118645e-06,
      "loss": 36.1574,
      "step": 371
    },
    {
      "epoch": 0.19703389830508475,
      "grad_norm": 16.24416732788086,
      "learning_rate": 8.029661016949153e-06,
      "loss": 37.1773,
      "step": 372
    },
    {
      "epoch": 0.1975635593220339,
      "grad_norm": 12.23160457611084,
      "learning_rate": 8.024364406779662e-06,
      "loss": 36.2246,
      "step": 373
    },
    {
      "epoch": 0.19809322033898305,
      "grad_norm": 12.668546676635742,
      "learning_rate": 8.019067796610171e-06,
      "loss": 35.3108,
      "step": 374
    },
    {
      "epoch": 0.1986228813559322,
      "grad_norm": 16.283843994140625,
      "learning_rate": 8.013771186440678e-06,
      "loss": 36.7857,
      "step": 375
    },
    {
      "epoch": 0.19915254237288135,
      "grad_norm": 15.66490650177002,
      "learning_rate": 8.008474576271187e-06,
      "loss": 39.2099,
      "step": 376
    },
    {
      "epoch": 0.1996822033898305,
      "grad_norm": 15.377089500427246,
      "learning_rate": 8.003177966101695e-06,
      "loss": 37.9667,
      "step": 377
    },
    {
      "epoch": 0.20021186440677965,
      "grad_norm": 13.770482063293457,
      "learning_rate": 7.997881355932204e-06,
      "loss": 37.0291,
      "step": 378
    },
    {
      "epoch": 0.2007415254237288,
      "grad_norm": 13.117777824401855,
      "learning_rate": 7.992584745762713e-06,
      "loss": 37.7153,
      "step": 379
    },
    {
      "epoch": 0.20127118644067796,
      "grad_norm": 13.280060768127441,
      "learning_rate": 7.987288135593222e-06,
      "loss": 38.2426,
      "step": 380
    },
    {
      "epoch": 0.2018008474576271,
      "grad_norm": 14.14598274230957,
      "learning_rate": 7.981991525423729e-06,
      "loss": 37.8936,
      "step": 381
    },
    {
      "epoch": 0.20233050847457626,
      "grad_norm": 14.772379875183105,
      "learning_rate": 7.976694915254239e-06,
      "loss": 38.5434,
      "step": 382
    },
    {
      "epoch": 0.2028601694915254,
      "grad_norm": 14.686677932739258,
      "learning_rate": 7.971398305084746e-06,
      "loss": 37.4939,
      "step": 383
    },
    {
      "epoch": 0.2033898305084746,
      "grad_norm": 14.272186279296875,
      "learning_rate": 7.966101694915255e-06,
      "loss": 35.7427,
      "step": 384
    },
    {
      "epoch": 0.20391949152542374,
      "grad_norm": 18.501901626586914,
      "learning_rate": 7.960805084745764e-06,
      "loss": 37.4882,
      "step": 385
    },
    {
      "epoch": 0.2044491525423729,
      "grad_norm": 20.163265228271484,
      "learning_rate": 7.955508474576272e-06,
      "loss": 37.8782,
      "step": 386
    },
    {
      "epoch": 0.20497881355932204,
      "grad_norm": 15.181340217590332,
      "learning_rate": 7.950211864406781e-06,
      "loss": 37.7643,
      "step": 387
    },
    {
      "epoch": 0.2055084745762712,
      "grad_norm": 14.075495719909668,
      "learning_rate": 7.944915254237288e-06,
      "loss": 37.6469,
      "step": 388
    },
    {
      "epoch": 0.20603813559322035,
      "grad_norm": 14.970874786376953,
      "learning_rate": 7.939618644067797e-06,
      "loss": 37.969,
      "step": 389
    },
    {
      "epoch": 0.2065677966101695,
      "grad_norm": 12.968758583068848,
      "learning_rate": 7.934322033898306e-06,
      "loss": 35.7424,
      "step": 390
    },
    {
      "epoch": 0.20709745762711865,
      "grad_norm": 16.765079498291016,
      "learning_rate": 7.929025423728814e-06,
      "loss": 37.9629,
      "step": 391
    },
    {
      "epoch": 0.2076271186440678,
      "grad_norm": 14.527029037475586,
      "learning_rate": 7.923728813559323e-06,
      "loss": 36.7617,
      "step": 392
    },
    {
      "epoch": 0.20815677966101695,
      "grad_norm": 14.467700004577637,
      "learning_rate": 7.91843220338983e-06,
      "loss": 36.5531,
      "step": 393
    },
    {
      "epoch": 0.2086864406779661,
      "grad_norm": 14.621841430664062,
      "learning_rate": 7.913135593220339e-06,
      "loss": 38.1907,
      "step": 394
    },
    {
      "epoch": 0.20921610169491525,
      "grad_norm": 14.844414710998535,
      "learning_rate": 7.907838983050848e-06,
      "loss": 36.7918,
      "step": 395
    },
    {
      "epoch": 0.2097457627118644,
      "grad_norm": 12.957280158996582,
      "learning_rate": 7.902542372881357e-06,
      "loss": 36.5903,
      "step": 396
    },
    {
      "epoch": 0.21027542372881355,
      "grad_norm": 13.241612434387207,
      "learning_rate": 7.897245762711865e-06,
      "loss": 36.93,
      "step": 397
    },
    {
      "epoch": 0.2108050847457627,
      "grad_norm": 12.306845664978027,
      "learning_rate": 7.891949152542372e-06,
      "loss": 36.174,
      "step": 398
    },
    {
      "epoch": 0.21133474576271186,
      "grad_norm": 14.577982902526855,
      "learning_rate": 7.886652542372883e-06,
      "loss": 36.7113,
      "step": 399
    },
    {
      "epoch": 0.211864406779661,
      "grad_norm": 13.36111068725586,
      "learning_rate": 7.88135593220339e-06,
      "loss": 36.325,
      "step": 400
    },
    {
      "epoch": 0.21239406779661016,
      "grad_norm": 16.200069427490234,
      "learning_rate": 7.876059322033899e-06,
      "loss": 37.1834,
      "step": 401
    },
    {
      "epoch": 0.2129237288135593,
      "grad_norm": 13.555863380432129,
      "learning_rate": 7.870762711864407e-06,
      "loss": 36.2399,
      "step": 402
    },
    {
      "epoch": 0.21345338983050846,
      "grad_norm": 14.421463012695312,
      "learning_rate": 7.865466101694916e-06,
      "loss": 36.285,
      "step": 403
    },
    {
      "epoch": 0.21398305084745764,
      "grad_norm": 16.564464569091797,
      "learning_rate": 7.860169491525425e-06,
      "loss": 37.0492,
      "step": 404
    },
    {
      "epoch": 0.2145127118644068,
      "grad_norm": 17.40530776977539,
      "learning_rate": 7.854872881355934e-06,
      "loss": 38.223,
      "step": 405
    },
    {
      "epoch": 0.21504237288135594,
      "grad_norm": 17.59159278869629,
      "learning_rate": 7.84957627118644e-06,
      "loss": 37.7988,
      "step": 406
    },
    {
      "epoch": 0.2155720338983051,
      "grad_norm": 14.481407165527344,
      "learning_rate": 7.844279661016951e-06,
      "loss": 37.2099,
      "step": 407
    },
    {
      "epoch": 0.21610169491525424,
      "grad_norm": 14.543986320495605,
      "learning_rate": 7.838983050847458e-06,
      "loss": 36.6947,
      "step": 408
    },
    {
      "epoch": 0.2166313559322034,
      "grad_norm": 15.12159252166748,
      "learning_rate": 7.833686440677967e-06,
      "loss": 34.9774,
      "step": 409
    },
    {
      "epoch": 0.21716101694915255,
      "grad_norm": 15.058058738708496,
      "learning_rate": 7.828389830508476e-06,
      "loss": 37.9512,
      "step": 410
    },
    {
      "epoch": 0.2176906779661017,
      "grad_norm": 12.692740440368652,
      "learning_rate": 7.823093220338984e-06,
      "loss": 37.7849,
      "step": 411
    },
    {
      "epoch": 0.21822033898305085,
      "grad_norm": 14.874739646911621,
      "learning_rate": 7.817796610169493e-06,
      "loss": 36.276,
      "step": 412
    },
    {
      "epoch": 0.21875,
      "grad_norm": 10.866676330566406,
      "learning_rate": 7.8125e-06,
      "loss": 35.9967,
      "step": 413
    },
    {
      "epoch": 0.21927966101694915,
      "grad_norm": 13.37695026397705,
      "learning_rate": 7.807203389830509e-06,
      "loss": 37.7138,
      "step": 414
    },
    {
      "epoch": 0.2198093220338983,
      "grad_norm": 18.56627082824707,
      "learning_rate": 7.801906779661018e-06,
      "loss": 37.5726,
      "step": 415
    },
    {
      "epoch": 0.22033898305084745,
      "grad_norm": 14.760334968566895,
      "learning_rate": 7.796610169491526e-06,
      "loss": 38.6441,
      "step": 416
    },
    {
      "epoch": 0.2208686440677966,
      "grad_norm": 15.370715141296387,
      "learning_rate": 7.791313559322035e-06,
      "loss": 35.7603,
      "step": 417
    },
    {
      "epoch": 0.22139830508474576,
      "grad_norm": 13.887012481689453,
      "learning_rate": 7.786016949152542e-06,
      "loss": 36.0982,
      "step": 418
    },
    {
      "epoch": 0.2219279661016949,
      "grad_norm": 11.730676651000977,
      "learning_rate": 7.780720338983051e-06,
      "loss": 35.7375,
      "step": 419
    },
    {
      "epoch": 0.22245762711864406,
      "grad_norm": 14.909584045410156,
      "learning_rate": 7.77542372881356e-06,
      "loss": 36.8677,
      "step": 420
    },
    {
      "epoch": 0.2229872881355932,
      "grad_norm": 14.385584831237793,
      "learning_rate": 7.770127118644068e-06,
      "loss": 35.6429,
      "step": 421
    },
    {
      "epoch": 0.22351694915254236,
      "grad_norm": 12.89328670501709,
      "learning_rate": 7.764830508474577e-06,
      "loss": 35.0705,
      "step": 422
    },
    {
      "epoch": 0.22404661016949154,
      "grad_norm": 17.612462997436523,
      "learning_rate": 7.759533898305084e-06,
      "loss": 36.4236,
      "step": 423
    },
    {
      "epoch": 0.2245762711864407,
      "grad_norm": 16.163755416870117,
      "learning_rate": 7.754237288135595e-06,
      "loss": 36.9028,
      "step": 424
    },
    {
      "epoch": 0.22510593220338984,
      "grad_norm": 14.078839302062988,
      "learning_rate": 7.748940677966102e-06,
      "loss": 38.0464,
      "step": 425
    },
    {
      "epoch": 0.225635593220339,
      "grad_norm": 12.508034706115723,
      "learning_rate": 7.74364406779661e-06,
      "loss": 35.1789,
      "step": 426
    },
    {
      "epoch": 0.22616525423728814,
      "grad_norm": 14.0387601852417,
      "learning_rate": 7.73834745762712e-06,
      "loss": 36.6567,
      "step": 427
    },
    {
      "epoch": 0.2266949152542373,
      "grad_norm": 12.07306957244873,
      "learning_rate": 7.733050847457628e-06,
      "loss": 35.1888,
      "step": 428
    },
    {
      "epoch": 0.22722457627118645,
      "grad_norm": 12.650291442871094,
      "learning_rate": 7.727754237288137e-06,
      "loss": 34.5795,
      "step": 429
    },
    {
      "epoch": 0.2277542372881356,
      "grad_norm": 15.604769706726074,
      "learning_rate": 7.722457627118645e-06,
      "loss": 37.2186,
      "step": 430
    },
    {
      "epoch": 0.22828389830508475,
      "grad_norm": 13.434661865234375,
      "learning_rate": 7.717161016949153e-06,
      "loss": 36.3673,
      "step": 431
    },
    {
      "epoch": 0.2288135593220339,
      "grad_norm": 14.793424606323242,
      "learning_rate": 7.711864406779663e-06,
      "loss": 37.0799,
      "step": 432
    },
    {
      "epoch": 0.22934322033898305,
      "grad_norm": 11.482912063598633,
      "learning_rate": 7.70656779661017e-06,
      "loss": 35.5861,
      "step": 433
    },
    {
      "epoch": 0.2298728813559322,
      "grad_norm": 14.342568397521973,
      "learning_rate": 7.701271186440679e-06,
      "loss": 35.5337,
      "step": 434
    },
    {
      "epoch": 0.23040254237288135,
      "grad_norm": 15.650469779968262,
      "learning_rate": 7.695974576271188e-06,
      "loss": 36.8128,
      "step": 435
    },
    {
      "epoch": 0.2309322033898305,
      "grad_norm": 15.991424560546875,
      "learning_rate": 7.690677966101695e-06,
      "loss": 37.0812,
      "step": 436
    },
    {
      "epoch": 0.23146186440677965,
      "grad_norm": 13.7311429977417,
      "learning_rate": 7.685381355932205e-06,
      "loss": 36.3043,
      "step": 437
    },
    {
      "epoch": 0.2319915254237288,
      "grad_norm": 13.111760139465332,
      "learning_rate": 7.680084745762712e-06,
      "loss": 35.8702,
      "step": 438
    },
    {
      "epoch": 0.23252118644067796,
      "grad_norm": 12.667045593261719,
      "learning_rate": 7.67478813559322e-06,
      "loss": 35.839,
      "step": 439
    },
    {
      "epoch": 0.2330508474576271,
      "grad_norm": 13.852143287658691,
      "learning_rate": 7.66949152542373e-06,
      "loss": 35.4605,
      "step": 440
    },
    {
      "epoch": 0.23358050847457626,
      "grad_norm": 11.194706916809082,
      "learning_rate": 7.664194915254238e-06,
      "loss": 35.0994,
      "step": 441
    },
    {
      "epoch": 0.2341101694915254,
      "grad_norm": 14.387446403503418,
      "learning_rate": 7.658898305084747e-06,
      "loss": 37.049,
      "step": 442
    },
    {
      "epoch": 0.2346398305084746,
      "grad_norm": 12.108460426330566,
      "learning_rate": 7.653601694915254e-06,
      "loss": 35.222,
      "step": 443
    },
    {
      "epoch": 0.23516949152542374,
      "grad_norm": 13.49776554107666,
      "learning_rate": 7.648305084745763e-06,
      "loss": 37.117,
      "step": 444
    },
    {
      "epoch": 0.2356991525423729,
      "grad_norm": 12.823853492736816,
      "learning_rate": 7.643008474576272e-06,
      "loss": 35.5313,
      "step": 445
    },
    {
      "epoch": 0.23622881355932204,
      "grad_norm": 12.872989654541016,
      "learning_rate": 7.63771186440678e-06,
      "loss": 36.6576,
      "step": 446
    },
    {
      "epoch": 0.2367584745762712,
      "grad_norm": 14.687725067138672,
      "learning_rate": 7.632415254237289e-06,
      "loss": 35.9383,
      "step": 447
    },
    {
      "epoch": 0.23728813559322035,
      "grad_norm": 13.950675010681152,
      "learning_rate": 7.627118644067797e-06,
      "loss": 35.8499,
      "step": 448
    },
    {
      "epoch": 0.2378177966101695,
      "grad_norm": 12.251441955566406,
      "learning_rate": 7.621822033898307e-06,
      "loss": 35.1809,
      "step": 449
    },
    {
      "epoch": 0.23834745762711865,
      "grad_norm": 10.617510795593262,
      "learning_rate": 7.6165254237288145e-06,
      "loss": 34.5212,
      "step": 450
    },
    {
      "epoch": 0.2388771186440678,
      "grad_norm": 12.790823936462402,
      "learning_rate": 7.611228813559322e-06,
      "loss": 35.9684,
      "step": 451
    },
    {
      "epoch": 0.23940677966101695,
      "grad_norm": 14.022799491882324,
      "learning_rate": 7.605932203389831e-06,
      "loss": 35.5855,
      "step": 452
    },
    {
      "epoch": 0.2399364406779661,
      "grad_norm": 16.07508087158203,
      "learning_rate": 7.600635593220339e-06,
      "loss": 37.2118,
      "step": 453
    },
    {
      "epoch": 0.24046610169491525,
      "grad_norm": 15.487733840942383,
      "learning_rate": 7.595338983050849e-06,
      "loss": 36.4803,
      "step": 454
    },
    {
      "epoch": 0.2409957627118644,
      "grad_norm": 12.216702461242676,
      "learning_rate": 7.5900423728813566e-06,
      "loss": 35.4833,
      "step": 455
    },
    {
      "epoch": 0.24152542372881355,
      "grad_norm": 13.599502563476562,
      "learning_rate": 7.5847457627118645e-06,
      "loss": 36.6042,
      "step": 456
    },
    {
      "epoch": 0.2420550847457627,
      "grad_norm": 13.38609504699707,
      "learning_rate": 7.579449152542373e-06,
      "loss": 35.3173,
      "step": 457
    },
    {
      "epoch": 0.24258474576271186,
      "grad_norm": 15.103922843933105,
      "learning_rate": 7.574152542372882e-06,
      "loss": 35.8786,
      "step": 458
    },
    {
      "epoch": 0.243114406779661,
      "grad_norm": 13.98809814453125,
      "learning_rate": 7.568855932203391e-06,
      "loss": 35.12,
      "step": 459
    },
    {
      "epoch": 0.24364406779661016,
      "grad_norm": 12.736790657043457,
      "learning_rate": 7.563559322033899e-06,
      "loss": 36.1708,
      "step": 460
    },
    {
      "epoch": 0.2441737288135593,
      "grad_norm": 13.747405052185059,
      "learning_rate": 7.558262711864407e-06,
      "loss": 36.5864,
      "step": 461
    },
    {
      "epoch": 0.24470338983050846,
      "grad_norm": 13.04160213470459,
      "learning_rate": 7.552966101694916e-06,
      "loss": 35.0197,
      "step": 462
    },
    {
      "epoch": 0.24523305084745764,
      "grad_norm": 12.303763389587402,
      "learning_rate": 7.547669491525425e-06,
      "loss": 35.2411,
      "step": 463
    },
    {
      "epoch": 0.2457627118644068,
      "grad_norm": 19.88754653930664,
      "learning_rate": 7.542372881355933e-06,
      "loss": 37.8196,
      "step": 464
    },
    {
      "epoch": 0.24629237288135594,
      "grad_norm": 12.700716972351074,
      "learning_rate": 7.537076271186441e-06,
      "loss": 34.8035,
      "step": 465
    },
    {
      "epoch": 0.2468220338983051,
      "grad_norm": 13.4606351852417,
      "learning_rate": 7.53177966101695e-06,
      "loss": 35.1558,
      "step": 466
    },
    {
      "epoch": 0.24735169491525424,
      "grad_norm": 13.608341217041016,
      "learning_rate": 7.526483050847458e-06,
      "loss": 35.5202,
      "step": 467
    },
    {
      "epoch": 0.2478813559322034,
      "grad_norm": 12.665416717529297,
      "learning_rate": 7.521186440677967e-06,
      "loss": 35.8215,
      "step": 468
    },
    {
      "epoch": 0.24841101694915255,
      "grad_norm": 11.7763090133667,
      "learning_rate": 7.515889830508475e-06,
      "loss": 34.6889,
      "step": 469
    },
    {
      "epoch": 0.2489406779661017,
      "grad_norm": 15.366496086120605,
      "learning_rate": 7.510593220338984e-06,
      "loss": 36.5941,
      "step": 470
    },
    {
      "epoch": 0.24947033898305085,
      "grad_norm": 17.286909103393555,
      "learning_rate": 7.505296610169492e-06,
      "loss": 37.0892,
      "step": 471
    },
    {
      "epoch": 0.25,
      "grad_norm": 13.034377098083496,
      "learning_rate": 7.500000000000001e-06,
      "loss": 35.7045,
      "step": 472
    },
    {
      "epoch": 0.2505296610169492,
      "grad_norm": 14.313055038452148,
      "learning_rate": 7.494703389830509e-06,
      "loss": 35.7783,
      "step": 473
    },
    {
      "epoch": 0.2510593220338983,
      "grad_norm": 16.570783615112305,
      "learning_rate": 7.489406779661017e-06,
      "loss": 36.2294,
      "step": 474
    },
    {
      "epoch": 0.2515889830508475,
      "grad_norm": 11.87003231048584,
      "learning_rate": 7.4841101694915264e-06,
      "loss": 34.5185,
      "step": 475
    },
    {
      "epoch": 0.2521186440677966,
      "grad_norm": 11.322108268737793,
      "learning_rate": 7.478813559322034e-06,
      "loss": 35.6784,
      "step": 476
    },
    {
      "epoch": 0.2526483050847458,
      "grad_norm": 16.87864875793457,
      "learning_rate": 7.473516949152543e-06,
      "loss": 35.6567,
      "step": 477
    },
    {
      "epoch": 0.2531779661016949,
      "grad_norm": 12.462778091430664,
      "learning_rate": 7.468220338983051e-06,
      "loss": 34.4818,
      "step": 478
    },
    {
      "epoch": 0.2537076271186441,
      "grad_norm": 12.841659545898438,
      "learning_rate": 7.462923728813561e-06,
      "loss": 35.6833,
      "step": 479
    },
    {
      "epoch": 0.2542372881355932,
      "grad_norm": 14.566444396972656,
      "learning_rate": 7.4576271186440685e-06,
      "loss": 35.9223,
      "step": 480
    },
    {
      "epoch": 0.2547669491525424,
      "grad_norm": 15.032759666442871,
      "learning_rate": 7.452330508474576e-06,
      "loss": 35.3623,
      "step": 481
    },
    {
      "epoch": 0.2552966101694915,
      "grad_norm": 13.192596435546875,
      "learning_rate": 7.447033898305085e-06,
      "loss": 34.911,
      "step": 482
    },
    {
      "epoch": 0.2558262711864407,
      "grad_norm": 14.381868362426758,
      "learning_rate": 7.441737288135594e-06,
      "loss": 35.9336,
      "step": 483
    },
    {
      "epoch": 0.2563559322033898,
      "grad_norm": 12.850976943969727,
      "learning_rate": 7.436440677966103e-06,
      "loss": 35.6824,
      "step": 484
    },
    {
      "epoch": 0.256885593220339,
      "grad_norm": 12.497685432434082,
      "learning_rate": 7.4311440677966105e-06,
      "loss": 36.343,
      "step": 485
    },
    {
      "epoch": 0.2574152542372881,
      "grad_norm": 13.908935546875,
      "learning_rate": 7.425847457627119e-06,
      "loss": 35.4696,
      "step": 486
    },
    {
      "epoch": 0.2579449152542373,
      "grad_norm": 13.70351505279541,
      "learning_rate": 7.420550847457628e-06,
      "loss": 35.3874,
      "step": 487
    },
    {
      "epoch": 0.2584745762711864,
      "grad_norm": 13.918442726135254,
      "learning_rate": 7.415254237288137e-06,
      "loss": 35.1164,
      "step": 488
    },
    {
      "epoch": 0.2590042372881356,
      "grad_norm": 12.070303916931152,
      "learning_rate": 7.409957627118645e-06,
      "loss": 34.3897,
      "step": 489
    },
    {
      "epoch": 0.2595338983050847,
      "grad_norm": 11.308910369873047,
      "learning_rate": 7.4046610169491526e-06,
      "loss": 34.2978,
      "step": 490
    },
    {
      "epoch": 0.2600635593220339,
      "grad_norm": 13.477808952331543,
      "learning_rate": 7.399364406779662e-06,
      "loss": 36.438,
      "step": 491
    },
    {
      "epoch": 0.2605932203389831,
      "grad_norm": 16.292407989501953,
      "learning_rate": 7.39406779661017e-06,
      "loss": 35.2849,
      "step": 492
    },
    {
      "epoch": 0.2611228813559322,
      "grad_norm": 13.881239891052246,
      "learning_rate": 7.388771186440679e-06,
      "loss": 34.786,
      "step": 493
    },
    {
      "epoch": 0.2616525423728814,
      "grad_norm": 12.140098571777344,
      "learning_rate": 7.383474576271187e-06,
      "loss": 34.6671,
      "step": 494
    },
    {
      "epoch": 0.2621822033898305,
      "grad_norm": 11.392627716064453,
      "learning_rate": 7.378177966101695e-06,
      "loss": 34.2748,
      "step": 495
    },
    {
      "epoch": 0.2627118644067797,
      "grad_norm": 11.277288436889648,
      "learning_rate": 7.372881355932204e-06,
      "loss": 34.555,
      "step": 496
    },
    {
      "epoch": 0.2632415254237288,
      "grad_norm": 59.96910095214844,
      "learning_rate": 7.367584745762713e-06,
      "loss": 33.764,
      "step": 497
    },
    {
      "epoch": 0.263771186440678,
      "grad_norm": 12.529248237609863,
      "learning_rate": 7.362288135593221e-06,
      "loss": 33.9436,
      "step": 498
    },
    {
      "epoch": 0.2643008474576271,
      "grad_norm": 10.365516662597656,
      "learning_rate": 7.356991525423729e-06,
      "loss": 33.1056,
      "step": 499
    },
    {
      "epoch": 0.2648305084745763,
      "grad_norm": 12.834813117980957,
      "learning_rate": 7.351694915254238e-06,
      "loss": 35.5655,
      "step": 500
    },
    {
      "epoch": 0.2653601694915254,
      "grad_norm": 9.925095558166504,
      "learning_rate": 7.346398305084746e-06,
      "loss": 34.6964,
      "step": 501
    },
    {
      "epoch": 0.2658898305084746,
      "grad_norm": 17.39658546447754,
      "learning_rate": 7.341101694915255e-06,
      "loss": 35.4393,
      "step": 502
    },
    {
      "epoch": 0.2664194915254237,
      "grad_norm": 11.933084487915039,
      "learning_rate": 7.335805084745763e-06,
      "loss": 34.5921,
      "step": 503
    },
    {
      "epoch": 0.2669491525423729,
      "grad_norm": 10.869941711425781,
      "learning_rate": 7.3305084745762725e-06,
      "loss": 33.8036,
      "step": 504
    },
    {
      "epoch": 0.267478813559322,
      "grad_norm": 13.051691055297852,
      "learning_rate": 7.32521186440678e-06,
      "loss": 34.7909,
      "step": 505
    },
    {
      "epoch": 0.2680084745762712,
      "grad_norm": 13.613442420959473,
      "learning_rate": 7.319915254237288e-06,
      "loss": 33.9471,
      "step": 506
    },
    {
      "epoch": 0.2685381355932203,
      "grad_norm": 11.368952751159668,
      "learning_rate": 7.314618644067797e-06,
      "loss": 33.5189,
      "step": 507
    },
    {
      "epoch": 0.2690677966101695,
      "grad_norm": 14.318575859069824,
      "learning_rate": 7.309322033898306e-06,
      "loss": 33.9782,
      "step": 508
    },
    {
      "epoch": 0.2695974576271186,
      "grad_norm": 11.636754989624023,
      "learning_rate": 7.3040254237288145e-06,
      "loss": 33.7921,
      "step": 509
    },
    {
      "epoch": 0.2701271186440678,
      "grad_norm": 18.974300384521484,
      "learning_rate": 7.2987288135593224e-06,
      "loss": 36.3686,
      "step": 510
    },
    {
      "epoch": 0.2706567796610169,
      "grad_norm": 14.772768020629883,
      "learning_rate": 7.293432203389831e-06,
      "loss": 34.8133,
      "step": 511
    },
    {
      "epoch": 0.2711864406779661,
      "grad_norm": 12.064062118530273,
      "learning_rate": 7.288135593220339e-06,
      "loss": 34.7745,
      "step": 512
    },
    {
      "epoch": 0.2717161016949153,
      "grad_norm": 14.701587677001953,
      "learning_rate": 7.282838983050849e-06,
      "loss": 34.4906,
      "step": 513
    },
    {
      "epoch": 0.2722457627118644,
      "grad_norm": 11.460667610168457,
      "learning_rate": 7.277542372881357e-06,
      "loss": 35.3497,
      "step": 514
    },
    {
      "epoch": 0.2727754237288136,
      "grad_norm": 11.826642990112305,
      "learning_rate": 7.2722457627118645e-06,
      "loss": 33.4112,
      "step": 515
    },
    {
      "epoch": 0.2733050847457627,
      "grad_norm": 14.721673965454102,
      "learning_rate": 7.266949152542373e-06,
      "loss": 34.761,
      "step": 516
    },
    {
      "epoch": 0.2738347457627119,
      "grad_norm": 11.404622077941895,
      "learning_rate": 7.261652542372882e-06,
      "loss": 33.6517,
      "step": 517
    },
    {
      "epoch": 0.274364406779661,
      "grad_norm": 11.54326057434082,
      "learning_rate": 7.256355932203391e-06,
      "loss": 33.5161,
      "step": 518
    },
    {
      "epoch": 0.2748940677966102,
      "grad_norm": 13.854162216186523,
      "learning_rate": 7.251059322033899e-06,
      "loss": 34.5694,
      "step": 519
    },
    {
      "epoch": 0.2754237288135593,
      "grad_norm": 13.567435264587402,
      "learning_rate": 7.2457627118644065e-06,
      "loss": 33.5446,
      "step": 520
    },
    {
      "epoch": 0.2759533898305085,
      "grad_norm": 11.384541511535645,
      "learning_rate": 7.240466101694916e-06,
      "loss": 33.234,
      "step": 521
    },
    {
      "epoch": 0.2764830508474576,
      "grad_norm": 13.899309158325195,
      "learning_rate": 7.235169491525425e-06,
      "loss": 34.5407,
      "step": 522
    },
    {
      "epoch": 0.2770127118644068,
      "grad_norm": 11.295031547546387,
      "learning_rate": 7.229872881355933e-06,
      "loss": 34.9697,
      "step": 523
    },
    {
      "epoch": 0.2775423728813559,
      "grad_norm": 10.763614654541016,
      "learning_rate": 7.224576271186441e-06,
      "loss": 33.8054,
      "step": 524
    },
    {
      "epoch": 0.2780720338983051,
      "grad_norm": 13.057011604309082,
      "learning_rate": 7.21927966101695e-06,
      "loss": 33.4305,
      "step": 525
    },
    {
      "epoch": 0.2786016949152542,
      "grad_norm": 12.341489791870117,
      "learning_rate": 7.213983050847458e-06,
      "loss": 34.7282,
      "step": 526
    },
    {
      "epoch": 0.2791313559322034,
      "grad_norm": 10.381377220153809,
      "learning_rate": 7.208686440677967e-06,
      "loss": 33.9001,
      "step": 527
    },
    {
      "epoch": 0.2796610169491525,
      "grad_norm": 11.537259101867676,
      "learning_rate": 7.203389830508475e-06,
      "loss": 33.0938,
      "step": 528
    },
    {
      "epoch": 0.2801906779661017,
      "grad_norm": 11.558427810668945,
      "learning_rate": 7.198093220338984e-06,
      "loss": 32.8472,
      "step": 529
    },
    {
      "epoch": 0.2807203389830508,
      "grad_norm": 10.459383964538574,
      "learning_rate": 7.192796610169492e-06,
      "loss": 33.9119,
      "step": 530
    },
    {
      "epoch": 0.28125,
      "grad_norm": 12.366400718688965,
      "learning_rate": 7.1875e-06,
      "loss": 33.8058,
      "step": 531
    },
    {
      "epoch": 0.2817796610169492,
      "grad_norm": 12.510037422180176,
      "learning_rate": 7.182203389830509e-06,
      "loss": 33.6078,
      "step": 532
    },
    {
      "epoch": 0.2823093220338983,
      "grad_norm": 14.498547554016113,
      "learning_rate": 7.176906779661017e-06,
      "loss": 34.7195,
      "step": 533
    },
    {
      "epoch": 0.2828389830508475,
      "grad_norm": 12.815942764282227,
      "learning_rate": 7.1716101694915265e-06,
      "loss": 33.9789,
      "step": 534
    },
    {
      "epoch": 0.2833686440677966,
      "grad_norm": 10.604270935058594,
      "learning_rate": 7.166313559322034e-06,
      "loss": 32.4532,
      "step": 535
    },
    {
      "epoch": 0.2838983050847458,
      "grad_norm": 12.58456039428711,
      "learning_rate": 7.161016949152543e-06,
      "loss": 33.1084,
      "step": 536
    },
    {
      "epoch": 0.2844279661016949,
      "grad_norm": 13.782327651977539,
      "learning_rate": 7.155720338983051e-06,
      "loss": 34.1727,
      "step": 537
    },
    {
      "epoch": 0.2849576271186441,
      "grad_norm": 12.61552619934082,
      "learning_rate": 7.150423728813561e-06,
      "loss": 33.1652,
      "step": 538
    },
    {
      "epoch": 0.2854872881355932,
      "grad_norm": 12.807329177856445,
      "learning_rate": 7.1451271186440685e-06,
      "loss": 33.9162,
      "step": 539
    },
    {
      "epoch": 0.2860169491525424,
      "grad_norm": 12.52315902709961,
      "learning_rate": 7.139830508474576e-06,
      "loss": 32.9679,
      "step": 540
    },
    {
      "epoch": 0.2865466101694915,
      "grad_norm": 12.99775218963623,
      "learning_rate": 7.134533898305085e-06,
      "loss": 32.9446,
      "step": 541
    },
    {
      "epoch": 0.2870762711864407,
      "grad_norm": 10.964652061462402,
      "learning_rate": 7.129237288135594e-06,
      "loss": 32.4934,
      "step": 542
    },
    {
      "epoch": 0.2876059322033898,
      "grad_norm": 12.407109260559082,
      "learning_rate": 7.123940677966103e-06,
      "loss": 33.8566,
      "step": 543
    },
    {
      "epoch": 0.288135593220339,
      "grad_norm": 10.491747856140137,
      "learning_rate": 7.1186440677966106e-06,
      "loss": 33.5154,
      "step": 544
    },
    {
      "epoch": 0.2886652542372881,
      "grad_norm": 11.62002182006836,
      "learning_rate": 7.1133474576271185e-06,
      "loss": 33.06,
      "step": 545
    },
    {
      "epoch": 0.2891949152542373,
      "grad_norm": 9.352611541748047,
      "learning_rate": 7.108050847457628e-06,
      "loss": 31.9987,
      "step": 546
    },
    {
      "epoch": 0.2897245762711864,
      "grad_norm": 11.293587684631348,
      "learning_rate": 7.102754237288137e-06,
      "loss": 32.8019,
      "step": 547
    },
    {
      "epoch": 0.2902542372881356,
      "grad_norm": 13.726049423217773,
      "learning_rate": 7.097457627118645e-06,
      "loss": 33.2192,
      "step": 548
    },
    {
      "epoch": 0.2907838983050847,
      "grad_norm": 12.967650413513184,
      "learning_rate": 7.092161016949153e-06,
      "loss": 33.8782,
      "step": 549
    },
    {
      "epoch": 0.2913135593220339,
      "grad_norm": 14.151078224182129,
      "learning_rate": 7.086864406779662e-06,
      "loss": 33.0658,
      "step": 550
    },
    {
      "epoch": 0.2918432203389831,
      "grad_norm": 12.911285400390625,
      "learning_rate": 7.08156779661017e-06,
      "loss": 33.6507,
      "step": 551
    },
    {
      "epoch": 0.2923728813559322,
      "grad_norm": 16.055044174194336,
      "learning_rate": 7.076271186440679e-06,
      "loss": 33.882,
      "step": 552
    },
    {
      "epoch": 0.2929025423728814,
      "grad_norm": 12.773218154907227,
      "learning_rate": 7.070974576271187e-06,
      "loss": 35.0815,
      "step": 553
    },
    {
      "epoch": 0.2934322033898305,
      "grad_norm": 11.634058952331543,
      "learning_rate": 7.065677966101695e-06,
      "loss": 33.3633,
      "step": 554
    },
    {
      "epoch": 0.2939618644067797,
      "grad_norm": 12.683808326721191,
      "learning_rate": 7.060381355932204e-06,
      "loss": 33.5528,
      "step": 555
    },
    {
      "epoch": 0.2944915254237288,
      "grad_norm": 10.98642349243164,
      "learning_rate": 7.055084745762712e-06,
      "loss": 33.615,
      "step": 556
    },
    {
      "epoch": 0.295021186440678,
      "grad_norm": 12.497872352600098,
      "learning_rate": 7.049788135593221e-06,
      "loss": 33.0034,
      "step": 557
    },
    {
      "epoch": 0.2955508474576271,
      "grad_norm": 15.557939529418945,
      "learning_rate": 7.044491525423729e-06,
      "loss": 33.6689,
      "step": 558
    },
    {
      "epoch": 0.2960805084745763,
      "grad_norm": 12.206475257873535,
      "learning_rate": 7.039194915254238e-06,
      "loss": 33.5566,
      "step": 559
    },
    {
      "epoch": 0.2966101694915254,
      "grad_norm": 10.843818664550781,
      "learning_rate": 7.033898305084746e-06,
      "loss": 32.515,
      "step": 560
    },
    {
      "epoch": 0.2971398305084746,
      "grad_norm": 12.097030639648438,
      "learning_rate": 7.028601694915255e-06,
      "loss": 32.8726,
      "step": 561
    },
    {
      "epoch": 0.2976694915254237,
      "grad_norm": 11.419021606445312,
      "learning_rate": 7.023305084745763e-06,
      "loss": 33.139,
      "step": 562
    },
    {
      "epoch": 0.2981991525423729,
      "grad_norm": 9.71640396118164,
      "learning_rate": 7.0180084745762725e-06,
      "loss": 33.0309,
      "step": 563
    },
    {
      "epoch": 0.298728813559322,
      "grad_norm": 11.198091506958008,
      "learning_rate": 7.0127118644067804e-06,
      "loss": 32.6658,
      "step": 564
    },
    {
      "epoch": 0.2992584745762712,
      "grad_norm": 9.589323043823242,
      "learning_rate": 7.007415254237288e-06,
      "loss": 32.4419,
      "step": 565
    },
    {
      "epoch": 0.2997881355932203,
      "grad_norm": 10.299332618713379,
      "learning_rate": 7.002118644067797e-06,
      "loss": 33.4529,
      "step": 566
    },
    {
      "epoch": 0.3003177966101695,
      "grad_norm": 11.988761901855469,
      "learning_rate": 6.996822033898306e-06,
      "loss": 33.1753,
      "step": 567
    },
    {
      "epoch": 0.3008474576271186,
      "grad_norm": 11.485057830810547,
      "learning_rate": 6.9915254237288146e-06,
      "loss": 32.2786,
      "step": 568
    },
    {
      "epoch": 0.3013771186440678,
      "grad_norm": 12.280195236206055,
      "learning_rate": 6.9862288135593225e-06,
      "loss": 32.78,
      "step": 569
    },
    {
      "epoch": 0.3019067796610169,
      "grad_norm": 11.032910346984863,
      "learning_rate": 6.980932203389831e-06,
      "loss": 32.8686,
      "step": 570
    },
    {
      "epoch": 0.3024364406779661,
      "grad_norm": 11.897245407104492,
      "learning_rate": 6.975635593220339e-06,
      "loss": 33.1014,
      "step": 571
    },
    {
      "epoch": 0.3029661016949153,
      "grad_norm": 12.445066452026367,
      "learning_rate": 6.970338983050849e-06,
      "loss": 32.5846,
      "step": 572
    },
    {
      "epoch": 0.3034957627118644,
      "grad_norm": 12.156046867370605,
      "learning_rate": 6.965042372881357e-06,
      "loss": 33.1475,
      "step": 573
    },
    {
      "epoch": 0.3040254237288136,
      "grad_norm": 12.070045471191406,
      "learning_rate": 6.9597457627118645e-06,
      "loss": 33.6978,
      "step": 574
    },
    {
      "epoch": 0.3045550847457627,
      "grad_norm": 10.587719917297363,
      "learning_rate": 6.954449152542373e-06,
      "loss": 32.776,
      "step": 575
    },
    {
      "epoch": 0.3050847457627119,
      "grad_norm": 13.830223083496094,
      "learning_rate": 6.949152542372882e-06,
      "loss": 33.7619,
      "step": 576
    },
    {
      "epoch": 0.305614406779661,
      "grad_norm": 12.232824325561523,
      "learning_rate": 6.943855932203391e-06,
      "loss": 33.428,
      "step": 577
    },
    {
      "epoch": 0.3061440677966102,
      "grad_norm": 11.567238807678223,
      "learning_rate": 6.938559322033899e-06,
      "loss": 32.3573,
      "step": 578
    },
    {
      "epoch": 0.3066737288135593,
      "grad_norm": 10.54849910736084,
      "learning_rate": 6.9332627118644066e-06,
      "loss": 32.5946,
      "step": 579
    },
    {
      "epoch": 0.3072033898305085,
      "grad_norm": 12.322686195373535,
      "learning_rate": 6.927966101694916e-06,
      "loss": 32.9455,
      "step": 580
    },
    {
      "epoch": 0.3077330508474576,
      "grad_norm": 12.480887413024902,
      "learning_rate": 6.922669491525424e-06,
      "loss": 33.7016,
      "step": 581
    },
    {
      "epoch": 0.3082627118644068,
      "grad_norm": 10.954991340637207,
      "learning_rate": 6.917372881355933e-06,
      "loss": 32.6709,
      "step": 582
    },
    {
      "epoch": 0.3087923728813559,
      "grad_norm": 12.050299644470215,
      "learning_rate": 6.912076271186441e-06,
      "loss": 32.378,
      "step": 583
    },
    {
      "epoch": 0.3093220338983051,
      "grad_norm": 11.375011444091797,
      "learning_rate": 6.90677966101695e-06,
      "loss": 32.4443,
      "step": 584
    },
    {
      "epoch": 0.3098516949152542,
      "grad_norm": 10.948837280273438,
      "learning_rate": 6.901483050847458e-06,
      "loss": 31.594,
      "step": 585
    },
    {
      "epoch": 0.3103813559322034,
      "grad_norm": 11.192353248596191,
      "learning_rate": 6.896186440677967e-06,
      "loss": 32.8349,
      "step": 586
    },
    {
      "epoch": 0.3109110169491525,
      "grad_norm": 15.179421424865723,
      "learning_rate": 6.890889830508475e-06,
      "loss": 34.1417,
      "step": 587
    },
    {
      "epoch": 0.3114406779661017,
      "grad_norm": 11.307619094848633,
      "learning_rate": 6.8855932203389844e-06,
      "loss": 31.7207,
      "step": 588
    },
    {
      "epoch": 0.3119703389830508,
      "grad_norm": 10.702316284179688,
      "learning_rate": 6.880296610169492e-06,
      "loss": 31.9491,
      "step": 589
    },
    {
      "epoch": 0.3125,
      "grad_norm": 12.604379653930664,
      "learning_rate": 6.875e-06,
      "loss": 34.2009,
      "step": 590
    },
    {
      "epoch": 0.3130296610169492,
      "grad_norm": 12.416142463684082,
      "learning_rate": 6.869703389830509e-06,
      "loss": 32.495,
      "step": 591
    },
    {
      "epoch": 0.3135593220338983,
      "grad_norm": 12.094220161437988,
      "learning_rate": 6.864406779661017e-06,
      "loss": 32.7487,
      "step": 592
    },
    {
      "epoch": 0.3140889830508475,
      "grad_norm": 11.89113998413086,
      "learning_rate": 6.8591101694915265e-06,
      "loss": 32.1466,
      "step": 593
    },
    {
      "epoch": 0.3146186440677966,
      "grad_norm": 11.398275375366211,
      "learning_rate": 6.853813559322034e-06,
      "loss": 32.1344,
      "step": 594
    },
    {
      "epoch": 0.3151483050847458,
      "grad_norm": 12.436342239379883,
      "learning_rate": 6.848516949152543e-06,
      "loss": 33.2552,
      "step": 595
    },
    {
      "epoch": 0.3156779661016949,
      "grad_norm": 10.873980522155762,
      "learning_rate": 6.843220338983051e-06,
      "loss": 32.9078,
      "step": 596
    },
    {
      "epoch": 0.3162076271186441,
      "grad_norm": 10.3889741897583,
      "learning_rate": 6.837923728813561e-06,
      "loss": 32.1263,
      "step": 597
    },
    {
      "epoch": 0.3167372881355932,
      "grad_norm": 12.309548377990723,
      "learning_rate": 6.8326271186440685e-06,
      "loss": 33.1992,
      "step": 598
    },
    {
      "epoch": 0.3172669491525424,
      "grad_norm": 11.370207786560059,
      "learning_rate": 6.8273305084745764e-06,
      "loss": 32.1801,
      "step": 599
    },
    {
      "epoch": 0.3177966101694915,
      "grad_norm": 11.728866577148438,
      "learning_rate": 6.822033898305085e-06,
      "loss": 31.9409,
      "step": 600
    },
    {
      "epoch": 0.3183262711864407,
      "grad_norm": 11.204320907592773,
      "learning_rate": 6.816737288135594e-06,
      "loss": 32.6477,
      "step": 601
    },
    {
      "epoch": 0.3188559322033898,
      "grad_norm": 13.467142105102539,
      "learning_rate": 6.811440677966103e-06,
      "loss": 33.2858,
      "step": 602
    },
    {
      "epoch": 0.319385593220339,
      "grad_norm": 10.245979309082031,
      "learning_rate": 6.806144067796611e-06,
      "loss": 31.5708,
      "step": 603
    },
    {
      "epoch": 0.3199152542372881,
      "grad_norm": 10.40089225769043,
      "learning_rate": 6.8008474576271185e-06,
      "loss": 30.5193,
      "step": 604
    },
    {
      "epoch": 0.3204449152542373,
      "grad_norm": 9.998153686523438,
      "learning_rate": 6.795550847457628e-06,
      "loss": 32.3746,
      "step": 605
    },
    {
      "epoch": 0.3209745762711864,
      "grad_norm": 13.911192893981934,
      "learning_rate": 6.790254237288136e-06,
      "loss": 33.5233,
      "step": 606
    },
    {
      "epoch": 0.3215042372881356,
      "grad_norm": 14.008895874023438,
      "learning_rate": 6.784957627118645e-06,
      "loss": 32.5878,
      "step": 607
    },
    {
      "epoch": 0.3220338983050847,
      "grad_norm": 11.328495025634766,
      "learning_rate": 6.779661016949153e-06,
      "loss": 32.9157,
      "step": 608
    },
    {
      "epoch": 0.3225635593220339,
      "grad_norm": 10.467864990234375,
      "learning_rate": 6.774364406779662e-06,
      "loss": 32.1355,
      "step": 609
    },
    {
      "epoch": 0.3230932203389831,
      "grad_norm": 10.77238941192627,
      "learning_rate": 6.76906779661017e-06,
      "loss": 31.585,
      "step": 610
    },
    {
      "epoch": 0.3236228813559322,
      "grad_norm": 11.614456176757812,
      "learning_rate": 6.763771186440679e-06,
      "loss": 31.7846,
      "step": 611
    },
    {
      "epoch": 0.3241525423728814,
      "grad_norm": 12.825079917907715,
      "learning_rate": 6.758474576271187e-06,
      "loss": 32.4325,
      "step": 612
    },
    {
      "epoch": 0.3246822033898305,
      "grad_norm": 10.862740516662598,
      "learning_rate": 6.753177966101695e-06,
      "loss": 31.8535,
      "step": 613
    },
    {
      "epoch": 0.3252118644067797,
      "grad_norm": 12.531156539916992,
      "learning_rate": 6.747881355932204e-06,
      "loss": 31.658,
      "step": 614
    },
    {
      "epoch": 0.3257415254237288,
      "grad_norm": 10.605332374572754,
      "learning_rate": 6.742584745762712e-06,
      "loss": 31.1134,
      "step": 615
    },
    {
      "epoch": 0.326271186440678,
      "grad_norm": 12.358251571655273,
      "learning_rate": 6.737288135593221e-06,
      "loss": 31.5715,
      "step": 616
    },
    {
      "epoch": 0.3268008474576271,
      "grad_norm": 10.972562789916992,
      "learning_rate": 6.731991525423729e-06,
      "loss": 31.5454,
      "step": 617
    },
    {
      "epoch": 0.3273305084745763,
      "grad_norm": 11.936002731323242,
      "learning_rate": 6.726694915254238e-06,
      "loss": 33.2111,
      "step": 618
    },
    {
      "epoch": 0.3278601694915254,
      "grad_norm": 12.629659652709961,
      "learning_rate": 6.721398305084746e-06,
      "loss": 32.1191,
      "step": 619
    },
    {
      "epoch": 0.3283898305084746,
      "grad_norm": 12.262224197387695,
      "learning_rate": 6.716101694915255e-06,
      "loss": 31.2691,
      "step": 620
    },
    {
      "epoch": 0.3289194915254237,
      "grad_norm": 11.038138389587402,
      "learning_rate": 6.710805084745763e-06,
      "loss": 31.6352,
      "step": 621
    },
    {
      "epoch": 0.3294491525423729,
      "grad_norm": 10.85988998413086,
      "learning_rate": 6.7055084745762726e-06,
      "loss": 30.9202,
      "step": 622
    },
    {
      "epoch": 0.329978813559322,
      "grad_norm": 10.732693672180176,
      "learning_rate": 6.7002118644067805e-06,
      "loss": 31.7637,
      "step": 623
    },
    {
      "epoch": 0.3305084745762712,
      "grad_norm": 12.448695182800293,
      "learning_rate": 6.694915254237288e-06,
      "loss": 31.9588,
      "step": 624
    },
    {
      "epoch": 0.3310381355932203,
      "grad_norm": 8.557010650634766,
      "learning_rate": 6.689618644067797e-06,
      "loss": 29.7232,
      "step": 625
    },
    {
      "epoch": 0.3315677966101695,
      "grad_norm": 11.386064529418945,
      "learning_rate": 6.684322033898306e-06,
      "loss": 31.526,
      "step": 626
    },
    {
      "epoch": 0.3320974576271186,
      "grad_norm": 12.7029390335083,
      "learning_rate": 6.679025423728815e-06,
      "loss": 32.4698,
      "step": 627
    },
    {
      "epoch": 0.3326271186440678,
      "grad_norm": 11.511479377746582,
      "learning_rate": 6.6737288135593225e-06,
      "loss": 32.4941,
      "step": 628
    },
    {
      "epoch": 0.3331567796610169,
      "grad_norm": 10.043827056884766,
      "learning_rate": 6.66843220338983e-06,
      "loss": 30.7388,
      "step": 629
    },
    {
      "epoch": 0.3336864406779661,
      "grad_norm": 9.825746536254883,
      "learning_rate": 6.663135593220339e-06,
      "loss": 31.6598,
      "step": 630
    },
    {
      "epoch": 0.3342161016949153,
      "grad_norm": 9.828339576721191,
      "learning_rate": 6.657838983050848e-06,
      "loss": 30.6916,
      "step": 631
    },
    {
      "epoch": 0.3347457627118644,
      "grad_norm": 12.046125411987305,
      "learning_rate": 6.652542372881357e-06,
      "loss": 31.0784,
      "step": 632
    },
    {
      "epoch": 0.3352754237288136,
      "grad_norm": 12.957756996154785,
      "learning_rate": 6.6472457627118645e-06,
      "loss": 32.4117,
      "step": 633
    },
    {
      "epoch": 0.3358050847457627,
      "grad_norm": 12.890677452087402,
      "learning_rate": 6.641949152542373e-06,
      "loss": 31.2808,
      "step": 634
    },
    {
      "epoch": 0.3363347457627119,
      "grad_norm": 9.797857284545898,
      "learning_rate": 6.636652542372882e-06,
      "loss": 30.8578,
      "step": 635
    },
    {
      "epoch": 0.336864406779661,
      "grad_norm": 9.247540473937988,
      "learning_rate": 6.631355932203391e-06,
      "loss": 30.4748,
      "step": 636
    },
    {
      "epoch": 0.3373940677966102,
      "grad_norm": 11.290739059448242,
      "learning_rate": 6.626059322033899e-06,
      "loss": 32.9712,
      "step": 637
    },
    {
      "epoch": 0.3379237288135593,
      "grad_norm": 11.905488967895508,
      "learning_rate": 6.620762711864407e-06,
      "loss": 31.9562,
      "step": 638
    },
    {
      "epoch": 0.3384533898305085,
      "grad_norm": 12.112495422363281,
      "learning_rate": 6.615466101694916e-06,
      "loss": 31.7699,
      "step": 639
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 10.916360855102539,
      "learning_rate": 6.610169491525424e-06,
      "loss": 30.9222,
      "step": 640
    },
    {
      "epoch": 0.3395127118644068,
      "grad_norm": 11.145485877990723,
      "learning_rate": 6.604872881355933e-06,
      "loss": 31.8168,
      "step": 641
    },
    {
      "epoch": 0.3400423728813559,
      "grad_norm": 10.169391632080078,
      "learning_rate": 6.599576271186441e-06,
      "loss": 31.2568,
      "step": 642
    },
    {
      "epoch": 0.3405720338983051,
      "grad_norm": 10.916389465332031,
      "learning_rate": 6.59427966101695e-06,
      "loss": 30.9186,
      "step": 643
    },
    {
      "epoch": 0.3411016949152542,
      "grad_norm": 10.238324165344238,
      "learning_rate": 6.588983050847458e-06,
      "loss": 31.6594,
      "step": 644
    },
    {
      "epoch": 0.3416313559322034,
      "grad_norm": 9.175080299377441,
      "learning_rate": 6.583686440677967e-06,
      "loss": 29.8924,
      "step": 645
    },
    {
      "epoch": 0.3421610169491525,
      "grad_norm": 9.815884590148926,
      "learning_rate": 6.578389830508475e-06,
      "loss": 31.4887,
      "step": 646
    },
    {
      "epoch": 0.3426906779661017,
      "grad_norm": 12.269852638244629,
      "learning_rate": 6.5730932203389845e-06,
      "loss": 31.6612,
      "step": 647
    },
    {
      "epoch": 0.3432203389830508,
      "grad_norm": 11.067300796508789,
      "learning_rate": 6.567796610169492e-06,
      "loss": 30.4315,
      "step": 648
    },
    {
      "epoch": 0.34375,
      "grad_norm": 12.742807388305664,
      "learning_rate": 6.5625e-06,
      "loss": 31.4898,
      "step": 649
    },
    {
      "epoch": 0.3442796610169492,
      "grad_norm": 10.118514060974121,
      "learning_rate": 6.557203389830509e-06,
      "loss": 30.7557,
      "step": 650
    },
    {
      "epoch": 0.3448093220338983,
      "grad_norm": 9.775418281555176,
      "learning_rate": 6.551906779661017e-06,
      "loss": 29.5887,
      "step": 651
    },
    {
      "epoch": 0.3453389830508475,
      "grad_norm": 12.865402221679688,
      "learning_rate": 6.5466101694915265e-06,
      "loss": 31.5912,
      "step": 652
    },
    {
      "epoch": 0.3458686440677966,
      "grad_norm": 12.059957504272461,
      "learning_rate": 6.541313559322034e-06,
      "loss": 31.2377,
      "step": 653
    },
    {
      "epoch": 0.3463983050847458,
      "grad_norm": 10.355350494384766,
      "learning_rate": 6.536016949152542e-06,
      "loss": 31.2364,
      "step": 654
    },
    {
      "epoch": 0.3469279661016949,
      "grad_norm": 9.873594284057617,
      "learning_rate": 6.530720338983051e-06,
      "loss": 31.5343,
      "step": 655
    },
    {
      "epoch": 0.3474576271186441,
      "grad_norm": 11.071700096130371,
      "learning_rate": 6.52542372881356e-06,
      "loss": 31.3243,
      "step": 656
    },
    {
      "epoch": 0.3479872881355932,
      "grad_norm": 12.219120025634766,
      "learning_rate": 6.5201271186440686e-06,
      "loss": 30.5373,
      "step": 657
    },
    {
      "epoch": 0.3485169491525424,
      "grad_norm": 11.669831275939941,
      "learning_rate": 6.5148305084745765e-06,
      "loss": 31.2083,
      "step": 658
    },
    {
      "epoch": 0.3490466101694915,
      "grad_norm": 11.368453025817871,
      "learning_rate": 6.509533898305085e-06,
      "loss": 31.068,
      "step": 659
    },
    {
      "epoch": 0.3495762711864407,
      "grad_norm": 9.503265380859375,
      "learning_rate": 6.504237288135594e-06,
      "loss": 30.1628,
      "step": 660
    },
    {
      "epoch": 0.3501059322033898,
      "grad_norm": 10.706294059753418,
      "learning_rate": 6.498940677966103e-06,
      "loss": 31.1612,
      "step": 661
    },
    {
      "epoch": 0.350635593220339,
      "grad_norm": 13.61571979522705,
      "learning_rate": 6.493644067796611e-06,
      "loss": 31.3345,
      "step": 662
    },
    {
      "epoch": 0.3511652542372881,
      "grad_norm": 10.433880805969238,
      "learning_rate": 6.4883474576271185e-06,
      "loss": 30.4672,
      "step": 663
    },
    {
      "epoch": 0.3516949152542373,
      "grad_norm": 11.424784660339355,
      "learning_rate": 6.483050847457628e-06,
      "loss": 30.4152,
      "step": 664
    },
    {
      "epoch": 0.3522245762711864,
      "grad_norm": 10.696197509765625,
      "learning_rate": 6.477754237288136e-06,
      "loss": 30.0269,
      "step": 665
    },
    {
      "epoch": 0.3527542372881356,
      "grad_norm": 11.765337944030762,
      "learning_rate": 6.472457627118645e-06,
      "loss": 31.1506,
      "step": 666
    },
    {
      "epoch": 0.3532838983050847,
      "grad_norm": 10.843339920043945,
      "learning_rate": 6.467161016949153e-06,
      "loss": 31.0483,
      "step": 667
    },
    {
      "epoch": 0.3538135593220339,
      "grad_norm": 10.522350311279297,
      "learning_rate": 6.461864406779662e-06,
      "loss": 30.626,
      "step": 668
    },
    {
      "epoch": 0.3543432203389831,
      "grad_norm": 12.584833145141602,
      "learning_rate": 6.45656779661017e-06,
      "loss": 31.116,
      "step": 669
    },
    {
      "epoch": 0.3548728813559322,
      "grad_norm": 10.52464771270752,
      "learning_rate": 6.451271186440679e-06,
      "loss": 30.7113,
      "step": 670
    },
    {
      "epoch": 0.3554025423728814,
      "grad_norm": 11.0693941116333,
      "learning_rate": 6.445974576271187e-06,
      "loss": 30.8206,
      "step": 671
    },
    {
      "epoch": 0.3559322033898305,
      "grad_norm": 12.24295425415039,
      "learning_rate": 6.440677966101695e-06,
      "loss": 31.1673,
      "step": 672
    },
    {
      "epoch": 0.3564618644067797,
      "grad_norm": 9.573784828186035,
      "learning_rate": 6.435381355932204e-06,
      "loss": 31.1304,
      "step": 673
    },
    {
      "epoch": 0.3569915254237288,
      "grad_norm": 10.404757499694824,
      "learning_rate": 6.430084745762712e-06,
      "loss": 31.3155,
      "step": 674
    },
    {
      "epoch": 0.357521186440678,
      "grad_norm": 11.680147171020508,
      "learning_rate": 6.424788135593221e-06,
      "loss": 31.1133,
      "step": 675
    },
    {
      "epoch": 0.3580508474576271,
      "grad_norm": 11.256165504455566,
      "learning_rate": 6.419491525423729e-06,
      "loss": 30.9632,
      "step": 676
    },
    {
      "epoch": 0.3585805084745763,
      "grad_norm": 12.186362266540527,
      "learning_rate": 6.4141949152542384e-06,
      "loss": 30.9135,
      "step": 677
    },
    {
      "epoch": 0.3591101694915254,
      "grad_norm": 12.126640319824219,
      "learning_rate": 6.408898305084746e-06,
      "loss": 30.7763,
      "step": 678
    },
    {
      "epoch": 0.3596398305084746,
      "grad_norm": 11.16136360168457,
      "learning_rate": 6.403601694915254e-06,
      "loss": 29.7174,
      "step": 679
    },
    {
      "epoch": 0.3601694915254237,
      "grad_norm": 11.530482292175293,
      "learning_rate": 6.398305084745763e-06,
      "loss": 31.8552,
      "step": 680
    },
    {
      "epoch": 0.3606991525423729,
      "grad_norm": 11.40404224395752,
      "learning_rate": 6.393008474576273e-06,
      "loss": 30.986,
      "step": 681
    },
    {
      "epoch": 0.361228813559322,
      "grad_norm": 11.1862154006958,
      "learning_rate": 6.3877118644067805e-06,
      "loss": 31.1316,
      "step": 682
    },
    {
      "epoch": 0.3617584745762712,
      "grad_norm": 11.048447608947754,
      "learning_rate": 6.382415254237288e-06,
      "loss": 30.0342,
      "step": 683
    },
    {
      "epoch": 0.3622881355932203,
      "grad_norm": 14.527169227600098,
      "learning_rate": 6.377118644067797e-06,
      "loss": 31.318,
      "step": 684
    },
    {
      "epoch": 0.3628177966101695,
      "grad_norm": 9.015714645385742,
      "learning_rate": 6.371822033898306e-06,
      "loss": 30.0528,
      "step": 685
    },
    {
      "epoch": 0.3633474576271186,
      "grad_norm": 12.809593200683594,
      "learning_rate": 6.366525423728815e-06,
      "loss": 31.9563,
      "step": 686
    },
    {
      "epoch": 0.3638771186440678,
      "grad_norm": 9.738286972045898,
      "learning_rate": 6.3612288135593225e-06,
      "loss": 29.5473,
      "step": 687
    },
    {
      "epoch": 0.3644067796610169,
      "grad_norm": 12.460847854614258,
      "learning_rate": 6.3559322033898304e-06,
      "loss": 30.573,
      "step": 688
    },
    {
      "epoch": 0.3649364406779661,
      "grad_norm": 11.316383361816406,
      "learning_rate": 6.350635593220339e-06,
      "loss": 30.6215,
      "step": 689
    },
    {
      "epoch": 0.3654661016949153,
      "grad_norm": 10.451315879821777,
      "learning_rate": 6.345338983050848e-06,
      "loss": 30.804,
      "step": 690
    },
    {
      "epoch": 0.3659957627118644,
      "grad_norm": 9.129371643066406,
      "learning_rate": 6.340042372881357e-06,
      "loss": 29.8995,
      "step": 691
    },
    {
      "epoch": 0.3665254237288136,
      "grad_norm": 10.459317207336426,
      "learning_rate": 6.3347457627118646e-06,
      "loss": 30.8103,
      "step": 692
    },
    {
      "epoch": 0.3670550847457627,
      "grad_norm": 10.561874389648438,
      "learning_rate": 6.329449152542373e-06,
      "loss": 30.9898,
      "step": 693
    },
    {
      "epoch": 0.3675847457627119,
      "grad_norm": 9.121829986572266,
      "learning_rate": 6.324152542372882e-06,
      "loss": 29.4428,
      "step": 694
    },
    {
      "epoch": 0.368114406779661,
      "grad_norm": 10.445877075195312,
      "learning_rate": 6.318855932203391e-06,
      "loss": 30.5566,
      "step": 695
    },
    {
      "epoch": 0.3686440677966102,
      "grad_norm": 9.714262008666992,
      "learning_rate": 6.313559322033899e-06,
      "loss": 30.5357,
      "step": 696
    },
    {
      "epoch": 0.3691737288135593,
      "grad_norm": 13.121987342834473,
      "learning_rate": 6.308262711864407e-06,
      "loss": 31.6567,
      "step": 697
    },
    {
      "epoch": 0.3697033898305085,
      "grad_norm": 16.6763916015625,
      "learning_rate": 6.302966101694916e-06,
      "loss": 29.5448,
      "step": 698
    },
    {
      "epoch": 0.3702330508474576,
      "grad_norm": 10.691883087158203,
      "learning_rate": 6.297669491525424e-06,
      "loss": 30.4205,
      "step": 699
    },
    {
      "epoch": 0.3707627118644068,
      "grad_norm": 13.513520240783691,
      "learning_rate": 6.292372881355933e-06,
      "loss": 29.4912,
      "step": 700
    },
    {
      "epoch": 0.3712923728813559,
      "grad_norm": 10.024726867675781,
      "learning_rate": 6.287076271186441e-06,
      "loss": 29.6706,
      "step": 701
    },
    {
      "epoch": 0.3718220338983051,
      "grad_norm": 11.596128463745117,
      "learning_rate": 6.28177966101695e-06,
      "loss": 30.6581,
      "step": 702
    },
    {
      "epoch": 0.3723516949152542,
      "grad_norm": 10.308332443237305,
      "learning_rate": 6.276483050847458e-06,
      "loss": 29.6051,
      "step": 703
    },
    {
      "epoch": 0.3728813559322034,
      "grad_norm": 10.397635459899902,
      "learning_rate": 6.271186440677966e-06,
      "loss": 30.0706,
      "step": 704
    },
    {
      "epoch": 0.3734110169491525,
      "grad_norm": 10.870616912841797,
      "learning_rate": 6.265889830508475e-06,
      "loss": 30.9923,
      "step": 705
    },
    {
      "epoch": 0.3739406779661017,
      "grad_norm": 10.170995712280273,
      "learning_rate": 6.2605932203389845e-06,
      "loss": 30.2146,
      "step": 706
    },
    {
      "epoch": 0.3744703389830508,
      "grad_norm": 11.816580772399902,
      "learning_rate": 6.255296610169492e-06,
      "loss": 30.594,
      "step": 707
    },
    {
      "epoch": 0.375,
      "grad_norm": 11.366488456726074,
      "learning_rate": 6.25e-06,
      "loss": 29.9678,
      "step": 708
    },
    {
      "epoch": 0.3755296610169492,
      "grad_norm": 12.06419563293457,
      "learning_rate": 6.244703389830509e-06,
      "loss": 31.526,
      "step": 709
    },
    {
      "epoch": 0.3760593220338983,
      "grad_norm": 10.20919418334961,
      "learning_rate": 6.239406779661017e-06,
      "loss": 29.203,
      "step": 710
    },
    {
      "epoch": 0.3765889830508475,
      "grad_norm": 11.471548080444336,
      "learning_rate": 6.2341101694915265e-06,
      "loss": 30.7708,
      "step": 711
    },
    {
      "epoch": 0.3771186440677966,
      "grad_norm": 10.844676971435547,
      "learning_rate": 6.2288135593220344e-06,
      "loss": 29.8013,
      "step": 712
    },
    {
      "epoch": 0.3776483050847458,
      "grad_norm": 12.203682899475098,
      "learning_rate": 6.223516949152542e-06,
      "loss": 30.1095,
      "step": 713
    },
    {
      "epoch": 0.3781779661016949,
      "grad_norm": 11.840448379516602,
      "learning_rate": 6.218220338983051e-06,
      "loss": 31.1357,
      "step": 714
    },
    {
      "epoch": 0.3787076271186441,
      "grad_norm": 12.020177841186523,
      "learning_rate": 6.21292372881356e-06,
      "loss": 30.7523,
      "step": 715
    },
    {
      "epoch": 0.3792372881355932,
      "grad_norm": 9.973782539367676,
      "learning_rate": 6.207627118644069e-06,
      "loss": 30.7837,
      "step": 716
    },
    {
      "epoch": 0.3797669491525424,
      "grad_norm": 10.088130950927734,
      "learning_rate": 6.2023305084745765e-06,
      "loss": 30.2049,
      "step": 717
    },
    {
      "epoch": 0.3802966101694915,
      "grad_norm": 10.764695167541504,
      "learning_rate": 6.197033898305085e-06,
      "loss": 30.8609,
      "step": 718
    },
    {
      "epoch": 0.3808262711864407,
      "grad_norm": 11.877981185913086,
      "learning_rate": 6.191737288135594e-06,
      "loss": 31.3378,
      "step": 719
    },
    {
      "epoch": 0.3813559322033898,
      "grad_norm": 9.268331527709961,
      "learning_rate": 6.186440677966103e-06,
      "loss": 28.5752,
      "step": 720
    },
    {
      "epoch": 0.381885593220339,
      "grad_norm": 9.026597023010254,
      "learning_rate": 6.181144067796611e-06,
      "loss": 28.9631,
      "step": 721
    },
    {
      "epoch": 0.3824152542372881,
      "grad_norm": 10.39443588256836,
      "learning_rate": 6.1758474576271185e-06,
      "loss": 29.4135,
      "step": 722
    },
    {
      "epoch": 0.3829449152542373,
      "grad_norm": 10.477686882019043,
      "learning_rate": 6.170550847457628e-06,
      "loss": 30.8673,
      "step": 723
    },
    {
      "epoch": 0.3834745762711864,
      "grad_norm": 10.5062894821167,
      "learning_rate": 6.165254237288136e-06,
      "loss": 29.8827,
      "step": 724
    },
    {
      "epoch": 0.3840042372881356,
      "grad_norm": 11.332282066345215,
      "learning_rate": 6.159957627118645e-06,
      "loss": 29.0742,
      "step": 725
    },
    {
      "epoch": 0.3845338983050847,
      "grad_norm": 10.808051109313965,
      "learning_rate": 6.154661016949153e-06,
      "loss": 30.131,
      "step": 726
    },
    {
      "epoch": 0.3850635593220339,
      "grad_norm": 10.050503730773926,
      "learning_rate": 6.149364406779662e-06,
      "loss": 30.5843,
      "step": 727
    },
    {
      "epoch": 0.3855932203389831,
      "grad_norm": 10.530410766601562,
      "learning_rate": 6.14406779661017e-06,
      "loss": 30.3022,
      "step": 728
    },
    {
      "epoch": 0.3861228813559322,
      "grad_norm": 11.05832576751709,
      "learning_rate": 6.138771186440678e-06,
      "loss": 30.3935,
      "step": 729
    },
    {
      "epoch": 0.3866525423728814,
      "grad_norm": 11.96713638305664,
      "learning_rate": 6.133474576271187e-06,
      "loss": 29.3696,
      "step": 730
    },
    {
      "epoch": 0.3871822033898305,
      "grad_norm": 11.17237377166748,
      "learning_rate": 6.128177966101695e-06,
      "loss": 29.9438,
      "step": 731
    },
    {
      "epoch": 0.3877118644067797,
      "grad_norm": 12.235527992248535,
      "learning_rate": 6.122881355932204e-06,
      "loss": 31.4271,
      "step": 732
    },
    {
      "epoch": 0.3882415254237288,
      "grad_norm": 10.416925430297852,
      "learning_rate": 6.117584745762712e-06,
      "loss": 29.8247,
      "step": 733
    },
    {
      "epoch": 0.388771186440678,
      "grad_norm": 8.910356521606445,
      "learning_rate": 6.112288135593221e-06,
      "loss": 29.0345,
      "step": 734
    },
    {
      "epoch": 0.3893008474576271,
      "grad_norm": 10.938699722290039,
      "learning_rate": 6.106991525423729e-06,
      "loss": 31.6115,
      "step": 735
    },
    {
      "epoch": 0.3898305084745763,
      "grad_norm": 11.411805152893066,
      "learning_rate": 6.1016949152542385e-06,
      "loss": 30.762,
      "step": 736
    },
    {
      "epoch": 0.3903601694915254,
      "grad_norm": 12.502922058105469,
      "learning_rate": 6.096398305084746e-06,
      "loss": 30.7948,
      "step": 737
    },
    {
      "epoch": 0.3908898305084746,
      "grad_norm": 14.986532211303711,
      "learning_rate": 6.091101694915254e-06,
      "loss": 29.2434,
      "step": 738
    },
    {
      "epoch": 0.3914194915254237,
      "grad_norm": 10.717660903930664,
      "learning_rate": 6.085805084745763e-06,
      "loss": 30.1264,
      "step": 739
    },
    {
      "epoch": 0.3919491525423729,
      "grad_norm": 10.593116760253906,
      "learning_rate": 6.080508474576272e-06,
      "loss": 29.2903,
      "step": 740
    },
    {
      "epoch": 0.392478813559322,
      "grad_norm": 10.818785667419434,
      "learning_rate": 6.0752118644067805e-06,
      "loss": 29.5001,
      "step": 741
    },
    {
      "epoch": 0.3930084745762712,
      "grad_norm": 10.588887214660645,
      "learning_rate": 6.069915254237288e-06,
      "loss": 28.7931,
      "step": 742
    },
    {
      "epoch": 0.3935381355932203,
      "grad_norm": 11.391775131225586,
      "learning_rate": 6.064618644067797e-06,
      "loss": 30.8896,
      "step": 743
    },
    {
      "epoch": 0.3940677966101695,
      "grad_norm": 10.542014122009277,
      "learning_rate": 6.059322033898306e-06,
      "loss": 29.1755,
      "step": 744
    },
    {
      "epoch": 0.3945974576271186,
      "grad_norm": 10.342534065246582,
      "learning_rate": 6.054025423728815e-06,
      "loss": 29.162,
      "step": 745
    },
    {
      "epoch": 0.3951271186440678,
      "grad_norm": 10.462386131286621,
      "learning_rate": 6.0487288135593226e-06,
      "loss": 29.4285,
      "step": 746
    },
    {
      "epoch": 0.3956567796610169,
      "grad_norm": 10.792566299438477,
      "learning_rate": 6.0434322033898305e-06,
      "loss": 29.7651,
      "step": 747
    },
    {
      "epoch": 0.3961864406779661,
      "grad_norm": 9.95241928100586,
      "learning_rate": 6.038135593220339e-06,
      "loss": 29.1477,
      "step": 748
    },
    {
      "epoch": 0.3967161016949153,
      "grad_norm": 10.602727890014648,
      "learning_rate": 6.032838983050848e-06,
      "loss": 30.7898,
      "step": 749
    },
    {
      "epoch": 0.3972457627118644,
      "grad_norm": 9.646119117736816,
      "learning_rate": 6.027542372881357e-06,
      "loss": 29.1185,
      "step": 750
    },
    {
      "epoch": 0.3977754237288136,
      "grad_norm": 9.457530975341797,
      "learning_rate": 6.022245762711865e-06,
      "loss": 29.9376,
      "step": 751
    },
    {
      "epoch": 0.3983050847457627,
      "grad_norm": 11.559026718139648,
      "learning_rate": 6.0169491525423725e-06,
      "loss": 29.4526,
      "step": 752
    },
    {
      "epoch": 0.3988347457627119,
      "grad_norm": 9.269049644470215,
      "learning_rate": 6.011652542372882e-06,
      "loss": 28.6881,
      "step": 753
    },
    {
      "epoch": 0.399364406779661,
      "grad_norm": 9.873361587524414,
      "learning_rate": 6.00635593220339e-06,
      "loss": 29.3317,
      "step": 754
    },
    {
      "epoch": 0.3998940677966102,
      "grad_norm": 8.83154010772705,
      "learning_rate": 6.001059322033899e-06,
      "loss": 29.1224,
      "step": 755
    },
    {
      "epoch": 0.4004237288135593,
      "grad_norm": 10.458867073059082,
      "learning_rate": 5.995762711864407e-06,
      "loss": 29.7582,
      "step": 756
    },
    {
      "epoch": 0.4009533898305085,
      "grad_norm": 9.275065422058105,
      "learning_rate": 5.990466101694916e-06,
      "loss": 28.7446,
      "step": 757
    },
    {
      "epoch": 0.4014830508474576,
      "grad_norm": 10.518202781677246,
      "learning_rate": 5.985169491525424e-06,
      "loss": 30.101,
      "step": 758
    },
    {
      "epoch": 0.4020127118644068,
      "grad_norm": 9.685051918029785,
      "learning_rate": 5.979872881355933e-06,
      "loss": 28.246,
      "step": 759
    },
    {
      "epoch": 0.4025423728813559,
      "grad_norm": 9.75174331665039,
      "learning_rate": 5.974576271186441e-06,
      "loss": 29.0553,
      "step": 760
    },
    {
      "epoch": 0.4030720338983051,
      "grad_norm": 9.261038780212402,
      "learning_rate": 5.96927966101695e-06,
      "loss": 28.0333,
      "step": 761
    },
    {
      "epoch": 0.4036016949152542,
      "grad_norm": 9.992767333984375,
      "learning_rate": 5.963983050847458e-06,
      "loss": 29.722,
      "step": 762
    },
    {
      "epoch": 0.4041313559322034,
      "grad_norm": 32.98759460449219,
      "learning_rate": 5.958686440677966e-06,
      "loss": 29.3736,
      "step": 763
    },
    {
      "epoch": 0.4046610169491525,
      "grad_norm": 10.725773811340332,
      "learning_rate": 5.953389830508475e-06,
      "loss": 30.1189,
      "step": 764
    },
    {
      "epoch": 0.4051906779661017,
      "grad_norm": 10.089203834533691,
      "learning_rate": 5.948093220338984e-06,
      "loss": 29.1657,
      "step": 765
    },
    {
      "epoch": 0.4057203389830508,
      "grad_norm": 11.682673454284668,
      "learning_rate": 5.9427966101694924e-06,
      "loss": 30.01,
      "step": 766
    },
    {
      "epoch": 0.40625,
      "grad_norm": 9.922711372375488,
      "learning_rate": 5.9375e-06,
      "loss": 29.3861,
      "step": 767
    },
    {
      "epoch": 0.4067796610169492,
      "grad_norm": 10.5263671875,
      "learning_rate": 5.932203389830509e-06,
      "loss": 28.2906,
      "step": 768
    },
    {
      "epoch": 0.4073093220338983,
      "grad_norm": 10.713048934936523,
      "learning_rate": 5.926906779661017e-06,
      "loss": 29.8628,
      "step": 769
    },
    {
      "epoch": 0.4078389830508475,
      "grad_norm": 9.705453872680664,
      "learning_rate": 5.9216101694915266e-06,
      "loss": 28.9218,
      "step": 770
    },
    {
      "epoch": 0.4083686440677966,
      "grad_norm": 8.391393661499023,
      "learning_rate": 5.9163135593220345e-06,
      "loss": 28.2795,
      "step": 771
    },
    {
      "epoch": 0.4088983050847458,
      "grad_norm": 12.250814437866211,
      "learning_rate": 5.911016949152542e-06,
      "loss": 29.1466,
      "step": 772
    },
    {
      "epoch": 0.4094279661016949,
      "grad_norm": 9.221171379089355,
      "learning_rate": 5.905720338983051e-06,
      "loss": 30.1141,
      "step": 773
    },
    {
      "epoch": 0.4099576271186441,
      "grad_norm": 10.59292221069336,
      "learning_rate": 5.90042372881356e-06,
      "loss": 28.1315,
      "step": 774
    },
    {
      "epoch": 0.4104872881355932,
      "grad_norm": 11.405505180358887,
      "learning_rate": 5.895127118644069e-06,
      "loss": 29.3387,
      "step": 775
    },
    {
      "epoch": 0.4110169491525424,
      "grad_norm": 11.156059265136719,
      "learning_rate": 5.8898305084745765e-06,
      "loss": 29.0197,
      "step": 776
    },
    {
      "epoch": 0.4115466101694915,
      "grad_norm": 9.85000228881836,
      "learning_rate": 5.884533898305084e-06,
      "loss": 29.1187,
      "step": 777
    },
    {
      "epoch": 0.4120762711864407,
      "grad_norm": 12.187344551086426,
      "learning_rate": 5.879237288135594e-06,
      "loss": 29.5278,
      "step": 778
    },
    {
      "epoch": 0.4126059322033898,
      "grad_norm": 10.458483695983887,
      "learning_rate": 5.873940677966103e-06,
      "loss": 28.5985,
      "step": 779
    },
    {
      "epoch": 0.413135593220339,
      "grad_norm": 8.854232788085938,
      "learning_rate": 5.868644067796611e-06,
      "loss": 28.0535,
      "step": 780
    },
    {
      "epoch": 0.4136652542372881,
      "grad_norm": 12.585275650024414,
      "learning_rate": 5.8633474576271186e-06,
      "loss": 30.9786,
      "step": 781
    },
    {
      "epoch": 0.4141949152542373,
      "grad_norm": 9.899686813354492,
      "learning_rate": 5.858050847457628e-06,
      "loss": 28.3618,
      "step": 782
    },
    {
      "epoch": 0.4147245762711864,
      "grad_norm": 10.474006652832031,
      "learning_rate": 5.852754237288136e-06,
      "loss": 29.9129,
      "step": 783
    },
    {
      "epoch": 0.4152542372881356,
      "grad_norm": 9.118659019470215,
      "learning_rate": 5.847457627118645e-06,
      "loss": 28.6029,
      "step": 784
    },
    {
      "epoch": 0.4157838983050847,
      "grad_norm": 10.777518272399902,
      "learning_rate": 5.842161016949153e-06,
      "loss": 28.7458,
      "step": 785
    },
    {
      "epoch": 0.4163135593220339,
      "grad_norm": 10.025184631347656,
      "learning_rate": 5.836864406779662e-06,
      "loss": 28.6059,
      "step": 786
    },
    {
      "epoch": 0.4168432203389831,
      "grad_norm": 10.79825496673584,
      "learning_rate": 5.83156779661017e-06,
      "loss": 28.9304,
      "step": 787
    },
    {
      "epoch": 0.4173728813559322,
      "grad_norm": 11.882911682128906,
      "learning_rate": 5.826271186440678e-06,
      "loss": 29.7042,
      "step": 788
    },
    {
      "epoch": 0.4179025423728814,
      "grad_norm": 11.559529304504395,
      "learning_rate": 5.820974576271187e-06,
      "loss": 28.9925,
      "step": 789
    },
    {
      "epoch": 0.4184322033898305,
      "grad_norm": 10.472793579101562,
      "learning_rate": 5.815677966101695e-06,
      "loss": 29.3911,
      "step": 790
    },
    {
      "epoch": 0.4189618644067797,
      "grad_norm": 11.390270233154297,
      "learning_rate": 5.810381355932204e-06,
      "loss": 28.6883,
      "step": 791
    },
    {
      "epoch": 0.4194915254237288,
      "grad_norm": 9.824049949645996,
      "learning_rate": 5.805084745762712e-06,
      "loss": 28.8316,
      "step": 792
    },
    {
      "epoch": 0.420021186440678,
      "grad_norm": 10.7042875289917,
      "learning_rate": 5.799788135593221e-06,
      "loss": 29.2831,
      "step": 793
    },
    {
      "epoch": 0.4205508474576271,
      "grad_norm": 9.336030960083008,
      "learning_rate": 5.794491525423729e-06,
      "loss": 28.2021,
      "step": 794
    },
    {
      "epoch": 0.4210805084745763,
      "grad_norm": 14.876826286315918,
      "learning_rate": 5.7891949152542385e-06,
      "loss": 29.6163,
      "step": 795
    },
    {
      "epoch": 0.4216101694915254,
      "grad_norm": 12.191364288330078,
      "learning_rate": 5.783898305084746e-06,
      "loss": 27.6402,
      "step": 796
    },
    {
      "epoch": 0.4221398305084746,
      "grad_norm": 10.234912872314453,
      "learning_rate": 5.778601694915254e-06,
      "loss": 28.5537,
      "step": 797
    },
    {
      "epoch": 0.4226694915254237,
      "grad_norm": 22.142990112304688,
      "learning_rate": 5.773305084745763e-06,
      "loss": 28.3359,
      "step": 798
    },
    {
      "epoch": 0.4231991525423729,
      "grad_norm": 10.234535217285156,
      "learning_rate": 5.768008474576272e-06,
      "loss": 29.254,
      "step": 799
    },
    {
      "epoch": 0.423728813559322,
      "grad_norm": 13.76799488067627,
      "learning_rate": 5.7627118644067805e-06,
      "loss": 28.4106,
      "step": 800
    },
    {
      "epoch": 0.4242584745762712,
      "grad_norm": 11.15936279296875,
      "learning_rate": 5.7574152542372884e-06,
      "loss": 29.2017,
      "step": 801
    },
    {
      "epoch": 0.4247881355932203,
      "grad_norm": 10.992734909057617,
      "learning_rate": 5.752118644067796e-06,
      "loss": 28.2742,
      "step": 802
    },
    {
      "epoch": 0.4253177966101695,
      "grad_norm": 8.67967414855957,
      "learning_rate": 5.746822033898306e-06,
      "loss": 28.1277,
      "step": 803
    },
    {
      "epoch": 0.4258474576271186,
      "grad_norm": 9.953408241271973,
      "learning_rate": 5.741525423728815e-06,
      "loss": 29.111,
      "step": 804
    },
    {
      "epoch": 0.4263771186440678,
      "grad_norm": 10.097567558288574,
      "learning_rate": 5.736228813559323e-06,
      "loss": 28.6606,
      "step": 805
    },
    {
      "epoch": 0.4269067796610169,
      "grad_norm": 10.583150863647461,
      "learning_rate": 5.7309322033898305e-06,
      "loss": 28.6022,
      "step": 806
    },
    {
      "epoch": 0.4274364406779661,
      "grad_norm": 10.849091529846191,
      "learning_rate": 5.725635593220339e-06,
      "loss": 29.6588,
      "step": 807
    },
    {
      "epoch": 0.4279661016949153,
      "grad_norm": 9.775583267211914,
      "learning_rate": 5.720338983050848e-06,
      "loss": 28.4527,
      "step": 808
    },
    {
      "epoch": 0.4284957627118644,
      "grad_norm": 9.362996101379395,
      "learning_rate": 5.715042372881357e-06,
      "loss": 28.7144,
      "step": 809
    },
    {
      "epoch": 0.4290254237288136,
      "grad_norm": 10.046856880187988,
      "learning_rate": 5.709745762711865e-06,
      "loss": 28.5578,
      "step": 810
    },
    {
      "epoch": 0.4295550847457627,
      "grad_norm": 10.672782897949219,
      "learning_rate": 5.7044491525423725e-06,
      "loss": 29.1543,
      "step": 811
    },
    {
      "epoch": 0.4300847457627119,
      "grad_norm": 11.831570625305176,
      "learning_rate": 5.699152542372882e-06,
      "loss": 28.0049,
      "step": 812
    },
    {
      "epoch": 0.430614406779661,
      "grad_norm": 9.407232284545898,
      "learning_rate": 5.69385593220339e-06,
      "loss": 28.4006,
      "step": 813
    },
    {
      "epoch": 0.4311440677966102,
      "grad_norm": 10.519378662109375,
      "learning_rate": 5.688559322033899e-06,
      "loss": 29.4552,
      "step": 814
    },
    {
      "epoch": 0.4316737288135593,
      "grad_norm": 10.75218677520752,
      "learning_rate": 5.683262711864407e-06,
      "loss": 27.8656,
      "step": 815
    },
    {
      "epoch": 0.4322033898305085,
      "grad_norm": 9.487586975097656,
      "learning_rate": 5.677966101694916e-06,
      "loss": 28.8643,
      "step": 816
    },
    {
      "epoch": 0.4327330508474576,
      "grad_norm": 9.987401962280273,
      "learning_rate": 5.672669491525424e-06,
      "loss": 28.9192,
      "step": 817
    },
    {
      "epoch": 0.4332627118644068,
      "grad_norm": 12.139631271362305,
      "learning_rate": 5.667372881355933e-06,
      "loss": 28.874,
      "step": 818
    },
    {
      "epoch": 0.4337923728813559,
      "grad_norm": 11.81332778930664,
      "learning_rate": 5.662076271186441e-06,
      "loss": 29.9232,
      "step": 819
    },
    {
      "epoch": 0.4343220338983051,
      "grad_norm": 12.554872512817383,
      "learning_rate": 5.65677966101695e-06,
      "loss": 26.943,
      "step": 820
    },
    {
      "epoch": 0.4348516949152542,
      "grad_norm": 10.098166465759277,
      "learning_rate": 5.651483050847458e-06,
      "loss": 28.429,
      "step": 821
    },
    {
      "epoch": 0.4353813559322034,
      "grad_norm": 10.163734436035156,
      "learning_rate": 5.646186440677966e-06,
      "loss": 28.3057,
      "step": 822
    },
    {
      "epoch": 0.4359110169491525,
      "grad_norm": 9.728160858154297,
      "learning_rate": 5.640889830508475e-06,
      "loss": 28.5924,
      "step": 823
    },
    {
      "epoch": 0.4364406779661017,
      "grad_norm": 11.73422908782959,
      "learning_rate": 5.635593220338984e-06,
      "loss": 27.3522,
      "step": 824
    },
    {
      "epoch": 0.4369703389830508,
      "grad_norm": 10.765636444091797,
      "learning_rate": 5.6302966101694925e-06,
      "loss": 29.5835,
      "step": 825
    },
    {
      "epoch": 0.4375,
      "grad_norm": 10.88581657409668,
      "learning_rate": 5.625e-06,
      "loss": 28.1111,
      "step": 826
    },
    {
      "epoch": 0.4380296610169492,
      "grad_norm": 10.576986312866211,
      "learning_rate": 5.619703389830508e-06,
      "loss": 28.3323,
      "step": 827
    },
    {
      "epoch": 0.4385593220338983,
      "grad_norm": 9.725213050842285,
      "learning_rate": 5.614406779661017e-06,
      "loss": 27.2161,
      "step": 828
    },
    {
      "epoch": 0.4390889830508475,
      "grad_norm": 10.266420364379883,
      "learning_rate": 5.609110169491527e-06,
      "loss": 27.0632,
      "step": 829
    },
    {
      "epoch": 0.4396186440677966,
      "grad_norm": 10.568148612976074,
      "learning_rate": 5.6038135593220345e-06,
      "loss": 28.2226,
      "step": 830
    },
    {
      "epoch": 0.4401483050847458,
      "grad_norm": 14.217046737670898,
      "learning_rate": 5.598516949152542e-06,
      "loss": 28.4379,
      "step": 831
    },
    {
      "epoch": 0.4406779661016949,
      "grad_norm": 9.85610580444336,
      "learning_rate": 5.593220338983051e-06,
      "loss": 27.7795,
      "step": 832
    },
    {
      "epoch": 0.4412076271186441,
      "grad_norm": 10.263837814331055,
      "learning_rate": 5.58792372881356e-06,
      "loss": 28.3396,
      "step": 833
    },
    {
      "epoch": 0.4417372881355932,
      "grad_norm": 10.403985977172852,
      "learning_rate": 5.582627118644069e-06,
      "loss": 29.3269,
      "step": 834
    },
    {
      "epoch": 0.4422669491525424,
      "grad_norm": 10.483768463134766,
      "learning_rate": 5.5773305084745765e-06,
      "loss": 28.24,
      "step": 835
    },
    {
      "epoch": 0.4427966101694915,
      "grad_norm": 11.245745658874512,
      "learning_rate": 5.5720338983050844e-06,
      "loss": 27.5316,
      "step": 836
    },
    {
      "epoch": 0.4433262711864407,
      "grad_norm": 14.56145191192627,
      "learning_rate": 5.566737288135594e-06,
      "loss": 29.2829,
      "step": 837
    },
    {
      "epoch": 0.4438559322033898,
      "grad_norm": 8.82700252532959,
      "learning_rate": 5.561440677966102e-06,
      "loss": 28.1516,
      "step": 838
    },
    {
      "epoch": 0.444385593220339,
      "grad_norm": 10.627715110778809,
      "learning_rate": 5.556144067796611e-06,
      "loss": 29.5228,
      "step": 839
    },
    {
      "epoch": 0.4449152542372881,
      "grad_norm": 14.896368980407715,
      "learning_rate": 5.550847457627119e-06,
      "loss": 27.2085,
      "step": 840
    },
    {
      "epoch": 0.4454449152542373,
      "grad_norm": 11.01024055480957,
      "learning_rate": 5.545550847457628e-06,
      "loss": 27.4646,
      "step": 841
    },
    {
      "epoch": 0.4459745762711864,
      "grad_norm": 10.873217582702637,
      "learning_rate": 5.540254237288136e-06,
      "loss": 28.0129,
      "step": 842
    },
    {
      "epoch": 0.4465042372881356,
      "grad_norm": 10.352852821350098,
      "learning_rate": 5.534957627118645e-06,
      "loss": 27.5614,
      "step": 843
    },
    {
      "epoch": 0.4470338983050847,
      "grad_norm": 11.687589645385742,
      "learning_rate": 5.529661016949153e-06,
      "loss": 29.5268,
      "step": 844
    },
    {
      "epoch": 0.4475635593220339,
      "grad_norm": 9.899469375610352,
      "learning_rate": 5.524364406779662e-06,
      "loss": 28.2152,
      "step": 845
    },
    {
      "epoch": 0.4480932203389831,
      "grad_norm": 9.662514686584473,
      "learning_rate": 5.51906779661017e-06,
      "loss": 28.2167,
      "step": 846
    },
    {
      "epoch": 0.4486228813559322,
      "grad_norm": 9.850876808166504,
      "learning_rate": 5.513771186440678e-06,
      "loss": 28.0072,
      "step": 847
    },
    {
      "epoch": 0.4491525423728814,
      "grad_norm": 10.014632225036621,
      "learning_rate": 5.508474576271187e-06,
      "loss": 27.615,
      "step": 848
    },
    {
      "epoch": 0.4496822033898305,
      "grad_norm": 13.800902366638184,
      "learning_rate": 5.503177966101695e-06,
      "loss": 28.6898,
      "step": 849
    },
    {
      "epoch": 0.4502118644067797,
      "grad_norm": 10.42664909362793,
      "learning_rate": 5.497881355932204e-06,
      "loss": 27.628,
      "step": 850
    },
    {
      "epoch": 0.4507415254237288,
      "grad_norm": 9.69643497467041,
      "learning_rate": 5.492584745762712e-06,
      "loss": 27.7852,
      "step": 851
    },
    {
      "epoch": 0.451271186440678,
      "grad_norm": 9.512052536010742,
      "learning_rate": 5.487288135593221e-06,
      "loss": 27.0548,
      "step": 852
    },
    {
      "epoch": 0.4518008474576271,
      "grad_norm": 10.42075252532959,
      "learning_rate": 5.481991525423729e-06,
      "loss": 28.3221,
      "step": 853
    },
    {
      "epoch": 0.4523305084745763,
      "grad_norm": 17.828882217407227,
      "learning_rate": 5.4766949152542385e-06,
      "loss": 26.7601,
      "step": 854
    },
    {
      "epoch": 0.4528601694915254,
      "grad_norm": 11.89970874786377,
      "learning_rate": 5.471398305084746e-06,
      "loss": 28.067,
      "step": 855
    },
    {
      "epoch": 0.4533898305084746,
      "grad_norm": 10.690147399902344,
      "learning_rate": 5.466101694915254e-06,
      "loss": 27.4306,
      "step": 856
    },
    {
      "epoch": 0.4539194915254237,
      "grad_norm": 10.33769416809082,
      "learning_rate": 5.460805084745763e-06,
      "loss": 28.9995,
      "step": 857
    },
    {
      "epoch": 0.4544491525423729,
      "grad_norm": 9.498560905456543,
      "learning_rate": 5.455508474576272e-06,
      "loss": 27.2879,
      "step": 858
    },
    {
      "epoch": 0.454978813559322,
      "grad_norm": 17.093935012817383,
      "learning_rate": 5.4502118644067806e-06,
      "loss": 27.9778,
      "step": 859
    },
    {
      "epoch": 0.4555084745762712,
      "grad_norm": 11.060958862304688,
      "learning_rate": 5.4449152542372885e-06,
      "loss": 27.6089,
      "step": 860
    },
    {
      "epoch": 0.4560381355932203,
      "grad_norm": 11.043152809143066,
      "learning_rate": 5.439618644067796e-06,
      "loss": 27.766,
      "step": 861
    },
    {
      "epoch": 0.4565677966101695,
      "grad_norm": 10.996400833129883,
      "learning_rate": 5.434322033898306e-06,
      "loss": 28.3465,
      "step": 862
    },
    {
      "epoch": 0.4570974576271186,
      "grad_norm": 9.528584480285645,
      "learning_rate": 5.429025423728814e-06,
      "loss": 27.3534,
      "step": 863
    },
    {
      "epoch": 0.4576271186440678,
      "grad_norm": 9.094212532043457,
      "learning_rate": 5.423728813559323e-06,
      "loss": 26.7011,
      "step": 864
    },
    {
      "epoch": 0.4581567796610169,
      "grad_norm": 9.437712669372559,
      "learning_rate": 5.4184322033898305e-06,
      "loss": 27.6492,
      "step": 865
    },
    {
      "epoch": 0.4586864406779661,
      "grad_norm": 13.829645156860352,
      "learning_rate": 5.413135593220339e-06,
      "loss": 28.0816,
      "step": 866
    },
    {
      "epoch": 0.4592161016949153,
      "grad_norm": 10.146621704101562,
      "learning_rate": 5.407838983050848e-06,
      "loss": 26.9902,
      "step": 867
    },
    {
      "epoch": 0.4597457627118644,
      "grad_norm": 11.155838012695312,
      "learning_rate": 5.402542372881357e-06,
      "loss": 26.5905,
      "step": 868
    },
    {
      "epoch": 0.4602754237288136,
      "grad_norm": 12.211896896362305,
      "learning_rate": 5.397245762711865e-06,
      "loss": 27.7173,
      "step": 869
    },
    {
      "epoch": 0.4608050847457627,
      "grad_norm": 11.949997901916504,
      "learning_rate": 5.3919491525423726e-06,
      "loss": 29.27,
      "step": 870
    },
    {
      "epoch": 0.4613347457627119,
      "grad_norm": 8.66355037689209,
      "learning_rate": 5.386652542372882e-06,
      "loss": 26.6558,
      "step": 871
    },
    {
      "epoch": 0.461864406779661,
      "grad_norm": 11.9173002243042,
      "learning_rate": 5.38135593220339e-06,
      "loss": 27.8841,
      "step": 872
    },
    {
      "epoch": 0.4623940677966102,
      "grad_norm": 53.54058837890625,
      "learning_rate": 5.376059322033899e-06,
      "loss": 27.0484,
      "step": 873
    },
    {
      "epoch": 0.4629237288135593,
      "grad_norm": 10.06408405303955,
      "learning_rate": 5.370762711864407e-06,
      "loss": 27.4022,
      "step": 874
    },
    {
      "epoch": 0.4634533898305085,
      "grad_norm": 9.05440902709961,
      "learning_rate": 5.365466101694916e-06,
      "loss": 26.971,
      "step": 875
    },
    {
      "epoch": 0.4639830508474576,
      "grad_norm": 10.499275207519531,
      "learning_rate": 5.360169491525424e-06,
      "loss": 28.3019,
      "step": 876
    },
    {
      "epoch": 0.4645127118644068,
      "grad_norm": 9.981285095214844,
      "learning_rate": 5.354872881355933e-06,
      "loss": 26.96,
      "step": 877
    },
    {
      "epoch": 0.4650423728813559,
      "grad_norm": 9.873538970947266,
      "learning_rate": 5.349576271186441e-06,
      "loss": 28.1036,
      "step": 878
    },
    {
      "epoch": 0.4655720338983051,
      "grad_norm": 10.589699745178223,
      "learning_rate": 5.3442796610169504e-06,
      "loss": 28.9047,
      "step": 879
    },
    {
      "epoch": 0.4661016949152542,
      "grad_norm": 10.086525917053223,
      "learning_rate": 5.338983050847458e-06,
      "loss": 27.746,
      "step": 880
    },
    {
      "epoch": 0.4666313559322034,
      "grad_norm": 10.325167655944824,
      "learning_rate": 5.333686440677966e-06,
      "loss": 28.4326,
      "step": 881
    },
    {
      "epoch": 0.4671610169491525,
      "grad_norm": 11.666194915771484,
      "learning_rate": 5.328389830508475e-06,
      "loss": 27.2949,
      "step": 882
    },
    {
      "epoch": 0.4676906779661017,
      "grad_norm": 247.8000946044922,
      "learning_rate": 5.323093220338984e-06,
      "loss": 27.9071,
      "step": 883
    },
    {
      "epoch": 0.4682203389830508,
      "grad_norm": 10.157005310058594,
      "learning_rate": 5.3177966101694925e-06,
      "loss": 25.9674,
      "step": 884
    },
    {
      "epoch": 0.46875,
      "grad_norm": 10.911566734313965,
      "learning_rate": 5.3125e-06,
      "loss": 27.8916,
      "step": 885
    },
    {
      "epoch": 0.4692796610169492,
      "grad_norm": 9.44286823272705,
      "learning_rate": 5.307203389830508e-06,
      "loss": 28.4255,
      "step": 886
    },
    {
      "epoch": 0.4698093220338983,
      "grad_norm": 9.391643524169922,
      "learning_rate": 5.301906779661017e-06,
      "loss": 28.5187,
      "step": 887
    },
    {
      "epoch": 0.4703389830508475,
      "grad_norm": 10.829914093017578,
      "learning_rate": 5.296610169491526e-06,
      "loss": 28.3101,
      "step": 888
    },
    {
      "epoch": 0.4708686440677966,
      "grad_norm": 12.18858814239502,
      "learning_rate": 5.2913135593220345e-06,
      "loss": 27.0851,
      "step": 889
    },
    {
      "epoch": 0.4713983050847458,
      "grad_norm": 9.445677757263184,
      "learning_rate": 5.2860169491525424e-06,
      "loss": 27.2459,
      "step": 890
    },
    {
      "epoch": 0.4719279661016949,
      "grad_norm": 10.952392578125,
      "learning_rate": 5.280720338983051e-06,
      "loss": 27.4948,
      "step": 891
    },
    {
      "epoch": 0.4724576271186441,
      "grad_norm": 9.526817321777344,
      "learning_rate": 5.27542372881356e-06,
      "loss": 26.7622,
      "step": 892
    },
    {
      "epoch": 0.4729872881355932,
      "grad_norm": 9.436016082763672,
      "learning_rate": 5.270127118644069e-06,
      "loss": 27.8001,
      "step": 893
    },
    {
      "epoch": 0.4735169491525424,
      "grad_norm": 10.718568801879883,
      "learning_rate": 5.2648305084745766e-06,
      "loss": 27.3853,
      "step": 894
    },
    {
      "epoch": 0.4740466101694915,
      "grad_norm": 10.829487800598145,
      "learning_rate": 5.2595338983050845e-06,
      "loss": 26.5659,
      "step": 895
    },
    {
      "epoch": 0.4745762711864407,
      "grad_norm": 10.53178882598877,
      "learning_rate": 5.254237288135594e-06,
      "loss": 27.2941,
      "step": 896
    },
    {
      "epoch": 0.4751059322033898,
      "grad_norm": 10.14508056640625,
      "learning_rate": 5.248940677966102e-06,
      "loss": 27.3297,
      "step": 897
    },
    {
      "epoch": 0.475635593220339,
      "grad_norm": 9.358980178833008,
      "learning_rate": 5.243644067796611e-06,
      "loss": 26.9543,
      "step": 898
    },
    {
      "epoch": 0.4761652542372881,
      "grad_norm": 11.077170372009277,
      "learning_rate": 5.238347457627119e-06,
      "loss": 27.1523,
      "step": 899
    },
    {
      "epoch": 0.4766949152542373,
      "grad_norm": 12.238937377929688,
      "learning_rate": 5.233050847457628e-06,
      "loss": 27.1253,
      "step": 900
    },
    {
      "epoch": 0.4772245762711864,
      "grad_norm": 10.436279296875,
      "learning_rate": 5.227754237288136e-06,
      "loss": 28.1798,
      "step": 901
    },
    {
      "epoch": 0.4777542372881356,
      "grad_norm": 13.020623207092285,
      "learning_rate": 5.222457627118645e-06,
      "loss": 27.5799,
      "step": 902
    },
    {
      "epoch": 0.4782838983050847,
      "grad_norm": 10.077445983886719,
      "learning_rate": 5.217161016949153e-06,
      "loss": 27.6341,
      "step": 903
    },
    {
      "epoch": 0.4788135593220339,
      "grad_norm": 9.721061706542969,
      "learning_rate": 5.211864406779662e-06,
      "loss": 27.3652,
      "step": 904
    },
    {
      "epoch": 0.4793432203389831,
      "grad_norm": 13.302886962890625,
      "learning_rate": 5.20656779661017e-06,
      "loss": 26.4702,
      "step": 905
    },
    {
      "epoch": 0.4798728813559322,
      "grad_norm": 11.620524406433105,
      "learning_rate": 5.201271186440678e-06,
      "loss": 27.4613,
      "step": 906
    },
    {
      "epoch": 0.4804025423728814,
      "grad_norm": 12.067590713500977,
      "learning_rate": 5.195974576271187e-06,
      "loss": 27.6885,
      "step": 907
    },
    {
      "epoch": 0.4809322033898305,
      "grad_norm": 9.922755241394043,
      "learning_rate": 5.190677966101695e-06,
      "loss": 27.1022,
      "step": 908
    },
    {
      "epoch": 0.4814618644067797,
      "grad_norm": 10.2785062789917,
      "learning_rate": 5.185381355932204e-06,
      "loss": 28.5318,
      "step": 909
    },
    {
      "epoch": 0.4819915254237288,
      "grad_norm": 11.024855613708496,
      "learning_rate": 5.180084745762712e-06,
      "loss": 26.9685,
      "step": 910
    },
    {
      "epoch": 0.482521186440678,
      "grad_norm": 10.544416427612305,
      "learning_rate": 5.17478813559322e-06,
      "loss": 26.7655,
      "step": 911
    },
    {
      "epoch": 0.4830508474576271,
      "grad_norm": 10.009196281433105,
      "learning_rate": 5.169491525423729e-06,
      "loss": 26.4187,
      "step": 912
    },
    {
      "epoch": 0.4835805084745763,
      "grad_norm": 9.589855194091797,
      "learning_rate": 5.164194915254238e-06,
      "loss": 26.3419,
      "step": 913
    },
    {
      "epoch": 0.4841101694915254,
      "grad_norm": 11.893472671508789,
      "learning_rate": 5.1588983050847464e-06,
      "loss": 27.2802,
      "step": 914
    },
    {
      "epoch": 0.4846398305084746,
      "grad_norm": 10.052364349365234,
      "learning_rate": 5.153601694915254e-06,
      "loss": 27.0295,
      "step": 915
    },
    {
      "epoch": 0.4851694915254237,
      "grad_norm": 10.667932510375977,
      "learning_rate": 5.148305084745763e-06,
      "loss": 26.8255,
      "step": 916
    },
    {
      "epoch": 0.4856991525423729,
      "grad_norm": 10.216922760009766,
      "learning_rate": 5.143008474576272e-06,
      "loss": 27.0324,
      "step": 917
    },
    {
      "epoch": 0.486228813559322,
      "grad_norm": 10.818635940551758,
      "learning_rate": 5.137711864406781e-06,
      "loss": 27.1953,
      "step": 918
    },
    {
      "epoch": 0.4867584745762712,
      "grad_norm": 10.570403099060059,
      "learning_rate": 5.1324152542372885e-06,
      "loss": 28.1117,
      "step": 919
    },
    {
      "epoch": 0.4872881355932203,
      "grad_norm": 11.312835693359375,
      "learning_rate": 5.127118644067796e-06,
      "loss": 26.9801,
      "step": 920
    },
    {
      "epoch": 0.4878177966101695,
      "grad_norm": 9.645171165466309,
      "learning_rate": 5.121822033898306e-06,
      "loss": 27.1798,
      "step": 921
    },
    {
      "epoch": 0.4883474576271186,
      "grad_norm": 9.532612800598145,
      "learning_rate": 5.116525423728814e-06,
      "loss": 26.9255,
      "step": 922
    },
    {
      "epoch": 0.4888771186440678,
      "grad_norm": 9.967107772827148,
      "learning_rate": 5.111228813559323e-06,
      "loss": 26.9581,
      "step": 923
    },
    {
      "epoch": 0.4894067796610169,
      "grad_norm": 13.518049240112305,
      "learning_rate": 5.1059322033898305e-06,
      "loss": 27.698,
      "step": 924
    },
    {
      "epoch": 0.4899364406779661,
      "grad_norm": 10.071680068969727,
      "learning_rate": 5.1006355932203384e-06,
      "loss": 27.6396,
      "step": 925
    },
    {
      "epoch": 0.4904661016949153,
      "grad_norm": 11.661785125732422,
      "learning_rate": 5.095338983050848e-06,
      "loss": 26.8492,
      "step": 926
    },
    {
      "epoch": 0.4909957627118644,
      "grad_norm": 9.121270179748535,
      "learning_rate": 5.090042372881357e-06,
      "loss": 27.0899,
      "step": 927
    },
    {
      "epoch": 0.4915254237288136,
      "grad_norm": 10.795284271240234,
      "learning_rate": 5.084745762711865e-06,
      "loss": 26.6423,
      "step": 928
    },
    {
      "epoch": 0.4920550847457627,
      "grad_norm": 10.766474723815918,
      "learning_rate": 5.079449152542373e-06,
      "loss": 26.3293,
      "step": 929
    },
    {
      "epoch": 0.4925847457627119,
      "grad_norm": 9.173215866088867,
      "learning_rate": 5.074152542372882e-06,
      "loss": 26.277,
      "step": 930
    },
    {
      "epoch": 0.493114406779661,
      "grad_norm": 9.696623802185059,
      "learning_rate": 5.06885593220339e-06,
      "loss": 26.1156,
      "step": 931
    },
    {
      "epoch": 0.4936440677966102,
      "grad_norm": 9.236590385437012,
      "learning_rate": 5.063559322033899e-06,
      "loss": 26.5768,
      "step": 932
    },
    {
      "epoch": 0.4941737288135593,
      "grad_norm": 9.92077350616455,
      "learning_rate": 5.058262711864407e-06,
      "loss": 27.8753,
      "step": 933
    },
    {
      "epoch": 0.4947033898305085,
      "grad_norm": 11.020543098449707,
      "learning_rate": 5.052966101694916e-06,
      "loss": 27.0707,
      "step": 934
    },
    {
      "epoch": 0.4952330508474576,
      "grad_norm": 9.93774127960205,
      "learning_rate": 5.047669491525424e-06,
      "loss": 25.9002,
      "step": 935
    },
    {
      "epoch": 0.4957627118644068,
      "grad_norm": 9.408080101013184,
      "learning_rate": 5.042372881355932e-06,
      "loss": 26.666,
      "step": 936
    },
    {
      "epoch": 0.4962923728813559,
      "grad_norm": 9.85637092590332,
      "learning_rate": 5.037076271186441e-06,
      "loss": 27.5374,
      "step": 937
    },
    {
      "epoch": 0.4968220338983051,
      "grad_norm": 15.283719062805176,
      "learning_rate": 5.03177966101695e-06,
      "loss": 26.5395,
      "step": 938
    },
    {
      "epoch": 0.4973516949152542,
      "grad_norm": 9.549177169799805,
      "learning_rate": 5.026483050847458e-06,
      "loss": 26.2252,
      "step": 939
    },
    {
      "epoch": 0.4978813559322034,
      "grad_norm": 8.848748207092285,
      "learning_rate": 5.021186440677966e-06,
      "loss": 26.4786,
      "step": 940
    },
    {
      "epoch": 0.4984110169491525,
      "grad_norm": 9.179309844970703,
      "learning_rate": 5.015889830508475e-06,
      "loss": 25.5285,
      "step": 941
    },
    {
      "epoch": 0.4989406779661017,
      "grad_norm": 8.45383358001709,
      "learning_rate": 5.010593220338984e-06,
      "loss": 26.3795,
      "step": 942
    },
    {
      "epoch": 0.4994703389830508,
      "grad_norm": 10.911385536193848,
      "learning_rate": 5.0052966101694925e-06,
      "loss": 25.8601,
      "step": 943
    },
    {
      "epoch": 0.5,
      "grad_norm": 10.21102523803711,
      "learning_rate": 5e-06,
      "loss": 27.2374,
      "step": 944
    },
    {
      "epoch": 0.5005296610169492,
      "grad_norm": 10.15752124786377,
      "learning_rate": 4.994703389830508e-06,
      "loss": 27.3285,
      "step": 945
    },
    {
      "epoch": 0.5010593220338984,
      "grad_norm": 9.418421745300293,
      "learning_rate": 4.989406779661017e-06,
      "loss": 26.918,
      "step": 946
    },
    {
      "epoch": 0.5015889830508474,
      "grad_norm": 10.84083080291748,
      "learning_rate": 4.984110169491526e-06,
      "loss": 27.3157,
      "step": 947
    },
    {
      "epoch": 0.5021186440677966,
      "grad_norm": 9.384167671203613,
      "learning_rate": 4.9788135593220346e-06,
      "loss": 26.1744,
      "step": 948
    },
    {
      "epoch": 0.5026483050847458,
      "grad_norm": 9.914606094360352,
      "learning_rate": 4.9735169491525425e-06,
      "loss": 26.0356,
      "step": 949
    },
    {
      "epoch": 0.503177966101695,
      "grad_norm": 9.686749458312988,
      "learning_rate": 4.968220338983051e-06,
      "loss": 25.653,
      "step": 950
    },
    {
      "epoch": 0.503707627118644,
      "grad_norm": 9.758296012878418,
      "learning_rate": 4.96292372881356e-06,
      "loss": 27.3094,
      "step": 951
    },
    {
      "epoch": 0.5042372881355932,
      "grad_norm": 9.74652099609375,
      "learning_rate": 4.957627118644069e-06,
      "loss": 26.0174,
      "step": 952
    },
    {
      "epoch": 0.5047669491525424,
      "grad_norm": 10.371943473815918,
      "learning_rate": 4.952330508474577e-06,
      "loss": 27.3304,
      "step": 953
    },
    {
      "epoch": 0.5052966101694916,
      "grad_norm": 9.41633415222168,
      "learning_rate": 4.947033898305085e-06,
      "loss": 26.3103,
      "step": 954
    },
    {
      "epoch": 0.5058262711864406,
      "grad_norm": 10.769155502319336,
      "learning_rate": 4.941737288135593e-06,
      "loss": 26.5003,
      "step": 955
    },
    {
      "epoch": 0.5063559322033898,
      "grad_norm": 9.285853385925293,
      "learning_rate": 4.936440677966102e-06,
      "loss": 26.8562,
      "step": 956
    },
    {
      "epoch": 0.506885593220339,
      "grad_norm": 10.467658042907715,
      "learning_rate": 4.931144067796611e-06,
      "loss": 27.4352,
      "step": 957
    },
    {
      "epoch": 0.5074152542372882,
      "grad_norm": 10.584656715393066,
      "learning_rate": 4.9258474576271195e-06,
      "loss": 26.6998,
      "step": 958
    },
    {
      "epoch": 0.5079449152542372,
      "grad_norm": 9.976274490356445,
      "learning_rate": 4.920550847457627e-06,
      "loss": 27.4765,
      "step": 959
    },
    {
      "epoch": 0.5084745762711864,
      "grad_norm": 9.766717910766602,
      "learning_rate": 4.915254237288136e-06,
      "loss": 26.822,
      "step": 960
    },
    {
      "epoch": 0.5090042372881356,
      "grad_norm": 8.186551094055176,
      "learning_rate": 4.909957627118644e-06,
      "loss": 26.1164,
      "step": 961
    },
    {
      "epoch": 0.5095338983050848,
      "grad_norm": 10.006693840026855,
      "learning_rate": 4.904661016949153e-06,
      "loss": 26.548,
      "step": 962
    },
    {
      "epoch": 0.5100635593220338,
      "grad_norm": 11.86121654510498,
      "learning_rate": 4.8993644067796615e-06,
      "loss": 26.5733,
      "step": 963
    },
    {
      "epoch": 0.510593220338983,
      "grad_norm": 10.32254409790039,
      "learning_rate": 4.8940677966101694e-06,
      "loss": 25.8277,
      "step": 964
    },
    {
      "epoch": 0.5111228813559322,
      "grad_norm": 11.358637809753418,
      "learning_rate": 4.888771186440678e-06,
      "loss": 26.2323,
      "step": 965
    },
    {
      "epoch": 0.5116525423728814,
      "grad_norm": 9.916906356811523,
      "learning_rate": 4.883474576271187e-06,
      "loss": 26.4997,
      "step": 966
    },
    {
      "epoch": 0.5121822033898306,
      "grad_norm": 11.25666332244873,
      "learning_rate": 4.878177966101696e-06,
      "loss": 26.2283,
      "step": 967
    },
    {
      "epoch": 0.5127118644067796,
      "grad_norm": 13.018211364746094,
      "learning_rate": 4.872881355932204e-06,
      "loss": 25.895,
      "step": 968
    },
    {
      "epoch": 0.5132415254237288,
      "grad_norm": 11.021915435791016,
      "learning_rate": 4.867584745762712e-06,
      "loss": 25.7065,
      "step": 969
    },
    {
      "epoch": 0.513771186440678,
      "grad_norm": 14.619555473327637,
      "learning_rate": 4.86228813559322e-06,
      "loss": 25.667,
      "step": 970
    },
    {
      "epoch": 0.5143008474576272,
      "grad_norm": 31.95389747619629,
      "learning_rate": 4.856991525423729e-06,
      "loss": 25.9913,
      "step": 971
    },
    {
      "epoch": 0.5148305084745762,
      "grad_norm": 8.705288887023926,
      "learning_rate": 4.851694915254238e-06,
      "loss": 26.0566,
      "step": 972
    },
    {
      "epoch": 0.5153601694915254,
      "grad_norm": 8.660079956054688,
      "learning_rate": 4.8463983050847465e-06,
      "loss": 25.6728,
      "step": 973
    },
    {
      "epoch": 0.5158898305084746,
      "grad_norm": 8.919596672058105,
      "learning_rate": 4.841101694915254e-06,
      "loss": 25.2272,
      "step": 974
    },
    {
      "epoch": 0.5164194915254238,
      "grad_norm": 10.835521697998047,
      "learning_rate": 4.835805084745763e-06,
      "loss": 26.6054,
      "step": 975
    },
    {
      "epoch": 0.5169491525423728,
      "grad_norm": 9.042764663696289,
      "learning_rate": 4.830508474576272e-06,
      "loss": 25.892,
      "step": 976
    },
    {
      "epoch": 0.517478813559322,
      "grad_norm": 10.609512329101562,
      "learning_rate": 4.825211864406781e-06,
      "loss": 26.1059,
      "step": 977
    },
    {
      "epoch": 0.5180084745762712,
      "grad_norm": 13.105138778686523,
      "learning_rate": 4.8199152542372885e-06,
      "loss": 25.3095,
      "step": 978
    },
    {
      "epoch": 0.5185381355932204,
      "grad_norm": 9.014115333557129,
      "learning_rate": 4.814618644067797e-06,
      "loss": 24.9876,
      "step": 979
    },
    {
      "epoch": 0.5190677966101694,
      "grad_norm": 9.262035369873047,
      "learning_rate": 4.809322033898305e-06,
      "loss": 25.5824,
      "step": 980
    },
    {
      "epoch": 0.5195974576271186,
      "grad_norm": 9.245668411254883,
      "learning_rate": 4.804025423728814e-06,
      "loss": 26.9367,
      "step": 981
    },
    {
      "epoch": 0.5201271186440678,
      "grad_norm": 9.691598892211914,
      "learning_rate": 4.798728813559323e-06,
      "loss": 25.722,
      "step": 982
    },
    {
      "epoch": 0.520656779661017,
      "grad_norm": 9.726609230041504,
      "learning_rate": 4.793432203389831e-06,
      "loss": 25.9602,
      "step": 983
    },
    {
      "epoch": 0.5211864406779662,
      "grad_norm": 9.325425148010254,
      "learning_rate": 4.788135593220339e-06,
      "loss": 26.8276,
      "step": 984
    },
    {
      "epoch": 0.5217161016949152,
      "grad_norm": 11.061068534851074,
      "learning_rate": 4.782838983050847e-06,
      "loss": 26.4239,
      "step": 985
    },
    {
      "epoch": 0.5222457627118644,
      "grad_norm": 9.397566795349121,
      "learning_rate": 4.777542372881356e-06,
      "loss": 26.9595,
      "step": 986
    },
    {
      "epoch": 0.5227754237288136,
      "grad_norm": 9.93615436553955,
      "learning_rate": 4.772245762711865e-06,
      "loss": 26.5836,
      "step": 987
    },
    {
      "epoch": 0.5233050847457628,
      "grad_norm": 9.851117134094238,
      "learning_rate": 4.7669491525423735e-06,
      "loss": 26.0532,
      "step": 988
    },
    {
      "epoch": 0.5238347457627118,
      "grad_norm": 13.248659133911133,
      "learning_rate": 4.761652542372881e-06,
      "loss": 25.4536,
      "step": 989
    },
    {
      "epoch": 0.524364406779661,
      "grad_norm": 9.586834907531738,
      "learning_rate": 4.75635593220339e-06,
      "loss": 26.3538,
      "step": 990
    },
    {
      "epoch": 0.5248940677966102,
      "grad_norm": 9.492921829223633,
      "learning_rate": 4.751059322033899e-06,
      "loss": 26.0505,
      "step": 991
    },
    {
      "epoch": 0.5254237288135594,
      "grad_norm": 9.804511070251465,
      "learning_rate": 4.745762711864408e-06,
      "loss": 26.1806,
      "step": 992
    },
    {
      "epoch": 0.5259533898305084,
      "grad_norm": 10.911938667297363,
      "learning_rate": 4.7404661016949155e-06,
      "loss": 25.8571,
      "step": 993
    },
    {
      "epoch": 0.5264830508474576,
      "grad_norm": 12.634607315063477,
      "learning_rate": 4.735169491525424e-06,
      "loss": 24.433,
      "step": 994
    },
    {
      "epoch": 0.5270127118644068,
      "grad_norm": 9.107898712158203,
      "learning_rate": 4.729872881355932e-06,
      "loss": 25.3653,
      "step": 995
    },
    {
      "epoch": 0.527542372881356,
      "grad_norm": 10.165032386779785,
      "learning_rate": 4.724576271186441e-06,
      "loss": 25.6232,
      "step": 996
    },
    {
      "epoch": 0.528072033898305,
      "grad_norm": 10.2758207321167,
      "learning_rate": 4.71927966101695e-06,
      "loss": 24.7926,
      "step": 997
    },
    {
      "epoch": 0.5286016949152542,
      "grad_norm": 11.841474533081055,
      "learning_rate": 4.713983050847458e-06,
      "loss": 26.061,
      "step": 998
    },
    {
      "epoch": 0.5291313559322034,
      "grad_norm": 12.174890518188477,
      "learning_rate": 4.708686440677966e-06,
      "loss": 26.9424,
      "step": 999
    },
    {
      "epoch": 0.5296610169491526,
      "grad_norm": 9.526517868041992,
      "learning_rate": 4.703389830508475e-06,
      "loss": 26.247,
      "step": 1000
    },
    {
      "epoch": 0.5301906779661016,
      "grad_norm": 10.138155937194824,
      "learning_rate": 4.698093220338984e-06,
      "loss": 26.318,
      "step": 1001
    },
    {
      "epoch": 0.5307203389830508,
      "grad_norm": 10.551715850830078,
      "learning_rate": 4.6927966101694925e-06,
      "loss": 25.1869,
      "step": 1002
    },
    {
      "epoch": 0.53125,
      "grad_norm": 9.193196296691895,
      "learning_rate": 4.6875000000000004e-06,
      "loss": 25.4577,
      "step": 1003
    },
    {
      "epoch": 0.5317796610169492,
      "grad_norm": 9.851347923278809,
      "learning_rate": 4.682203389830508e-06,
      "loss": 26.7424,
      "step": 1004
    },
    {
      "epoch": 0.5323093220338984,
      "grad_norm": 8.593067169189453,
      "learning_rate": 4.676906779661017e-06,
      "loss": 25.0624,
      "step": 1005
    },
    {
      "epoch": 0.5328389830508474,
      "grad_norm": 10.621316909790039,
      "learning_rate": 4.671610169491526e-06,
      "loss": 25.1982,
      "step": 1006
    },
    {
      "epoch": 0.5333686440677966,
      "grad_norm": 12.315521240234375,
      "learning_rate": 4.666313559322035e-06,
      "loss": 25.883,
      "step": 1007
    },
    {
      "epoch": 0.5338983050847458,
      "grad_norm": 11.520374298095703,
      "learning_rate": 4.6610169491525425e-06,
      "loss": 26.1494,
      "step": 1008
    },
    {
      "epoch": 0.534427966101695,
      "grad_norm": 9.8556489944458,
      "learning_rate": 4.655720338983051e-06,
      "loss": 26.5786,
      "step": 1009
    },
    {
      "epoch": 0.534957627118644,
      "grad_norm": 81.7789535522461,
      "learning_rate": 4.650423728813559e-06,
      "loss": 25.8337,
      "step": 1010
    },
    {
      "epoch": 0.5354872881355932,
      "grad_norm": 9.514196395874023,
      "learning_rate": 4.645127118644068e-06,
      "loss": 26.6861,
      "step": 1011
    },
    {
      "epoch": 0.5360169491525424,
      "grad_norm": 14.676339149475098,
      "learning_rate": 4.639830508474577e-06,
      "loss": 25.5181,
      "step": 1012
    },
    {
      "epoch": 0.5365466101694916,
      "grad_norm": 9.648924827575684,
      "learning_rate": 4.634533898305085e-06,
      "loss": 26.316,
      "step": 1013
    },
    {
      "epoch": 0.5370762711864406,
      "grad_norm": 9.704923629760742,
      "learning_rate": 4.629237288135593e-06,
      "loss": 24.3346,
      "step": 1014
    },
    {
      "epoch": 0.5376059322033898,
      "grad_norm": 8.760072708129883,
      "learning_rate": 4.623940677966102e-06,
      "loss": 26.1328,
      "step": 1015
    },
    {
      "epoch": 0.538135593220339,
      "grad_norm": 9.207759857177734,
      "learning_rate": 4.618644067796611e-06,
      "loss": 25.1321,
      "step": 1016
    },
    {
      "epoch": 0.5386652542372882,
      "grad_norm": 8.749143600463867,
      "learning_rate": 4.6133474576271195e-06,
      "loss": 25.3408,
      "step": 1017
    },
    {
      "epoch": 0.5391949152542372,
      "grad_norm": 9.527929306030273,
      "learning_rate": 4.608050847457627e-06,
      "loss": 25.722,
      "step": 1018
    },
    {
      "epoch": 0.5397245762711864,
      "grad_norm": 11.034014701843262,
      "learning_rate": 4.602754237288136e-06,
      "loss": 26.8492,
      "step": 1019
    },
    {
      "epoch": 0.5402542372881356,
      "grad_norm": 12.11474323272705,
      "learning_rate": 4.597457627118644e-06,
      "loss": 26.742,
      "step": 1020
    },
    {
      "epoch": 0.5407838983050848,
      "grad_norm": 11.736794471740723,
      "learning_rate": 4.592161016949153e-06,
      "loss": 25.4393,
      "step": 1021
    },
    {
      "epoch": 0.5413135593220338,
      "grad_norm": 8.953309059143066,
      "learning_rate": 4.5868644067796616e-06,
      "loss": 25.7261,
      "step": 1022
    },
    {
      "epoch": 0.541843220338983,
      "grad_norm": 10.015495300292969,
      "learning_rate": 4.5815677966101695e-06,
      "loss": 24.9692,
      "step": 1023
    },
    {
      "epoch": 0.5423728813559322,
      "grad_norm": 9.860246658325195,
      "learning_rate": 4.576271186440678e-06,
      "loss": 25.3925,
      "step": 1024
    },
    {
      "epoch": 0.5429025423728814,
      "grad_norm": 9.31448745727539,
      "learning_rate": 4.570974576271187e-06,
      "loss": 25.6138,
      "step": 1025
    },
    {
      "epoch": 0.5434322033898306,
      "grad_norm": 8.452093124389648,
      "learning_rate": 4.565677966101696e-06,
      "loss": 25.3927,
      "step": 1026
    },
    {
      "epoch": 0.5439618644067796,
      "grad_norm": 8.123156547546387,
      "learning_rate": 4.560381355932204e-06,
      "loss": 25.3844,
      "step": 1027
    },
    {
      "epoch": 0.5444915254237288,
      "grad_norm": 9.782318115234375,
      "learning_rate": 4.555084745762712e-06,
      "loss": 26.3264,
      "step": 1028
    },
    {
      "epoch": 0.545021186440678,
      "grad_norm": 10.198731422424316,
      "learning_rate": 4.54978813559322e-06,
      "loss": 25.7555,
      "step": 1029
    },
    {
      "epoch": 0.5455508474576272,
      "grad_norm": 9.589054107666016,
      "learning_rate": 4.544491525423729e-06,
      "loss": 25.2626,
      "step": 1030
    },
    {
      "epoch": 0.5460805084745762,
      "grad_norm": 10.080522537231445,
      "learning_rate": 4.539194915254238e-06,
      "loss": 24.8503,
      "step": 1031
    },
    {
      "epoch": 0.5466101694915254,
      "grad_norm": 10.847187042236328,
      "learning_rate": 4.5338983050847465e-06,
      "loss": 26.4629,
      "step": 1032
    },
    {
      "epoch": 0.5471398305084746,
      "grad_norm": 9.907984733581543,
      "learning_rate": 4.528601694915254e-06,
      "loss": 25.175,
      "step": 1033
    },
    {
      "epoch": 0.5476694915254238,
      "grad_norm": 89.31526184082031,
      "learning_rate": 4.523305084745763e-06,
      "loss": 24.6139,
      "step": 1034
    },
    {
      "epoch": 0.5481991525423728,
      "grad_norm": 12.324588775634766,
      "learning_rate": 4.518008474576271e-06,
      "loss": 25.3477,
      "step": 1035
    },
    {
      "epoch": 0.548728813559322,
      "grad_norm": 13.147720336914062,
      "learning_rate": 4.51271186440678e-06,
      "loss": 24.8621,
      "step": 1036
    },
    {
      "epoch": 0.5492584745762712,
      "grad_norm": 10.621199607849121,
      "learning_rate": 4.5074152542372885e-06,
      "loss": 25.3335,
      "step": 1037
    },
    {
      "epoch": 0.5497881355932204,
      "grad_norm": 10.063727378845215,
      "learning_rate": 4.502118644067797e-06,
      "loss": 26.0696,
      "step": 1038
    },
    {
      "epoch": 0.5503177966101694,
      "grad_norm": 9.560555458068848,
      "learning_rate": 4.496822033898305e-06,
      "loss": 25.5308,
      "step": 1039
    },
    {
      "epoch": 0.5508474576271186,
      "grad_norm": 9.594175338745117,
      "learning_rate": 4.491525423728814e-06,
      "loss": 25.0855,
      "step": 1040
    },
    {
      "epoch": 0.5513771186440678,
      "grad_norm": 9.177650451660156,
      "learning_rate": 4.486228813559323e-06,
      "loss": 25.2281,
      "step": 1041
    },
    {
      "epoch": 0.551906779661017,
      "grad_norm": 41.663719177246094,
      "learning_rate": 4.4809322033898314e-06,
      "loss": 25.0038,
      "step": 1042
    },
    {
      "epoch": 0.5524364406779662,
      "grad_norm": 9.742581367492676,
      "learning_rate": 4.475635593220339e-06,
      "loss": 26.0552,
      "step": 1043
    },
    {
      "epoch": 0.5529661016949152,
      "grad_norm": 9.67617130279541,
      "learning_rate": 4.470338983050847e-06,
      "loss": 25.2965,
      "step": 1044
    },
    {
      "epoch": 0.5534957627118644,
      "grad_norm": 11.60424518585205,
      "learning_rate": 4.465042372881356e-06,
      "loss": 25.9892,
      "step": 1045
    },
    {
      "epoch": 0.5540254237288136,
      "grad_norm": 9.907442092895508,
      "learning_rate": 4.459745762711865e-06,
      "loss": 25.0787,
      "step": 1046
    },
    {
      "epoch": 0.5545550847457628,
      "grad_norm": 9.520273208618164,
      "learning_rate": 4.4544491525423735e-06,
      "loss": 24.6096,
      "step": 1047
    },
    {
      "epoch": 0.5550847457627118,
      "grad_norm": 10.498645782470703,
      "learning_rate": 4.449152542372881e-06,
      "loss": 25.8895,
      "step": 1048
    },
    {
      "epoch": 0.555614406779661,
      "grad_norm": 8.840948104858398,
      "learning_rate": 4.44385593220339e-06,
      "loss": 25.6483,
      "step": 1049
    },
    {
      "epoch": 0.5561440677966102,
      "grad_norm": 10.253207206726074,
      "learning_rate": 4.438559322033899e-06,
      "loss": 26.0774,
      "step": 1050
    },
    {
      "epoch": 0.5566737288135594,
      "grad_norm": 44.44292449951172,
      "learning_rate": 4.433262711864408e-06,
      "loss": 25.532,
      "step": 1051
    },
    {
      "epoch": 0.5572033898305084,
      "grad_norm": 9.797219276428223,
      "learning_rate": 4.4279661016949155e-06,
      "loss": 25.3606,
      "step": 1052
    },
    {
      "epoch": 0.5577330508474576,
      "grad_norm": 9.763129234313965,
      "learning_rate": 4.422669491525424e-06,
      "loss": 25.5261,
      "step": 1053
    },
    {
      "epoch": 0.5582627118644068,
      "grad_norm": 10.057369232177734,
      "learning_rate": 4.417372881355932e-06,
      "loss": 26.165,
      "step": 1054
    },
    {
      "epoch": 0.558792372881356,
      "grad_norm": 11.64618968963623,
      "learning_rate": 4.412076271186441e-06,
      "loss": 26.0044,
      "step": 1055
    },
    {
      "epoch": 0.559322033898305,
      "grad_norm": 21.96701431274414,
      "learning_rate": 4.40677966101695e-06,
      "loss": 24.6859,
      "step": 1056
    },
    {
      "epoch": 0.5598516949152542,
      "grad_norm": 9.264628410339355,
      "learning_rate": 4.401483050847458e-06,
      "loss": 25.1008,
      "step": 1057
    },
    {
      "epoch": 0.5603813559322034,
      "grad_norm": 9.211146354675293,
      "learning_rate": 4.396186440677966e-06,
      "loss": 25.9011,
      "step": 1058
    },
    {
      "epoch": 0.5609110169491526,
      "grad_norm": 10.748031616210938,
      "learning_rate": 4.390889830508475e-06,
      "loss": 25.1799,
      "step": 1059
    },
    {
      "epoch": 0.5614406779661016,
      "grad_norm": 9.863192558288574,
      "learning_rate": 4.385593220338983e-06,
      "loss": 24.7491,
      "step": 1060
    },
    {
      "epoch": 0.5619703389830508,
      "grad_norm": 11.428956985473633,
      "learning_rate": 4.3802966101694926e-06,
      "loss": 25.8105,
      "step": 1061
    },
    {
      "epoch": 0.5625,
      "grad_norm": 23.029977798461914,
      "learning_rate": 4.3750000000000005e-06,
      "loss": 24.2728,
      "step": 1062
    },
    {
      "epoch": 0.5630296610169492,
      "grad_norm": 11.134401321411133,
      "learning_rate": 4.369703389830508e-06,
      "loss": 24.9478,
      "step": 1063
    },
    {
      "epoch": 0.5635593220338984,
      "grad_norm": 116.87535858154297,
      "learning_rate": 4.364406779661017e-06,
      "loss": 26.1425,
      "step": 1064
    },
    {
      "epoch": 0.5640889830508474,
      "grad_norm": 10.14490032196045,
      "learning_rate": 4.359110169491526e-06,
      "loss": 24.5372,
      "step": 1065
    },
    {
      "epoch": 0.5646186440677966,
      "grad_norm": 9.662193298339844,
      "learning_rate": 4.353813559322035e-06,
      "loss": 25.6732,
      "step": 1066
    },
    {
      "epoch": 0.5651483050847458,
      "grad_norm": 10.693428993225098,
      "learning_rate": 4.3485169491525425e-06,
      "loss": 24.4687,
      "step": 1067
    },
    {
      "epoch": 0.565677966101695,
      "grad_norm": 13.66655445098877,
      "learning_rate": 4.343220338983051e-06,
      "loss": 25.4921,
      "step": 1068
    },
    {
      "epoch": 0.566207627118644,
      "grad_norm": 10.88420581817627,
      "learning_rate": 4.337923728813559e-06,
      "loss": 25.1709,
      "step": 1069
    },
    {
      "epoch": 0.5667372881355932,
      "grad_norm": 12.542353630065918,
      "learning_rate": 4.332627118644068e-06,
      "loss": 25.6086,
      "step": 1070
    },
    {
      "epoch": 0.5672669491525424,
      "grad_norm": 9.886370658874512,
      "learning_rate": 4.327330508474577e-06,
      "loss": 25.1337,
      "step": 1071
    },
    {
      "epoch": 0.5677966101694916,
      "grad_norm": 11.280404090881348,
      "learning_rate": 4.322033898305085e-06,
      "loss": 24.7886,
      "step": 1072
    },
    {
      "epoch": 0.5683262711864406,
      "grad_norm": 9.399951934814453,
      "learning_rate": 4.316737288135593e-06,
      "loss": 25.1874,
      "step": 1073
    },
    {
      "epoch": 0.5688559322033898,
      "grad_norm": 10.659802436828613,
      "learning_rate": 4.311440677966102e-06,
      "loss": 24.5467,
      "step": 1074
    },
    {
      "epoch": 0.569385593220339,
      "grad_norm": 10.163819313049316,
      "learning_rate": 4.306144067796611e-06,
      "loss": 26.2075,
      "step": 1075
    },
    {
      "epoch": 0.5699152542372882,
      "grad_norm": 9.13677978515625,
      "learning_rate": 4.3008474576271195e-06,
      "loss": 25.286,
      "step": 1076
    },
    {
      "epoch": 0.5704449152542372,
      "grad_norm": 10.111644744873047,
      "learning_rate": 4.2955508474576274e-06,
      "loss": 24.9737,
      "step": 1077
    },
    {
      "epoch": 0.5709745762711864,
      "grad_norm": 9.784684181213379,
      "learning_rate": 4.290254237288136e-06,
      "loss": 23.8056,
      "step": 1078
    },
    {
      "epoch": 0.5715042372881356,
      "grad_norm": 9.067261695861816,
      "learning_rate": 4.284957627118644e-06,
      "loss": 24.6664,
      "step": 1079
    },
    {
      "epoch": 0.5720338983050848,
      "grad_norm": 10.386323928833008,
      "learning_rate": 4.279661016949153e-06,
      "loss": 24.8589,
      "step": 1080
    },
    {
      "epoch": 0.5725635593220338,
      "grad_norm": 9.600018501281738,
      "learning_rate": 4.274364406779662e-06,
      "loss": 25.1437,
      "step": 1081
    },
    {
      "epoch": 0.573093220338983,
      "grad_norm": 15.158066749572754,
      "learning_rate": 4.2690677966101695e-06,
      "loss": 25.0078,
      "step": 1082
    },
    {
      "epoch": 0.5736228813559322,
      "grad_norm": 8.936013221740723,
      "learning_rate": 4.263771186440678e-06,
      "loss": 25.3074,
      "step": 1083
    },
    {
      "epoch": 0.5741525423728814,
      "grad_norm": 10.129244804382324,
      "learning_rate": 4.258474576271186e-06,
      "loss": 25.5844,
      "step": 1084
    },
    {
      "epoch": 0.5746822033898306,
      "grad_norm": 11.096687316894531,
      "learning_rate": 4.253177966101695e-06,
      "loss": 24.4799,
      "step": 1085
    },
    {
      "epoch": 0.5752118644067796,
      "grad_norm": 9.874051094055176,
      "learning_rate": 4.247881355932204e-06,
      "loss": 24.7794,
      "step": 1086
    },
    {
      "epoch": 0.5757415254237288,
      "grad_norm": 9.789563179016113,
      "learning_rate": 4.242584745762712e-06,
      "loss": 25.48,
      "step": 1087
    },
    {
      "epoch": 0.576271186440678,
      "grad_norm": 11.416036605834961,
      "learning_rate": 4.23728813559322e-06,
      "loss": 24.913,
      "step": 1088
    },
    {
      "epoch": 0.5768008474576272,
      "grad_norm": 10.329532623291016,
      "learning_rate": 4.231991525423729e-06,
      "loss": 24.6915,
      "step": 1089
    },
    {
      "epoch": 0.5773305084745762,
      "grad_norm": 10.427496910095215,
      "learning_rate": 4.226694915254238e-06,
      "loss": 25.4072,
      "step": 1090
    },
    {
      "epoch": 0.5778601694915254,
      "grad_norm": 12.448758125305176,
      "learning_rate": 4.2213983050847465e-06,
      "loss": 25.8818,
      "step": 1091
    },
    {
      "epoch": 0.5783898305084746,
      "grad_norm": 12.400094032287598,
      "learning_rate": 4.2161016949152544e-06,
      "loss": 24.7554,
      "step": 1092
    },
    {
      "epoch": 0.5789194915254238,
      "grad_norm": 10.209159851074219,
      "learning_rate": 4.210805084745763e-06,
      "loss": 25.6169,
      "step": 1093
    },
    {
      "epoch": 0.5794491525423728,
      "grad_norm": 9.530536651611328,
      "learning_rate": 4.205508474576271e-06,
      "loss": 25.1893,
      "step": 1094
    },
    {
      "epoch": 0.579978813559322,
      "grad_norm": 11.021002769470215,
      "learning_rate": 4.20021186440678e-06,
      "loss": 24.9432,
      "step": 1095
    },
    {
      "epoch": 0.5805084745762712,
      "grad_norm": 9.532458305358887,
      "learning_rate": 4.1949152542372886e-06,
      "loss": 25.114,
      "step": 1096
    },
    {
      "epoch": 0.5810381355932204,
      "grad_norm": 10.010499000549316,
      "learning_rate": 4.189618644067797e-06,
      "loss": 24.5499,
      "step": 1097
    },
    {
      "epoch": 0.5815677966101694,
      "grad_norm": 78.7927017211914,
      "learning_rate": 4.184322033898305e-06,
      "loss": 24.3468,
      "step": 1098
    },
    {
      "epoch": 0.5820974576271186,
      "grad_norm": 10.345650672912598,
      "learning_rate": 4.179025423728814e-06,
      "loss": 24.9135,
      "step": 1099
    },
    {
      "epoch": 0.5826271186440678,
      "grad_norm": 9.410160064697266,
      "learning_rate": 4.173728813559323e-06,
      "loss": 25.1512,
      "step": 1100
    },
    {
      "epoch": 0.583156779661017,
      "grad_norm": 10.335663795471191,
      "learning_rate": 4.1684322033898315e-06,
      "loss": 24.9391,
      "step": 1101
    },
    {
      "epoch": 0.5836864406779662,
      "grad_norm": 10.428092956542969,
      "learning_rate": 4.163135593220339e-06,
      "loss": 24.564,
      "step": 1102
    },
    {
      "epoch": 0.5842161016949152,
      "grad_norm": 9.35929012298584,
      "learning_rate": 4.157838983050847e-06,
      "loss": 24.5142,
      "step": 1103
    },
    {
      "epoch": 0.5847457627118644,
      "grad_norm": 9.470669746398926,
      "learning_rate": 4.152542372881356e-06,
      "loss": 24.9154,
      "step": 1104
    },
    {
      "epoch": 0.5852754237288136,
      "grad_norm": 9.68586540222168,
      "learning_rate": 4.147245762711865e-06,
      "loss": 24.0942,
      "step": 1105
    },
    {
      "epoch": 0.5858050847457628,
      "grad_norm": 10.685935974121094,
      "learning_rate": 4.1419491525423735e-06,
      "loss": 25.2066,
      "step": 1106
    },
    {
      "epoch": 0.5863347457627118,
      "grad_norm": 248.66311645507812,
      "learning_rate": 4.136652542372881e-06,
      "loss": 25.1247,
      "step": 1107
    },
    {
      "epoch": 0.586864406779661,
      "grad_norm": 10.441880226135254,
      "learning_rate": 4.13135593220339e-06,
      "loss": 23.7791,
      "step": 1108
    },
    {
      "epoch": 0.5873940677966102,
      "grad_norm": 9.964195251464844,
      "learning_rate": 4.126059322033898e-06,
      "loss": 24.7134,
      "step": 1109
    },
    {
      "epoch": 0.5879237288135594,
      "grad_norm": 9.896218299865723,
      "learning_rate": 4.120762711864408e-06,
      "loss": 23.9944,
      "step": 1110
    },
    {
      "epoch": 0.5884533898305084,
      "grad_norm": 10.720209121704102,
      "learning_rate": 4.1154661016949156e-06,
      "loss": 24.376,
      "step": 1111
    },
    {
      "epoch": 0.5889830508474576,
      "grad_norm": 12.50767993927002,
      "learning_rate": 4.110169491525424e-06,
      "loss": 23.7222,
      "step": 1112
    },
    {
      "epoch": 0.5895127118644068,
      "grad_norm": 12.894476890563965,
      "learning_rate": 4.104872881355932e-06,
      "loss": 24.2838,
      "step": 1113
    },
    {
      "epoch": 0.590042372881356,
      "grad_norm": 10.486104965209961,
      "learning_rate": 4.099576271186441e-06,
      "loss": 24.3518,
      "step": 1114
    },
    {
      "epoch": 0.590572033898305,
      "grad_norm": 10.154159545898438,
      "learning_rate": 4.09427966101695e-06,
      "loss": 25.0354,
      "step": 1115
    },
    {
      "epoch": 0.5911016949152542,
      "grad_norm": 15.89498519897461,
      "learning_rate": 4.0889830508474584e-06,
      "loss": 24.6458,
      "step": 1116
    },
    {
      "epoch": 0.5916313559322034,
      "grad_norm": 9.557136535644531,
      "learning_rate": 4.083686440677966e-06,
      "loss": 24.5491,
      "step": 1117
    },
    {
      "epoch": 0.5921610169491526,
      "grad_norm": 9.147638320922852,
      "learning_rate": 4.078389830508475e-06,
      "loss": 24.9785,
      "step": 1118
    },
    {
      "epoch": 0.5926906779661016,
      "grad_norm": 9.788069725036621,
      "learning_rate": 4.073093220338983e-06,
      "loss": 24.4185,
      "step": 1119
    },
    {
      "epoch": 0.5932203389830508,
      "grad_norm": 9.853887557983398,
      "learning_rate": 4.067796610169492e-06,
      "loss": 24.2586,
      "step": 1120
    },
    {
      "epoch": 0.59375,
      "grad_norm": 10.976383209228516,
      "learning_rate": 4.0625000000000005e-06,
      "loss": 24.35,
      "step": 1121
    },
    {
      "epoch": 0.5942796610169492,
      "grad_norm": 10.286565780639648,
      "learning_rate": 4.057203389830508e-06,
      "loss": 24.5146,
      "step": 1122
    },
    {
      "epoch": 0.5948093220338984,
      "grad_norm": 9.093990325927734,
      "learning_rate": 4.051906779661017e-06,
      "loss": 22.8391,
      "step": 1123
    },
    {
      "epoch": 0.5953389830508474,
      "grad_norm": 10.486462593078613,
      "learning_rate": 4.046610169491526e-06,
      "loss": 25.3164,
      "step": 1124
    },
    {
      "epoch": 0.5958686440677966,
      "grad_norm": 10.396784782409668,
      "learning_rate": 4.041313559322035e-06,
      "loss": 24.3876,
      "step": 1125
    },
    {
      "epoch": 0.5963983050847458,
      "grad_norm": 11.81186294555664,
      "learning_rate": 4.0360169491525425e-06,
      "loss": 24.1885,
      "step": 1126
    },
    {
      "epoch": 0.596927966101695,
      "grad_norm": 10.325532913208008,
      "learning_rate": 4.030720338983051e-06,
      "loss": 24.7281,
      "step": 1127
    },
    {
      "epoch": 0.597457627118644,
      "grad_norm": 44.13285446166992,
      "learning_rate": 4.025423728813559e-06,
      "loss": 25.033,
      "step": 1128
    },
    {
      "epoch": 0.5979872881355932,
      "grad_norm": 11.043035507202148,
      "learning_rate": 4.020127118644068e-06,
      "loss": 25.5359,
      "step": 1129
    },
    {
      "epoch": 0.5985169491525424,
      "grad_norm": 9.762866020202637,
      "learning_rate": 4.014830508474577e-06,
      "loss": 24.9142,
      "step": 1130
    },
    {
      "epoch": 0.5990466101694916,
      "grad_norm": 9.503398895263672,
      "learning_rate": 4.0095338983050854e-06,
      "loss": 23.6382,
      "step": 1131
    },
    {
      "epoch": 0.5995762711864406,
      "grad_norm": 9.627556800842285,
      "learning_rate": 4.004237288135593e-06,
      "loss": 24.5748,
      "step": 1132
    },
    {
      "epoch": 0.6001059322033898,
      "grad_norm": 9.691171646118164,
      "learning_rate": 3.998940677966102e-06,
      "loss": 24.5292,
      "step": 1133
    },
    {
      "epoch": 0.600635593220339,
      "grad_norm": 9.5093412399292,
      "learning_rate": 3.993644067796611e-06,
      "loss": 24.7238,
      "step": 1134
    },
    {
      "epoch": 0.6011652542372882,
      "grad_norm": 19.99365997314453,
      "learning_rate": 3.9883474576271196e-06,
      "loss": 24.7079,
      "step": 1135
    },
    {
      "epoch": 0.6016949152542372,
      "grad_norm": 10.088316917419434,
      "learning_rate": 3.9830508474576275e-06,
      "loss": 23.9712,
      "step": 1136
    },
    {
      "epoch": 0.6022245762711864,
      "grad_norm": 25.218387603759766,
      "learning_rate": 3.977754237288136e-06,
      "loss": 24.3338,
      "step": 1137
    },
    {
      "epoch": 0.6027542372881356,
      "grad_norm": 10.313485145568848,
      "learning_rate": 3.972457627118644e-06,
      "loss": 24.76,
      "step": 1138
    },
    {
      "epoch": 0.6032838983050848,
      "grad_norm": 10.592580795288086,
      "learning_rate": 3.967161016949153e-06,
      "loss": 24.1372,
      "step": 1139
    },
    {
      "epoch": 0.6038135593220338,
      "grad_norm": 11.061704635620117,
      "learning_rate": 3.961864406779662e-06,
      "loss": 26.2774,
      "step": 1140
    },
    {
      "epoch": 0.604343220338983,
      "grad_norm": 9.601824760437012,
      "learning_rate": 3.9565677966101695e-06,
      "loss": 24.047,
      "step": 1141
    },
    {
      "epoch": 0.6048728813559322,
      "grad_norm": 9.56760025024414,
      "learning_rate": 3.951271186440678e-06,
      "loss": 24.0163,
      "step": 1142
    },
    {
      "epoch": 0.6054025423728814,
      "grad_norm": 9.45482349395752,
      "learning_rate": 3.945974576271186e-06,
      "loss": 24.1757,
      "step": 1143
    },
    {
      "epoch": 0.6059322033898306,
      "grad_norm": 10.0830078125,
      "learning_rate": 3.940677966101695e-06,
      "loss": 24.0635,
      "step": 1144
    },
    {
      "epoch": 0.6064618644067796,
      "grad_norm": 31.908533096313477,
      "learning_rate": 3.935381355932204e-06,
      "loss": 24.3986,
      "step": 1145
    },
    {
      "epoch": 0.6069915254237288,
      "grad_norm": 11.629376411437988,
      "learning_rate": 3.930084745762712e-06,
      "loss": 24.7006,
      "step": 1146
    },
    {
      "epoch": 0.607521186440678,
      "grad_norm": 10.284124374389648,
      "learning_rate": 3.92478813559322e-06,
      "loss": 24.0976,
      "step": 1147
    },
    {
      "epoch": 0.6080508474576272,
      "grad_norm": 9.982812881469727,
      "learning_rate": 3.919491525423729e-06,
      "loss": 24.2667,
      "step": 1148
    },
    {
      "epoch": 0.6085805084745762,
      "grad_norm": 9.964632034301758,
      "learning_rate": 3.914194915254238e-06,
      "loss": 24.4563,
      "step": 1149
    },
    {
      "epoch": 0.6091101694915254,
      "grad_norm": 11.392264366149902,
      "learning_rate": 3.9088983050847466e-06,
      "loss": 24.252,
      "step": 1150
    },
    {
      "epoch": 0.6096398305084746,
      "grad_norm": 10.743024826049805,
      "learning_rate": 3.9036016949152545e-06,
      "loss": 24.292,
      "step": 1151
    },
    {
      "epoch": 0.6101694915254238,
      "grad_norm": 9.783754348754883,
      "learning_rate": 3.898305084745763e-06,
      "loss": 23.8955,
      "step": 1152
    },
    {
      "epoch": 0.6106991525423728,
      "grad_norm": 10.276219367980957,
      "learning_rate": 3.893008474576271e-06,
      "loss": 25.0448,
      "step": 1153
    },
    {
      "epoch": 0.611228813559322,
      "grad_norm": 16.35580062866211,
      "learning_rate": 3.88771186440678e-06,
      "loss": 23.601,
      "step": 1154
    },
    {
      "epoch": 0.6117584745762712,
      "grad_norm": 9.768251419067383,
      "learning_rate": 3.882415254237289e-06,
      "loss": 24.1875,
      "step": 1155
    },
    {
      "epoch": 0.6122881355932204,
      "grad_norm": 10.312755584716797,
      "learning_rate": 3.877118644067797e-06,
      "loss": 24.5904,
      "step": 1156
    },
    {
      "epoch": 0.6128177966101694,
      "grad_norm": 9.846543312072754,
      "learning_rate": 3.871822033898305e-06,
      "loss": 23.4997,
      "step": 1157
    },
    {
      "epoch": 0.6133474576271186,
      "grad_norm": 11.763992309570312,
      "learning_rate": 3.866525423728814e-06,
      "loss": 23.7985,
      "step": 1158
    },
    {
      "epoch": 0.6138771186440678,
      "grad_norm": 11.233379364013672,
      "learning_rate": 3.861228813559323e-06,
      "loss": 24.3053,
      "step": 1159
    },
    {
      "epoch": 0.614406779661017,
      "grad_norm": 10.958587646484375,
      "learning_rate": 3.8559322033898315e-06,
      "loss": 24.3675,
      "step": 1160
    },
    {
      "epoch": 0.6149364406779662,
      "grad_norm": 9.699164390563965,
      "learning_rate": 3.850635593220339e-06,
      "loss": 22.7447,
      "step": 1161
    },
    {
      "epoch": 0.6154661016949152,
      "grad_norm": 22.104455947875977,
      "learning_rate": 3.845338983050847e-06,
      "loss": 23.9129,
      "step": 1162
    },
    {
      "epoch": 0.6159957627118644,
      "grad_norm": 10.090754508972168,
      "learning_rate": 3.840042372881356e-06,
      "loss": 24.465,
      "step": 1163
    },
    {
      "epoch": 0.6165254237288136,
      "grad_norm": 10.00118637084961,
      "learning_rate": 3.834745762711865e-06,
      "loss": 23.5197,
      "step": 1164
    },
    {
      "epoch": 0.6170550847457628,
      "grad_norm": 11.146249771118164,
      "learning_rate": 3.8294491525423735e-06,
      "loss": 23.3696,
      "step": 1165
    },
    {
      "epoch": 0.6175847457627118,
      "grad_norm": 13.2398681640625,
      "learning_rate": 3.8241525423728814e-06,
      "loss": 24.339,
      "step": 1166
    },
    {
      "epoch": 0.618114406779661,
      "grad_norm": 10.26270580291748,
      "learning_rate": 3.81885593220339e-06,
      "loss": 24.0497,
      "step": 1167
    },
    {
      "epoch": 0.6186440677966102,
      "grad_norm": 13.758082389831543,
      "learning_rate": 3.8135593220338985e-06,
      "loss": 24.9879,
      "step": 1168
    },
    {
      "epoch": 0.6191737288135594,
      "grad_norm": 9.839300155639648,
      "learning_rate": 3.8082627118644073e-06,
      "loss": 24.6305,
      "step": 1169
    },
    {
      "epoch": 0.6197033898305084,
      "grad_norm": 9.57972240447998,
      "learning_rate": 3.8029661016949156e-06,
      "loss": 24.1575,
      "step": 1170
    },
    {
      "epoch": 0.6202330508474576,
      "grad_norm": 11.481595039367676,
      "learning_rate": 3.7976694915254243e-06,
      "loss": 23.6731,
      "step": 1171
    },
    {
      "epoch": 0.6207627118644068,
      "grad_norm": 8.684883117675781,
      "learning_rate": 3.7923728813559322e-06,
      "loss": 23.7122,
      "step": 1172
    },
    {
      "epoch": 0.621292372881356,
      "grad_norm": 10.362202644348145,
      "learning_rate": 3.787076271186441e-06,
      "loss": 25.0746,
      "step": 1173
    },
    {
      "epoch": 0.621822033898305,
      "grad_norm": 10.028717994689941,
      "learning_rate": 3.7817796610169493e-06,
      "loss": 24.0309,
      "step": 1174
    },
    {
      "epoch": 0.6223516949152542,
      "grad_norm": 10.70693302154541,
      "learning_rate": 3.776483050847458e-06,
      "loss": 23.4595,
      "step": 1175
    },
    {
      "epoch": 0.6228813559322034,
      "grad_norm": 9.19271469116211,
      "learning_rate": 3.7711864406779664e-06,
      "loss": 24.0258,
      "step": 1176
    },
    {
      "epoch": 0.6234110169491526,
      "grad_norm": 16.79730987548828,
      "learning_rate": 3.765889830508475e-06,
      "loss": 24.5426,
      "step": 1177
    },
    {
      "epoch": 0.6239406779661016,
      "grad_norm": 10.011305809020996,
      "learning_rate": 3.7605932203389834e-06,
      "loss": 24.1457,
      "step": 1178
    },
    {
      "epoch": 0.6244703389830508,
      "grad_norm": 15.914223670959473,
      "learning_rate": 3.755296610169492e-06,
      "loss": 22.7426,
      "step": 1179
    },
    {
      "epoch": 0.625,
      "grad_norm": 10.091124534606934,
      "learning_rate": 3.7500000000000005e-06,
      "loss": 24.79,
      "step": 1180
    },
    {
      "epoch": 0.6255296610169492,
      "grad_norm": 12.3905611038208,
      "learning_rate": 3.7447033898305084e-06,
      "loss": 23.5796,
      "step": 1181
    },
    {
      "epoch": 0.6260593220338984,
      "grad_norm": 97.86878204345703,
      "learning_rate": 3.739406779661017e-06,
      "loss": 25.5959,
      "step": 1182
    },
    {
      "epoch": 0.6265889830508474,
      "grad_norm": 9.366894721984863,
      "learning_rate": 3.7341101694915255e-06,
      "loss": 24.1625,
      "step": 1183
    },
    {
      "epoch": 0.6271186440677966,
      "grad_norm": 10.245584487915039,
      "learning_rate": 3.7288135593220342e-06,
      "loss": 24.3822,
      "step": 1184
    },
    {
      "epoch": 0.6276483050847458,
      "grad_norm": 49.79570007324219,
      "learning_rate": 3.7235169491525426e-06,
      "loss": 24.5603,
      "step": 1185
    },
    {
      "epoch": 0.628177966101695,
      "grad_norm": 21.658100128173828,
      "learning_rate": 3.7182203389830513e-06,
      "loss": 24.5646,
      "step": 1186
    },
    {
      "epoch": 0.628707627118644,
      "grad_norm": 8.930754661560059,
      "learning_rate": 3.7129237288135596e-06,
      "loss": 24.1652,
      "step": 1187
    },
    {
      "epoch": 0.6292372881355932,
      "grad_norm": 75.37370300292969,
      "learning_rate": 3.7076271186440684e-06,
      "loss": 24.7222,
      "step": 1188
    },
    {
      "epoch": 0.6297669491525424,
      "grad_norm": 11.006770133972168,
      "learning_rate": 3.7023305084745763e-06,
      "loss": 24.471,
      "step": 1189
    },
    {
      "epoch": 0.6302966101694916,
      "grad_norm": 10.660253524780273,
      "learning_rate": 3.697033898305085e-06,
      "loss": 23.6851,
      "step": 1190
    },
    {
      "epoch": 0.6308262711864406,
      "grad_norm": 12.4434814453125,
      "learning_rate": 3.6917372881355934e-06,
      "loss": 24.4191,
      "step": 1191
    },
    {
      "epoch": 0.6313559322033898,
      "grad_norm": 12.021989822387695,
      "learning_rate": 3.686440677966102e-06,
      "loss": 23.4108,
      "step": 1192
    },
    {
      "epoch": 0.631885593220339,
      "grad_norm": 10.56857681274414,
      "learning_rate": 3.6811440677966104e-06,
      "loss": 24.4474,
      "step": 1193
    },
    {
      "epoch": 0.6324152542372882,
      "grad_norm": 368.1564025878906,
      "learning_rate": 3.675847457627119e-06,
      "loss": 27.3552,
      "step": 1194
    },
    {
      "epoch": 0.6329449152542372,
      "grad_norm": 10.762954711914062,
      "learning_rate": 3.6705508474576275e-06,
      "loss": 24.1341,
      "step": 1195
    },
    {
      "epoch": 0.6334745762711864,
      "grad_norm": 435.47576904296875,
      "learning_rate": 3.6652542372881362e-06,
      "loss": 23.8605,
      "step": 1196
    },
    {
      "epoch": 0.6340042372881356,
      "grad_norm": 10.262953758239746,
      "learning_rate": 3.659957627118644e-06,
      "loss": 23.8075,
      "step": 1197
    },
    {
      "epoch": 0.6345338983050848,
      "grad_norm": 12.280848503112793,
      "learning_rate": 3.654661016949153e-06,
      "loss": 24.7596,
      "step": 1198
    },
    {
      "epoch": 0.6350635593220338,
      "grad_norm": 10.049210548400879,
      "learning_rate": 3.6493644067796612e-06,
      "loss": 24.647,
      "step": 1199
    },
    {
      "epoch": 0.635593220338983,
      "grad_norm": 10.073694229125977,
      "learning_rate": 3.6440677966101695e-06,
      "loss": 23.3205,
      "step": 1200
    },
    {
      "epoch": 0.6361228813559322,
      "grad_norm": 11.216842651367188,
      "learning_rate": 3.6387711864406783e-06,
      "loss": 23.701,
      "step": 1201
    },
    {
      "epoch": 0.6366525423728814,
      "grad_norm": 10.639878273010254,
      "learning_rate": 3.6334745762711866e-06,
      "loss": 24.1818,
      "step": 1202
    },
    {
      "epoch": 0.6371822033898306,
      "grad_norm": 10.993823051452637,
      "learning_rate": 3.6281779661016954e-06,
      "loss": 23.586,
      "step": 1203
    },
    {
      "epoch": 0.6377118644067796,
      "grad_norm": 58.3480110168457,
      "learning_rate": 3.6228813559322033e-06,
      "loss": 24.0709,
      "step": 1204
    },
    {
      "epoch": 0.6382415254237288,
      "grad_norm": 13.779478073120117,
      "learning_rate": 3.6175847457627124e-06,
      "loss": 24.2659,
      "step": 1205
    },
    {
      "epoch": 0.638771186440678,
      "grad_norm": 11.231605529785156,
      "learning_rate": 3.6122881355932203e-06,
      "loss": 24.203,
      "step": 1206
    },
    {
      "epoch": 0.6393008474576272,
      "grad_norm": 8.953361511230469,
      "learning_rate": 3.606991525423729e-06,
      "loss": 23.4564,
      "step": 1207
    },
    {
      "epoch": 0.6398305084745762,
      "grad_norm": 14.690338134765625,
      "learning_rate": 3.6016949152542374e-06,
      "loss": 24.3561,
      "step": 1208
    },
    {
      "epoch": 0.6403601694915254,
      "grad_norm": 10.943671226501465,
      "learning_rate": 3.596398305084746e-06,
      "loss": 24.5093,
      "step": 1209
    },
    {
      "epoch": 0.6408898305084746,
      "grad_norm": 29.864561080932617,
      "learning_rate": 3.5911016949152545e-06,
      "loss": 23.1173,
      "step": 1210
    },
    {
      "epoch": 0.6414194915254238,
      "grad_norm": 13.453721046447754,
      "learning_rate": 3.5858050847457632e-06,
      "loss": 24.2758,
      "step": 1211
    },
    {
      "epoch": 0.6419491525423728,
      "grad_norm": 10.426319122314453,
      "learning_rate": 3.5805084745762716e-06,
      "loss": 23.3845,
      "step": 1212
    },
    {
      "epoch": 0.642478813559322,
      "grad_norm": 11.651690483093262,
      "learning_rate": 3.5752118644067803e-06,
      "loss": 22.8073,
      "step": 1213
    },
    {
      "epoch": 0.6430084745762712,
      "grad_norm": 10.34839153289795,
      "learning_rate": 3.569915254237288e-06,
      "loss": 23.2429,
      "step": 1214
    },
    {
      "epoch": 0.6435381355932204,
      "grad_norm": 41.4907341003418,
      "learning_rate": 3.564618644067797e-06,
      "loss": 23.151,
      "step": 1215
    },
    {
      "epoch": 0.6440677966101694,
      "grad_norm": 228.6885986328125,
      "learning_rate": 3.5593220338983053e-06,
      "loss": 24.775,
      "step": 1216
    },
    {
      "epoch": 0.6445974576271186,
      "grad_norm": 11.549619674682617,
      "learning_rate": 3.554025423728814e-06,
      "loss": 25.1413,
      "step": 1217
    },
    {
      "epoch": 0.6451271186440678,
      "grad_norm": 16.658159255981445,
      "learning_rate": 3.5487288135593223e-06,
      "loss": 22.7618,
      "step": 1218
    },
    {
      "epoch": 0.645656779661017,
      "grad_norm": 85.88858032226562,
      "learning_rate": 3.543432203389831e-06,
      "loss": 24.1371,
      "step": 1219
    },
    {
      "epoch": 0.6461864406779662,
      "grad_norm": 10.527076721191406,
      "learning_rate": 3.5381355932203394e-06,
      "loss": 23.5518,
      "step": 1220
    },
    {
      "epoch": 0.6467161016949152,
      "grad_norm": 9.125410079956055,
      "learning_rate": 3.5328389830508473e-06,
      "loss": 23.3642,
      "step": 1221
    },
    {
      "epoch": 0.6472457627118644,
      "grad_norm": 9.662039756774902,
      "learning_rate": 3.527542372881356e-06,
      "loss": 23.9111,
      "step": 1222
    },
    {
      "epoch": 0.6477754237288136,
      "grad_norm": 9.905253410339355,
      "learning_rate": 3.5222457627118644e-06,
      "loss": 23.2844,
      "step": 1223
    },
    {
      "epoch": 0.6483050847457628,
      "grad_norm": 21.030120849609375,
      "learning_rate": 3.516949152542373e-06,
      "loss": 22.6156,
      "step": 1224
    },
    {
      "epoch": 0.6488347457627118,
      "grad_norm": 38.88710021972656,
      "learning_rate": 3.5116525423728815e-06,
      "loss": 23.8953,
      "step": 1225
    },
    {
      "epoch": 0.649364406779661,
      "grad_norm": 11.529764175415039,
      "learning_rate": 3.5063559322033902e-06,
      "loss": 22.7202,
      "step": 1226
    },
    {
      "epoch": 0.6498940677966102,
      "grad_norm": 103.3489990234375,
      "learning_rate": 3.5010593220338985e-06,
      "loss": 23.1238,
      "step": 1227
    },
    {
      "epoch": 0.6504237288135594,
      "grad_norm": 9.8300142288208,
      "learning_rate": 3.4957627118644073e-06,
      "loss": 23.9518,
      "step": 1228
    },
    {
      "epoch": 0.6509533898305084,
      "grad_norm": 10.585115432739258,
      "learning_rate": 3.4904661016949156e-06,
      "loss": 23.5841,
      "step": 1229
    },
    {
      "epoch": 0.6514830508474576,
      "grad_norm": 10.97123908996582,
      "learning_rate": 3.4851694915254244e-06,
      "loss": 25.0189,
      "step": 1230
    },
    {
      "epoch": 0.6520127118644068,
      "grad_norm": 9.325423240661621,
      "learning_rate": 3.4798728813559323e-06,
      "loss": 22.9325,
      "step": 1231
    },
    {
      "epoch": 0.652542372881356,
      "grad_norm": 10.707155227661133,
      "learning_rate": 3.474576271186441e-06,
      "loss": 24.7509,
      "step": 1232
    },
    {
      "epoch": 0.653072033898305,
      "grad_norm": 19.400951385498047,
      "learning_rate": 3.4692796610169493e-06,
      "loss": 23.7473,
      "step": 1233
    },
    {
      "epoch": 0.6536016949152542,
      "grad_norm": 8.71873950958252,
      "learning_rate": 3.463983050847458e-06,
      "loss": 23.2639,
      "step": 1234
    },
    {
      "epoch": 0.6541313559322034,
      "grad_norm": 9.999886512756348,
      "learning_rate": 3.4586864406779664e-06,
      "loss": 22.8115,
      "step": 1235
    },
    {
      "epoch": 0.6546610169491526,
      "grad_norm": 10.424912452697754,
      "learning_rate": 3.453389830508475e-06,
      "loss": 24.2166,
      "step": 1236
    },
    {
      "epoch": 0.6551906779661016,
      "grad_norm": 10.064653396606445,
      "learning_rate": 3.4480932203389835e-06,
      "loss": 24.588,
      "step": 1237
    },
    {
      "epoch": 0.6557203389830508,
      "grad_norm": 13.618171691894531,
      "learning_rate": 3.4427966101694922e-06,
      "loss": 23.6279,
      "step": 1238
    },
    {
      "epoch": 0.65625,
      "grad_norm": 9.8294095993042,
      "learning_rate": 3.4375e-06,
      "loss": 23.7302,
      "step": 1239
    },
    {
      "epoch": 0.6567796610169492,
      "grad_norm": 9.663495063781738,
      "learning_rate": 3.4322033898305084e-06,
      "loss": 23.6878,
      "step": 1240
    },
    {
      "epoch": 0.6573093220338984,
      "grad_norm": 11.600693702697754,
      "learning_rate": 3.426906779661017e-06,
      "loss": 25.264,
      "step": 1241
    },
    {
      "epoch": 0.6578389830508474,
      "grad_norm": 22.763347625732422,
      "learning_rate": 3.4216101694915255e-06,
      "loss": 23.6316,
      "step": 1242
    },
    {
      "epoch": 0.6583686440677966,
      "grad_norm": 10.425761222839355,
      "learning_rate": 3.4163135593220343e-06,
      "loss": 22.9004,
      "step": 1243
    },
    {
      "epoch": 0.6588983050847458,
      "grad_norm": 10.619110107421875,
      "learning_rate": 3.4110169491525426e-06,
      "loss": 22.8133,
      "step": 1244
    },
    {
      "epoch": 0.659427966101695,
      "grad_norm": 14.020556449890137,
      "learning_rate": 3.4057203389830513e-06,
      "loss": 23.2571,
      "step": 1245
    },
    {
      "epoch": 0.659957627118644,
      "grad_norm": 10.536483764648438,
      "learning_rate": 3.4004237288135592e-06,
      "loss": 23.6092,
      "step": 1246
    },
    {
      "epoch": 0.6604872881355932,
      "grad_norm": 17.70094108581543,
      "learning_rate": 3.395127118644068e-06,
      "loss": 24.2733,
      "step": 1247
    },
    {
      "epoch": 0.6610169491525424,
      "grad_norm": 15.497590065002441,
      "learning_rate": 3.3898305084745763e-06,
      "loss": 22.6176,
      "step": 1248
    },
    {
      "epoch": 0.6615466101694916,
      "grad_norm": 10.057859420776367,
      "learning_rate": 3.384533898305085e-06,
      "loss": 22.8067,
      "step": 1249
    },
    {
      "epoch": 0.6620762711864406,
      "grad_norm": 10.033709526062012,
      "learning_rate": 3.3792372881355934e-06,
      "loss": 23.6486,
      "step": 1250
    },
    {
      "epoch": 0.6626059322033898,
      "grad_norm": 10.492881774902344,
      "learning_rate": 3.373940677966102e-06,
      "loss": 23.9741,
      "step": 1251
    },
    {
      "epoch": 0.663135593220339,
      "grad_norm": 138.3566436767578,
      "learning_rate": 3.3686440677966105e-06,
      "loss": 23.8718,
      "step": 1252
    },
    {
      "epoch": 0.6636652542372882,
      "grad_norm": 14.000347137451172,
      "learning_rate": 3.363347457627119e-06,
      "loss": 23.0474,
      "step": 1253
    },
    {
      "epoch": 0.6641949152542372,
      "grad_norm": 10.329654693603516,
      "learning_rate": 3.3580508474576275e-06,
      "loss": 23.1419,
      "step": 1254
    },
    {
      "epoch": 0.6647245762711864,
      "grad_norm": 10.36619758605957,
      "learning_rate": 3.3527542372881363e-06,
      "loss": 24.2603,
      "step": 1255
    },
    {
      "epoch": 0.6652542372881356,
      "grad_norm": 9.043996810913086,
      "learning_rate": 3.347457627118644e-06,
      "loss": 23.8408,
      "step": 1256
    },
    {
      "epoch": 0.6657838983050848,
      "grad_norm": 11.091649055480957,
      "learning_rate": 3.342161016949153e-06,
      "loss": 23.5326,
      "step": 1257
    },
    {
      "epoch": 0.6663135593220338,
      "grad_norm": 38.58718490600586,
      "learning_rate": 3.3368644067796612e-06,
      "loss": 23.7141,
      "step": 1258
    },
    {
      "epoch": 0.666843220338983,
      "grad_norm": 12.91553783416748,
      "learning_rate": 3.3315677966101696e-06,
      "loss": 23.9176,
      "step": 1259
    },
    {
      "epoch": 0.6673728813559322,
      "grad_norm": 28.393253326416016,
      "learning_rate": 3.3262711864406783e-06,
      "loss": 23.4979,
      "step": 1260
    },
    {
      "epoch": 0.6679025423728814,
      "grad_norm": 74.48238372802734,
      "learning_rate": 3.3209745762711866e-06,
      "loss": 23.8705,
      "step": 1261
    },
    {
      "epoch": 0.6684322033898306,
      "grad_norm": 11.580681800842285,
      "learning_rate": 3.3156779661016954e-06,
      "loss": 23.2448,
      "step": 1262
    },
    {
      "epoch": 0.6689618644067796,
      "grad_norm": 55.25362014770508,
      "learning_rate": 3.3103813559322033e-06,
      "loss": 23.517,
      "step": 1263
    },
    {
      "epoch": 0.6694915254237288,
      "grad_norm": 13.574389457702637,
      "learning_rate": 3.305084745762712e-06,
      "loss": 23.6044,
      "step": 1264
    },
    {
      "epoch": 0.670021186440678,
      "grad_norm": 9.91126537322998,
      "learning_rate": 3.2997881355932204e-06,
      "loss": 24.0526,
      "step": 1265
    },
    {
      "epoch": 0.6705508474576272,
      "grad_norm": 30.464168548583984,
      "learning_rate": 3.294491525423729e-06,
      "loss": 23.2474,
      "step": 1266
    },
    {
      "epoch": 0.6710805084745762,
      "grad_norm": 9.10863971710205,
      "learning_rate": 3.2891949152542374e-06,
      "loss": 22.5862,
      "step": 1267
    },
    {
      "epoch": 0.6716101694915254,
      "grad_norm": 9.896443367004395,
      "learning_rate": 3.283898305084746e-06,
      "loss": 23.2332,
      "step": 1268
    },
    {
      "epoch": 0.6721398305084746,
      "grad_norm": 18.492416381835938,
      "learning_rate": 3.2786016949152545e-06,
      "loss": 23.7644,
      "step": 1269
    },
    {
      "epoch": 0.6726694915254238,
      "grad_norm": 13.608457565307617,
      "learning_rate": 3.2733050847457633e-06,
      "loss": 23.5125,
      "step": 1270
    },
    {
      "epoch": 0.6731991525423728,
      "grad_norm": 39.14659881591797,
      "learning_rate": 3.268008474576271e-06,
      "loss": 23.0344,
      "step": 1271
    },
    {
      "epoch": 0.673728813559322,
      "grad_norm": 11.313254356384277,
      "learning_rate": 3.26271186440678e-06,
      "loss": 22.4895,
      "step": 1272
    },
    {
      "epoch": 0.6742584745762712,
      "grad_norm": 9.348698616027832,
      "learning_rate": 3.2574152542372882e-06,
      "loss": 23.2606,
      "step": 1273
    },
    {
      "epoch": 0.6747881355932204,
      "grad_norm": 12.74596118927002,
      "learning_rate": 3.252118644067797e-06,
      "loss": 22.0576,
      "step": 1274
    },
    {
      "epoch": 0.6753177966101694,
      "grad_norm": 21.668973922729492,
      "learning_rate": 3.2468220338983053e-06,
      "loss": 24.9295,
      "step": 1275
    },
    {
      "epoch": 0.6758474576271186,
      "grad_norm": 10.773639678955078,
      "learning_rate": 3.241525423728814e-06,
      "loss": 23.3794,
      "step": 1276
    },
    {
      "epoch": 0.6763771186440678,
      "grad_norm": 15.504162788391113,
      "learning_rate": 3.2362288135593224e-06,
      "loss": 23.8239,
      "step": 1277
    },
    {
      "epoch": 0.676906779661017,
      "grad_norm": 10.498186111450195,
      "learning_rate": 3.230932203389831e-06,
      "loss": 23.855,
      "step": 1278
    },
    {
      "epoch": 0.6774364406779662,
      "grad_norm": 11.421833992004395,
      "learning_rate": 3.2256355932203394e-06,
      "loss": 23.3279,
      "step": 1279
    },
    {
      "epoch": 0.6779661016949152,
      "grad_norm": 18.672578811645508,
      "learning_rate": 3.2203389830508473e-06,
      "loss": 22.848,
      "step": 1280
    },
    {
      "epoch": 0.6784957627118644,
      "grad_norm": 10.680612564086914,
      "learning_rate": 3.215042372881356e-06,
      "loss": 23.5434,
      "step": 1281
    },
    {
      "epoch": 0.6790254237288136,
      "grad_norm": 12.694751739501953,
      "learning_rate": 3.2097457627118644e-06,
      "loss": 23.5521,
      "step": 1282
    },
    {
      "epoch": 0.6795550847457628,
      "grad_norm": 11.034235954284668,
      "learning_rate": 3.204449152542373e-06,
      "loss": 22.1808,
      "step": 1283
    },
    {
      "epoch": 0.6800847457627118,
      "grad_norm": 10.518340110778809,
      "learning_rate": 3.1991525423728815e-06,
      "loss": 23.6271,
      "step": 1284
    },
    {
      "epoch": 0.680614406779661,
      "grad_norm": 15.377918243408203,
      "learning_rate": 3.1938559322033902e-06,
      "loss": 22.4605,
      "step": 1285
    },
    {
      "epoch": 0.6811440677966102,
      "grad_norm": 10.993374824523926,
      "learning_rate": 3.1885593220338986e-06,
      "loss": 22.0903,
      "step": 1286
    },
    {
      "epoch": 0.6816737288135594,
      "grad_norm": 9.668951034545898,
      "learning_rate": 3.1832627118644073e-06,
      "loss": 22.9658,
      "step": 1287
    },
    {
      "epoch": 0.6822033898305084,
      "grad_norm": 10.404802322387695,
      "learning_rate": 3.1779661016949152e-06,
      "loss": 23.301,
      "step": 1288
    },
    {
      "epoch": 0.6827330508474576,
      "grad_norm": 11.921629905700684,
      "learning_rate": 3.172669491525424e-06,
      "loss": 22.2202,
      "step": 1289
    },
    {
      "epoch": 0.6832627118644068,
      "grad_norm": 16.25481605529785,
      "learning_rate": 3.1673728813559323e-06,
      "loss": 23.7344,
      "step": 1290
    },
    {
      "epoch": 0.683792372881356,
      "grad_norm": 8.797981262207031,
      "learning_rate": 3.162076271186441e-06,
      "loss": 22.3776,
      "step": 1291
    },
    {
      "epoch": 0.684322033898305,
      "grad_norm": 10.16800594329834,
      "learning_rate": 3.1567796610169494e-06,
      "loss": 23.7705,
      "step": 1292
    },
    {
      "epoch": 0.6848516949152542,
      "grad_norm": 9.02823543548584,
      "learning_rate": 3.151483050847458e-06,
      "loss": 22.2229,
      "step": 1293
    },
    {
      "epoch": 0.6853813559322034,
      "grad_norm": 10.340743064880371,
      "learning_rate": 3.1461864406779664e-06,
      "loss": 23.5098,
      "step": 1294
    },
    {
      "epoch": 0.6859110169491526,
      "grad_norm": 28.614072799682617,
      "learning_rate": 3.140889830508475e-06,
      "loss": 22.6949,
      "step": 1295
    },
    {
      "epoch": 0.6864406779661016,
      "grad_norm": 42.818382263183594,
      "learning_rate": 3.135593220338983e-06,
      "loss": 23.1317,
      "step": 1296
    },
    {
      "epoch": 0.6869703389830508,
      "grad_norm": 11.680004119873047,
      "learning_rate": 3.1302966101694922e-06,
      "loss": 22.7322,
      "step": 1297
    },
    {
      "epoch": 0.6875,
      "grad_norm": 15.806204795837402,
      "learning_rate": 3.125e-06,
      "loss": 23.6652,
      "step": 1298
    },
    {
      "epoch": 0.6880296610169492,
      "grad_norm": 10.330615043640137,
      "learning_rate": 3.1197033898305085e-06,
      "loss": 23.3661,
      "step": 1299
    },
    {
      "epoch": 0.6885593220338984,
      "grad_norm": 9.95855712890625,
      "learning_rate": 3.1144067796610172e-06,
      "loss": 22.6358,
      "step": 1300
    },
    {
      "epoch": 0.6890889830508474,
      "grad_norm": 11.579343795776367,
      "learning_rate": 3.1091101694915255e-06,
      "loss": 22.627,
      "step": 1301
    },
    {
      "epoch": 0.6896186440677966,
      "grad_norm": 10.553118705749512,
      "learning_rate": 3.1038135593220343e-06,
      "loss": 22.7942,
      "step": 1302
    },
    {
      "epoch": 0.6901483050847458,
      "grad_norm": 10.932621955871582,
      "learning_rate": 3.0985169491525426e-06,
      "loss": 22.6523,
      "step": 1303
    },
    {
      "epoch": 0.690677966101695,
      "grad_norm": 13.309427261352539,
      "learning_rate": 3.0932203389830514e-06,
      "loss": 23.7154,
      "step": 1304
    },
    {
      "epoch": 0.691207627118644,
      "grad_norm": 13.14798355102539,
      "learning_rate": 3.0879237288135593e-06,
      "loss": 22.8299,
      "step": 1305
    },
    {
      "epoch": 0.6917372881355932,
      "grad_norm": 11.421483993530273,
      "learning_rate": 3.082627118644068e-06,
      "loss": 23.0336,
      "step": 1306
    },
    {
      "epoch": 0.6922669491525424,
      "grad_norm": 10.340615272521973,
      "learning_rate": 3.0773305084745763e-06,
      "loss": 23.4916,
      "step": 1307
    },
    {
      "epoch": 0.6927966101694916,
      "grad_norm": 11.427314758300781,
      "learning_rate": 3.072033898305085e-06,
      "loss": 22.5604,
      "step": 1308
    },
    {
      "epoch": 0.6933262711864406,
      "grad_norm": 11.155261993408203,
      "learning_rate": 3.0667372881355934e-06,
      "loss": 22.1965,
      "step": 1309
    },
    {
      "epoch": 0.6938559322033898,
      "grad_norm": 11.384081840515137,
      "learning_rate": 3.061440677966102e-06,
      "loss": 22.8625,
      "step": 1310
    },
    {
      "epoch": 0.694385593220339,
      "grad_norm": 10.825870513916016,
      "learning_rate": 3.0561440677966105e-06,
      "loss": 22.574,
      "step": 1311
    },
    {
      "epoch": 0.6949152542372882,
      "grad_norm": 10.077880859375,
      "learning_rate": 3.0508474576271192e-06,
      "loss": 23.4613,
      "step": 1312
    },
    {
      "epoch": 0.6954449152542372,
      "grad_norm": 12.125182151794434,
      "learning_rate": 3.045550847457627e-06,
      "loss": 22.9281,
      "step": 1313
    },
    {
      "epoch": 0.6959745762711864,
      "grad_norm": 18.800735473632812,
      "learning_rate": 3.040254237288136e-06,
      "loss": 22.4387,
      "step": 1314
    },
    {
      "epoch": 0.6965042372881356,
      "grad_norm": 10.947163581848145,
      "learning_rate": 3.034957627118644e-06,
      "loss": 23.8883,
      "step": 1315
    },
    {
      "epoch": 0.6970338983050848,
      "grad_norm": 23.843814849853516,
      "learning_rate": 3.029661016949153e-06,
      "loss": 23.8119,
      "step": 1316
    },
    {
      "epoch": 0.6975635593220338,
      "grad_norm": 10.97914981842041,
      "learning_rate": 3.0243644067796613e-06,
      "loss": 21.8837,
      "step": 1317
    },
    {
      "epoch": 0.698093220338983,
      "grad_norm": 10.892118453979492,
      "learning_rate": 3.0190677966101696e-06,
      "loss": 22.1218,
      "step": 1318
    },
    {
      "epoch": 0.6986228813559322,
      "grad_norm": 124.17777252197266,
      "learning_rate": 3.0137711864406783e-06,
      "loss": 23.8787,
      "step": 1319
    },
    {
      "epoch": 0.6991525423728814,
      "grad_norm": 11.263764381408691,
      "learning_rate": 3.0084745762711862e-06,
      "loss": 23.3216,
      "step": 1320
    },
    {
      "epoch": 0.6996822033898306,
      "grad_norm": 171.69134521484375,
      "learning_rate": 3.003177966101695e-06,
      "loss": 23.3937,
      "step": 1321
    },
    {
      "epoch": 0.7002118644067796,
      "grad_norm": 12.503958702087402,
      "learning_rate": 2.9978813559322033e-06,
      "loss": 22.6932,
      "step": 1322
    },
    {
      "epoch": 0.7007415254237288,
      "grad_norm": 18.390302658081055,
      "learning_rate": 2.992584745762712e-06,
      "loss": 22.6706,
      "step": 1323
    },
    {
      "epoch": 0.701271186440678,
      "grad_norm": 57.863525390625,
      "learning_rate": 2.9872881355932204e-06,
      "loss": 22.2516,
      "step": 1324
    },
    {
      "epoch": 0.7018008474576272,
      "grad_norm": 80.01395416259766,
      "learning_rate": 2.981991525423729e-06,
      "loss": 23.5685,
      "step": 1325
    },
    {
      "epoch": 0.7023305084745762,
      "grad_norm": 22.717716217041016,
      "learning_rate": 2.9766949152542375e-06,
      "loss": 22.79,
      "step": 1326
    },
    {
      "epoch": 0.7028601694915254,
      "grad_norm": 10.5966796875,
      "learning_rate": 2.9713983050847462e-06,
      "loss": 24.1506,
      "step": 1327
    },
    {
      "epoch": 0.7033898305084746,
      "grad_norm": 9.797510147094727,
      "learning_rate": 2.9661016949152545e-06,
      "loss": 22.6484,
      "step": 1328
    },
    {
      "epoch": 0.7039194915254238,
      "grad_norm": 93.52801513671875,
      "learning_rate": 2.9608050847457633e-06,
      "loss": 22.5459,
      "step": 1329
    },
    {
      "epoch": 0.7044491525423728,
      "grad_norm": 10.425220489501953,
      "learning_rate": 2.955508474576271e-06,
      "loss": 23.4443,
      "step": 1330
    },
    {
      "epoch": 0.704978813559322,
      "grad_norm": 9.98812484741211,
      "learning_rate": 2.95021186440678e-06,
      "loss": 22.9209,
      "step": 1331
    },
    {
      "epoch": 0.7055084745762712,
      "grad_norm": 10.473970413208008,
      "learning_rate": 2.9449152542372883e-06,
      "loss": 22.5356,
      "step": 1332
    },
    {
      "epoch": 0.7060381355932204,
      "grad_norm": 9.017339706420898,
      "learning_rate": 2.939618644067797e-06,
      "loss": 23.3649,
      "step": 1333
    },
    {
      "epoch": 0.7065677966101694,
      "grad_norm": 9.518965721130371,
      "learning_rate": 2.9343220338983053e-06,
      "loss": 22.8817,
      "step": 1334
    },
    {
      "epoch": 0.7070974576271186,
      "grad_norm": 10.563639640808105,
      "learning_rate": 2.929025423728814e-06,
      "loss": 23.998,
      "step": 1335
    },
    {
      "epoch": 0.7076271186440678,
      "grad_norm": 12.420814514160156,
      "learning_rate": 2.9237288135593224e-06,
      "loss": 23.8765,
      "step": 1336
    },
    {
      "epoch": 0.708156779661017,
      "grad_norm": 10.50168228149414,
      "learning_rate": 2.918432203389831e-06,
      "loss": 22.7585,
      "step": 1337
    },
    {
      "epoch": 0.7086864406779662,
      "grad_norm": 10.600319862365723,
      "learning_rate": 2.913135593220339e-06,
      "loss": 22.4532,
      "step": 1338
    },
    {
      "epoch": 0.7092161016949152,
      "grad_norm": 11.305073738098145,
      "learning_rate": 2.9078389830508474e-06,
      "loss": 21.6725,
      "step": 1339
    },
    {
      "epoch": 0.7097457627118644,
      "grad_norm": 29.892696380615234,
      "learning_rate": 2.902542372881356e-06,
      "loss": 22.9176,
      "step": 1340
    },
    {
      "epoch": 0.7102754237288136,
      "grad_norm": 12.988981246948242,
      "learning_rate": 2.8972457627118644e-06,
      "loss": 21.9255,
      "step": 1341
    },
    {
      "epoch": 0.7108050847457628,
      "grad_norm": 12.099458694458008,
      "learning_rate": 2.891949152542373e-06,
      "loss": 22.9242,
      "step": 1342
    },
    {
      "epoch": 0.7113347457627118,
      "grad_norm": 17.97080421447754,
      "learning_rate": 2.8866525423728815e-06,
      "loss": 23.0762,
      "step": 1343
    },
    {
      "epoch": 0.711864406779661,
      "grad_norm": 13.269303321838379,
      "learning_rate": 2.8813559322033903e-06,
      "loss": 20.9525,
      "step": 1344
    },
    {
      "epoch": 0.7123940677966102,
      "grad_norm": 11.17467212677002,
      "learning_rate": 2.876059322033898e-06,
      "loss": 22.5635,
      "step": 1345
    },
    {
      "epoch": 0.7129237288135594,
      "grad_norm": 16.212600708007812,
      "learning_rate": 2.8707627118644073e-06,
      "loss": 21.7964,
      "step": 1346
    },
    {
      "epoch": 0.7134533898305084,
      "grad_norm": 10.851371765136719,
      "learning_rate": 2.8654661016949152e-06,
      "loss": 22.7633,
      "step": 1347
    },
    {
      "epoch": 0.7139830508474576,
      "grad_norm": 10.423494338989258,
      "learning_rate": 2.860169491525424e-06,
      "loss": 22.9589,
      "step": 1348
    },
    {
      "epoch": 0.7145127118644068,
      "grad_norm": 12.109452247619629,
      "learning_rate": 2.8548728813559323e-06,
      "loss": 21.6587,
      "step": 1349
    },
    {
      "epoch": 0.715042372881356,
      "grad_norm": 17.420120239257812,
      "learning_rate": 2.849576271186441e-06,
      "loss": 23.0469,
      "step": 1350
    },
    {
      "epoch": 0.715572033898305,
      "grad_norm": 18.60619354248047,
      "learning_rate": 2.8442796610169494e-06,
      "loss": 23.1867,
      "step": 1351
    },
    {
      "epoch": 0.7161016949152542,
      "grad_norm": 10.411613464355469,
      "learning_rate": 2.838983050847458e-06,
      "loss": 23.3354,
      "step": 1352
    },
    {
      "epoch": 0.7166313559322034,
      "grad_norm": 162.94482421875,
      "learning_rate": 2.8336864406779665e-06,
      "loss": 23.3434,
      "step": 1353
    },
    {
      "epoch": 0.7171610169491526,
      "grad_norm": 10.431618690490723,
      "learning_rate": 2.828389830508475e-06,
      "loss": 22.5537,
      "step": 1354
    },
    {
      "epoch": 0.7176906779661016,
      "grad_norm": 13.052170753479004,
      "learning_rate": 2.823093220338983e-06,
      "loss": 23.2337,
      "step": 1355
    },
    {
      "epoch": 0.7182203389830508,
      "grad_norm": 11.258642196655273,
      "learning_rate": 2.817796610169492e-06,
      "loss": 22.0019,
      "step": 1356
    },
    {
      "epoch": 0.71875,
      "grad_norm": 9.59509563446045,
      "learning_rate": 2.8125e-06,
      "loss": 22.6863,
      "step": 1357
    },
    {
      "epoch": 0.7192796610169492,
      "grad_norm": 10.362071990966797,
      "learning_rate": 2.8072033898305085e-06,
      "loss": 23.1583,
      "step": 1358
    },
    {
      "epoch": 0.7198093220338984,
      "grad_norm": 11.35006332397461,
      "learning_rate": 2.8019067796610172e-06,
      "loss": 23.1595,
      "step": 1359
    },
    {
      "epoch": 0.7203389830508474,
      "grad_norm": 9.695998191833496,
      "learning_rate": 2.7966101694915256e-06,
      "loss": 23.076,
      "step": 1360
    },
    {
      "epoch": 0.7208686440677966,
      "grad_norm": 186.3176727294922,
      "learning_rate": 2.7913135593220343e-06,
      "loss": 22.5727,
      "step": 1361
    },
    {
      "epoch": 0.7213983050847458,
      "grad_norm": 25.869657516479492,
      "learning_rate": 2.7860169491525422e-06,
      "loss": 22.3083,
      "step": 1362
    },
    {
      "epoch": 0.721927966101695,
      "grad_norm": 10.12926959991455,
      "learning_rate": 2.780720338983051e-06,
      "loss": 23.4078,
      "step": 1363
    },
    {
      "epoch": 0.722457627118644,
      "grad_norm": 14.441068649291992,
      "learning_rate": 2.7754237288135593e-06,
      "loss": 22.669,
      "step": 1364
    },
    {
      "epoch": 0.7229872881355932,
      "grad_norm": 10.394011497497559,
      "learning_rate": 2.770127118644068e-06,
      "loss": 22.5018,
      "step": 1365
    },
    {
      "epoch": 0.7235169491525424,
      "grad_norm": 13.163445472717285,
      "learning_rate": 2.7648305084745764e-06,
      "loss": 22.9322,
      "step": 1366
    },
    {
      "epoch": 0.7240466101694916,
      "grad_norm": 9.879913330078125,
      "learning_rate": 2.759533898305085e-06,
      "loss": 22.5093,
      "step": 1367
    },
    {
      "epoch": 0.7245762711864406,
      "grad_norm": 16.15762710571289,
      "learning_rate": 2.7542372881355934e-06,
      "loss": 20.9159,
      "step": 1368
    },
    {
      "epoch": 0.7251059322033898,
      "grad_norm": 9.701997756958008,
      "learning_rate": 2.748940677966102e-06,
      "loss": 22.5742,
      "step": 1369
    },
    {
      "epoch": 0.725635593220339,
      "grad_norm": 12.095219612121582,
      "learning_rate": 2.7436440677966105e-06,
      "loss": 23.5293,
      "step": 1370
    },
    {
      "epoch": 0.7261652542372882,
      "grad_norm": 54.299949645996094,
      "learning_rate": 2.7383474576271193e-06,
      "loss": 22.4435,
      "step": 1371
    },
    {
      "epoch": 0.7266949152542372,
      "grad_norm": 10.642538070678711,
      "learning_rate": 2.733050847457627e-06,
      "loss": 22.1918,
      "step": 1372
    },
    {
      "epoch": 0.7272245762711864,
      "grad_norm": 30.163721084594727,
      "learning_rate": 2.727754237288136e-06,
      "loss": 22.1321,
      "step": 1373
    },
    {
      "epoch": 0.7277542372881356,
      "grad_norm": 10.204209327697754,
      "learning_rate": 2.7224576271186442e-06,
      "loss": 22.1433,
      "step": 1374
    },
    {
      "epoch": 0.7282838983050848,
      "grad_norm": 10.49092960357666,
      "learning_rate": 2.717161016949153e-06,
      "loss": 22.6413,
      "step": 1375
    },
    {
      "epoch": 0.7288135593220338,
      "grad_norm": 13.016411781311035,
      "learning_rate": 2.7118644067796613e-06,
      "loss": 23.3965,
      "step": 1376
    },
    {
      "epoch": 0.729343220338983,
      "grad_norm": 14.416947364807129,
      "learning_rate": 2.7065677966101696e-06,
      "loss": 21.9563,
      "step": 1377
    },
    {
      "epoch": 0.7298728813559322,
      "grad_norm": 10.702178955078125,
      "learning_rate": 2.7012711864406784e-06,
      "loss": 22.969,
      "step": 1378
    },
    {
      "epoch": 0.7304025423728814,
      "grad_norm": 9.945716857910156,
      "learning_rate": 2.6959745762711863e-06,
      "loss": 22.8203,
      "step": 1379
    },
    {
      "epoch": 0.7309322033898306,
      "grad_norm": 18.946273803710938,
      "learning_rate": 2.690677966101695e-06,
      "loss": 22.0792,
      "step": 1380
    },
    {
      "epoch": 0.7314618644067796,
      "grad_norm": 10.267123222351074,
      "learning_rate": 2.6853813559322033e-06,
      "loss": 23.2607,
      "step": 1381
    },
    {
      "epoch": 0.7319915254237288,
      "grad_norm": 10.907559394836426,
      "learning_rate": 2.680084745762712e-06,
      "loss": 22.7749,
      "step": 1382
    },
    {
      "epoch": 0.732521186440678,
      "grad_norm": 10.888479232788086,
      "learning_rate": 2.6747881355932204e-06,
      "loss": 22.4084,
      "step": 1383
    },
    {
      "epoch": 0.7330508474576272,
      "grad_norm": 21.53687286376953,
      "learning_rate": 2.669491525423729e-06,
      "loss": 22.2142,
      "step": 1384
    },
    {
      "epoch": 0.7335805084745762,
      "grad_norm": 272.1898193359375,
      "learning_rate": 2.6641949152542375e-06,
      "loss": 21.1849,
      "step": 1385
    },
    {
      "epoch": 0.7341101694915254,
      "grad_norm": 11.453563690185547,
      "learning_rate": 2.6588983050847462e-06,
      "loss": 23.0154,
      "step": 1386
    },
    {
      "epoch": 0.7346398305084746,
      "grad_norm": 15.219632148742676,
      "learning_rate": 2.653601694915254e-06,
      "loss": 23.2121,
      "step": 1387
    },
    {
      "epoch": 0.7351694915254238,
      "grad_norm": 10.583902359008789,
      "learning_rate": 2.648305084745763e-06,
      "loss": 22.0402,
      "step": 1388
    },
    {
      "epoch": 0.7356991525423728,
      "grad_norm": 195.5032501220703,
      "learning_rate": 2.6430084745762712e-06,
      "loss": 23.0628,
      "step": 1389
    },
    {
      "epoch": 0.736228813559322,
      "grad_norm": 12.251043319702148,
      "learning_rate": 2.63771186440678e-06,
      "loss": 22.6376,
      "step": 1390
    },
    {
      "epoch": 0.7367584745762712,
      "grad_norm": 36.90925216674805,
      "learning_rate": 2.6324152542372883e-06,
      "loss": 22.9813,
      "step": 1391
    },
    {
      "epoch": 0.7372881355932204,
      "grad_norm": 10.182879447937012,
      "learning_rate": 2.627118644067797e-06,
      "loss": 21.0854,
      "step": 1392
    },
    {
      "epoch": 0.7378177966101694,
      "grad_norm": 24.14988136291504,
      "learning_rate": 2.6218220338983054e-06,
      "loss": 21.9006,
      "step": 1393
    },
    {
      "epoch": 0.7383474576271186,
      "grad_norm": 16.651220321655273,
      "learning_rate": 2.616525423728814e-06,
      "loss": 21.8563,
      "step": 1394
    },
    {
      "epoch": 0.7388771186440678,
      "grad_norm": 11.417766571044922,
      "learning_rate": 2.6112288135593224e-06,
      "loss": 22.6266,
      "step": 1395
    },
    {
      "epoch": 0.739406779661017,
      "grad_norm": 11.58122730255127,
      "learning_rate": 2.605932203389831e-06,
      "loss": 23.0758,
      "step": 1396
    },
    {
      "epoch": 0.7399364406779662,
      "grad_norm": 10.764071464538574,
      "learning_rate": 2.600635593220339e-06,
      "loss": 22.268,
      "step": 1397
    },
    {
      "epoch": 0.7404661016949152,
      "grad_norm": 12.03806209564209,
      "learning_rate": 2.5953389830508474e-06,
      "loss": 23.2342,
      "step": 1398
    },
    {
      "epoch": 0.7409957627118644,
      "grad_norm": 10.415094375610352,
      "learning_rate": 2.590042372881356e-06,
      "loss": 22.4709,
      "step": 1399
    },
    {
      "epoch": 0.7415254237288136,
      "grad_norm": 11.7349271774292,
      "learning_rate": 2.5847457627118645e-06,
      "loss": 22.1733,
      "step": 1400
    },
    {
      "epoch": 0.7420550847457628,
      "grad_norm": 10.823676109313965,
      "learning_rate": 2.5794491525423732e-06,
      "loss": 22.4517,
      "step": 1401
    },
    {
      "epoch": 0.7425847457627118,
      "grad_norm": 87.86529541015625,
      "learning_rate": 2.5741525423728815e-06,
      "loss": 23.1585,
      "step": 1402
    },
    {
      "epoch": 0.743114406779661,
      "grad_norm": 30.532081604003906,
      "learning_rate": 2.5688559322033903e-06,
      "loss": 22.4133,
      "step": 1403
    },
    {
      "epoch": 0.7436440677966102,
      "grad_norm": 12.336112022399902,
      "learning_rate": 2.563559322033898e-06,
      "loss": 23.193,
      "step": 1404
    },
    {
      "epoch": 0.7441737288135594,
      "grad_norm": 11.795466423034668,
      "learning_rate": 2.558262711864407e-06,
      "loss": 23.0085,
      "step": 1405
    },
    {
      "epoch": 0.7447033898305084,
      "grad_norm": 17.491384506225586,
      "learning_rate": 2.5529661016949153e-06,
      "loss": 22.9923,
      "step": 1406
    },
    {
      "epoch": 0.7452330508474576,
      "grad_norm": 16.53756332397461,
      "learning_rate": 2.547669491525424e-06,
      "loss": 22.0009,
      "step": 1407
    },
    {
      "epoch": 0.7457627118644068,
      "grad_norm": 16.734663009643555,
      "learning_rate": 2.5423728813559323e-06,
      "loss": 22.2423,
      "step": 1408
    },
    {
      "epoch": 0.746292372881356,
      "grad_norm": 588.55126953125,
      "learning_rate": 2.537076271186441e-06,
      "loss": 22.1196,
      "step": 1409
    },
    {
      "epoch": 0.746822033898305,
      "grad_norm": 31.58880043029785,
      "learning_rate": 2.5317796610169494e-06,
      "loss": 21.7172,
      "step": 1410
    },
    {
      "epoch": 0.7473516949152542,
      "grad_norm": 9.64962387084961,
      "learning_rate": 2.526483050847458e-06,
      "loss": 22.7773,
      "step": 1411
    },
    {
      "epoch": 0.7478813559322034,
      "grad_norm": 11.388989448547363,
      "learning_rate": 2.521186440677966e-06,
      "loss": 21.2324,
      "step": 1412
    },
    {
      "epoch": 0.7484110169491526,
      "grad_norm": 16.807355880737305,
      "learning_rate": 2.515889830508475e-06,
      "loss": 23.317,
      "step": 1413
    },
    {
      "epoch": 0.7489406779661016,
      "grad_norm": 10.358735084533691,
      "learning_rate": 2.510593220338983e-06,
      "loss": 22.6423,
      "step": 1414
    },
    {
      "epoch": 0.7494703389830508,
      "grad_norm": 11.69416332244873,
      "learning_rate": 2.505296610169492e-06,
      "loss": 21.5766,
      "step": 1415
    },
    {
      "epoch": 0.75,
      "grad_norm": 13.341907501220703,
      "learning_rate": 2.5e-06,
      "loss": 22.7339,
      "step": 1416
    },
    {
      "epoch": 0.7505296610169492,
      "grad_norm": 11.2621431350708,
      "learning_rate": 2.4947033898305085e-06,
      "loss": 23.029,
      "step": 1417
    },
    {
      "epoch": 0.7510593220338984,
      "grad_norm": 10.595707893371582,
      "learning_rate": 2.4894067796610173e-06,
      "loss": 22.8953,
      "step": 1418
    },
    {
      "epoch": 0.7515889830508474,
      "grad_norm": 17.5727596282959,
      "learning_rate": 2.4841101694915256e-06,
      "loss": 23.0363,
      "step": 1419
    },
    {
      "epoch": 0.7521186440677966,
      "grad_norm": 325.8346252441406,
      "learning_rate": 2.4788135593220343e-06,
      "loss": 22.1502,
      "step": 1420
    },
    {
      "epoch": 0.7526483050847458,
      "grad_norm": 16.71080780029297,
      "learning_rate": 2.4735169491525427e-06,
      "loss": 22.1283,
      "step": 1421
    },
    {
      "epoch": 0.753177966101695,
      "grad_norm": 11.90884780883789,
      "learning_rate": 2.468220338983051e-06,
      "loss": 21.3862,
      "step": 1422
    },
    {
      "epoch": 0.753707627118644,
      "grad_norm": 12.220453262329102,
      "learning_rate": 2.4629237288135597e-06,
      "loss": 21.7583,
      "step": 1423
    },
    {
      "epoch": 0.7542372881355932,
      "grad_norm": 13.067875862121582,
      "learning_rate": 2.457627118644068e-06,
      "loss": 21.5401,
      "step": 1424
    },
    {
      "epoch": 0.7547669491525424,
      "grad_norm": 10.395727157592773,
      "learning_rate": 2.4523305084745764e-06,
      "loss": 22.8809,
      "step": 1425
    },
    {
      "epoch": 0.7552966101694916,
      "grad_norm": 17.568561553955078,
      "learning_rate": 2.4470338983050847e-06,
      "loss": 22.0323,
      "step": 1426
    },
    {
      "epoch": 0.7558262711864406,
      "grad_norm": 10.24776554107666,
      "learning_rate": 2.4417372881355935e-06,
      "loss": 22.3934,
      "step": 1427
    },
    {
      "epoch": 0.7563559322033898,
      "grad_norm": 28.52921485900879,
      "learning_rate": 2.436440677966102e-06,
      "loss": 22.0263,
      "step": 1428
    },
    {
      "epoch": 0.756885593220339,
      "grad_norm": 10.999152183532715,
      "learning_rate": 2.43114406779661e-06,
      "loss": 22.961,
      "step": 1429
    },
    {
      "epoch": 0.7574152542372882,
      "grad_norm": 9.9142427444458,
      "learning_rate": 2.425847457627119e-06,
      "loss": 22.0516,
      "step": 1430
    },
    {
      "epoch": 0.7579449152542372,
      "grad_norm": 57.41411209106445,
      "learning_rate": 2.420550847457627e-06,
      "loss": 23.255,
      "step": 1431
    },
    {
      "epoch": 0.7584745762711864,
      "grad_norm": 10.103428840637207,
      "learning_rate": 2.415254237288136e-06,
      "loss": 22.001,
      "step": 1432
    },
    {
      "epoch": 0.7590042372881356,
      "grad_norm": 10.747050285339355,
      "learning_rate": 2.4099576271186443e-06,
      "loss": 22.081,
      "step": 1433
    },
    {
      "epoch": 0.7595338983050848,
      "grad_norm": 13.373562812805176,
      "learning_rate": 2.4046610169491526e-06,
      "loss": 21.5839,
      "step": 1434
    },
    {
      "epoch": 0.7600635593220338,
      "grad_norm": 16.07575798034668,
      "learning_rate": 2.3993644067796613e-06,
      "loss": 21.9969,
      "step": 1435
    },
    {
      "epoch": 0.760593220338983,
      "grad_norm": 14.619297981262207,
      "learning_rate": 2.3940677966101697e-06,
      "loss": 22.9245,
      "step": 1436
    },
    {
      "epoch": 0.7611228813559322,
      "grad_norm": 79.43356323242188,
      "learning_rate": 2.388771186440678e-06,
      "loss": 22.3168,
      "step": 1437
    },
    {
      "epoch": 0.7616525423728814,
      "grad_norm": 21.654813766479492,
      "learning_rate": 2.3834745762711867e-06,
      "loss": 23.0362,
      "step": 1438
    },
    {
      "epoch": 0.7621822033898306,
      "grad_norm": 23.23194694519043,
      "learning_rate": 2.378177966101695e-06,
      "loss": 22.1913,
      "step": 1439
    },
    {
      "epoch": 0.7627118644067796,
      "grad_norm": 12.026060104370117,
      "learning_rate": 2.372881355932204e-06,
      "loss": 22.2314,
      "step": 1440
    },
    {
      "epoch": 0.7632415254237288,
      "grad_norm": 180.8503875732422,
      "learning_rate": 2.367584745762712e-06,
      "loss": 22.1149,
      "step": 1441
    },
    {
      "epoch": 0.763771186440678,
      "grad_norm": 13.60410213470459,
      "learning_rate": 2.3622881355932204e-06,
      "loss": 22.9171,
      "step": 1442
    },
    {
      "epoch": 0.7643008474576272,
      "grad_norm": 11.33047103881836,
      "learning_rate": 2.356991525423729e-06,
      "loss": 22.1753,
      "step": 1443
    },
    {
      "epoch": 0.7648305084745762,
      "grad_norm": 26.976179122924805,
      "learning_rate": 2.3516949152542375e-06,
      "loss": 22.5394,
      "step": 1444
    },
    {
      "epoch": 0.7653601694915254,
      "grad_norm": 11.057161331176758,
      "learning_rate": 2.3463983050847463e-06,
      "loss": 22.7252,
      "step": 1445
    },
    {
      "epoch": 0.7658898305084746,
      "grad_norm": 20.831729888916016,
      "learning_rate": 2.341101694915254e-06,
      "loss": 21.7503,
      "step": 1446
    },
    {
      "epoch": 0.7664194915254238,
      "grad_norm": 11.924007415771484,
      "learning_rate": 2.335805084745763e-06,
      "loss": 21.2585,
      "step": 1447
    },
    {
      "epoch": 0.7669491525423728,
      "grad_norm": 24.850074768066406,
      "learning_rate": 2.3305084745762712e-06,
      "loss": 22.2775,
      "step": 1448
    },
    {
      "epoch": 0.767478813559322,
      "grad_norm": 21.688560485839844,
      "learning_rate": 2.3252118644067796e-06,
      "loss": 21.2237,
      "step": 1449
    },
    {
      "epoch": 0.7680084745762712,
      "grad_norm": 12.142607688903809,
      "learning_rate": 2.3199152542372883e-06,
      "loss": 22.0624,
      "step": 1450
    },
    {
      "epoch": 0.7685381355932204,
      "grad_norm": 11.18102741241455,
      "learning_rate": 2.3146186440677966e-06,
      "loss": 22.2404,
      "step": 1451
    },
    {
      "epoch": 0.7690677966101694,
      "grad_norm": 12.526704788208008,
      "learning_rate": 2.3093220338983054e-06,
      "loss": 22.673,
      "step": 1452
    },
    {
      "epoch": 0.7695974576271186,
      "grad_norm": 22.714340209960938,
      "learning_rate": 2.3040254237288137e-06,
      "loss": 21.5968,
      "step": 1453
    },
    {
      "epoch": 0.7701271186440678,
      "grad_norm": 21.01752281188965,
      "learning_rate": 2.298728813559322e-06,
      "loss": 21.8815,
      "step": 1454
    },
    {
      "epoch": 0.770656779661017,
      "grad_norm": 10.520763397216797,
      "learning_rate": 2.2934322033898308e-06,
      "loss": 22.4596,
      "step": 1455
    },
    {
      "epoch": 0.7711864406779662,
      "grad_norm": 12.887395858764648,
      "learning_rate": 2.288135593220339e-06,
      "loss": 23.1127,
      "step": 1456
    },
    {
      "epoch": 0.7717161016949152,
      "grad_norm": 10.192863464355469,
      "learning_rate": 2.282838983050848e-06,
      "loss": 22.0498,
      "step": 1457
    },
    {
      "epoch": 0.7722457627118644,
      "grad_norm": 11.254172325134277,
      "learning_rate": 2.277542372881356e-06,
      "loss": 22.6027,
      "step": 1458
    },
    {
      "epoch": 0.7727754237288136,
      "grad_norm": 10.596721649169922,
      "learning_rate": 2.2722457627118645e-06,
      "loss": 22.265,
      "step": 1459
    },
    {
      "epoch": 0.7733050847457628,
      "grad_norm": 23.04663848876953,
      "learning_rate": 2.2669491525423732e-06,
      "loss": 21.2429,
      "step": 1460
    },
    {
      "epoch": 0.7738347457627118,
      "grad_norm": 69.86868286132812,
      "learning_rate": 2.2616525423728816e-06,
      "loss": 21.6517,
      "step": 1461
    },
    {
      "epoch": 0.774364406779661,
      "grad_norm": 10.820298194885254,
      "learning_rate": 2.25635593220339e-06,
      "loss": 23.4,
      "step": 1462
    },
    {
      "epoch": 0.7748940677966102,
      "grad_norm": 10.476434707641602,
      "learning_rate": 2.2510593220338986e-06,
      "loss": 22.6246,
      "step": 1463
    },
    {
      "epoch": 0.7754237288135594,
      "grad_norm": 10.52279281616211,
      "learning_rate": 2.245762711864407e-06,
      "loss": 21.4519,
      "step": 1464
    },
    {
      "epoch": 0.7759533898305084,
      "grad_norm": 14.163872718811035,
      "learning_rate": 2.2404661016949157e-06,
      "loss": 21.7663,
      "step": 1465
    },
    {
      "epoch": 0.7764830508474576,
      "grad_norm": 11.368695259094238,
      "learning_rate": 2.2351694915254236e-06,
      "loss": 21.9032,
      "step": 1466
    },
    {
      "epoch": 0.7770127118644068,
      "grad_norm": 22.995941162109375,
      "learning_rate": 2.2298728813559324e-06,
      "loss": 22.3758,
      "step": 1467
    },
    {
      "epoch": 0.777542372881356,
      "grad_norm": 11.596510887145996,
      "learning_rate": 2.2245762711864407e-06,
      "loss": 21.9793,
      "step": 1468
    },
    {
      "epoch": 0.778072033898305,
      "grad_norm": 31.465652465820312,
      "learning_rate": 2.2192796610169494e-06,
      "loss": 21.8731,
      "step": 1469
    },
    {
      "epoch": 0.7786016949152542,
      "grad_norm": 10.55839729309082,
      "learning_rate": 2.2139830508474578e-06,
      "loss": 22.5795,
      "step": 1470
    },
    {
      "epoch": 0.7791313559322034,
      "grad_norm": 672.3740844726562,
      "learning_rate": 2.208686440677966e-06,
      "loss": 26.4529,
      "step": 1471
    },
    {
      "epoch": 0.7796610169491526,
      "grad_norm": 13.812041282653809,
      "learning_rate": 2.203389830508475e-06,
      "loss": 23.1586,
      "step": 1472
    },
    {
      "epoch": 0.7801906779661016,
      "grad_norm": 64.79163360595703,
      "learning_rate": 2.198093220338983e-06,
      "loss": 21.4981,
      "step": 1473
    },
    {
      "epoch": 0.7807203389830508,
      "grad_norm": 33.998626708984375,
      "learning_rate": 2.1927966101694915e-06,
      "loss": 23.0052,
      "step": 1474
    },
    {
      "epoch": 0.78125,
      "grad_norm": 24.32859230041504,
      "learning_rate": 2.1875000000000002e-06,
      "loss": 22.3328,
      "step": 1475
    },
    {
      "epoch": 0.7817796610169492,
      "grad_norm": 10.702107429504395,
      "learning_rate": 2.1822033898305086e-06,
      "loss": 21.9081,
      "step": 1476
    },
    {
      "epoch": 0.7823093220338984,
      "grad_norm": 10.570788383483887,
      "learning_rate": 2.1769067796610173e-06,
      "loss": 21.4471,
      "step": 1477
    },
    {
      "epoch": 0.7828389830508474,
      "grad_norm": 11.0510892868042,
      "learning_rate": 2.1716101694915256e-06,
      "loss": 21.7015,
      "step": 1478
    },
    {
      "epoch": 0.7833686440677966,
      "grad_norm": 11.695838928222656,
      "learning_rate": 2.166313559322034e-06,
      "loss": 21.6139,
      "step": 1479
    },
    {
      "epoch": 0.7838983050847458,
      "grad_norm": 17.46983528137207,
      "learning_rate": 2.1610169491525427e-06,
      "loss": 22.0627,
      "step": 1480
    },
    {
      "epoch": 0.784427966101695,
      "grad_norm": 104.35166931152344,
      "learning_rate": 2.155720338983051e-06,
      "loss": 21.6728,
      "step": 1481
    },
    {
      "epoch": 0.784957627118644,
      "grad_norm": 49.96333312988281,
      "learning_rate": 2.1504237288135598e-06,
      "loss": 22.3671,
      "step": 1482
    },
    {
      "epoch": 0.7854872881355932,
      "grad_norm": 15.303165435791016,
      "learning_rate": 2.145127118644068e-06,
      "loss": 21.4315,
      "step": 1483
    },
    {
      "epoch": 0.7860169491525424,
      "grad_norm": 26.340612411499023,
      "learning_rate": 2.1398305084745764e-06,
      "loss": 20.445,
      "step": 1484
    },
    {
      "epoch": 0.7865466101694916,
      "grad_norm": 13.723596572875977,
      "learning_rate": 2.1345338983050847e-06,
      "loss": 21.5728,
      "step": 1485
    },
    {
      "epoch": 0.7870762711864406,
      "grad_norm": 12.121112823486328,
      "learning_rate": 2.129237288135593e-06,
      "loss": 21.3968,
      "step": 1486
    },
    {
      "epoch": 0.7876059322033898,
      "grad_norm": 16.091964721679688,
      "learning_rate": 2.123940677966102e-06,
      "loss": 21.291,
      "step": 1487
    },
    {
      "epoch": 0.788135593220339,
      "grad_norm": 12.58288288116455,
      "learning_rate": 2.11864406779661e-06,
      "loss": 20.723,
      "step": 1488
    },
    {
      "epoch": 0.7886652542372882,
      "grad_norm": 67.45587158203125,
      "learning_rate": 2.113347457627119e-06,
      "loss": 22.1106,
      "step": 1489
    },
    {
      "epoch": 0.7891949152542372,
      "grad_norm": 11.37731647491455,
      "learning_rate": 2.1080508474576272e-06,
      "loss": 21.858,
      "step": 1490
    },
    {
      "epoch": 0.7897245762711864,
      "grad_norm": 13.967123031616211,
      "learning_rate": 2.1027542372881355e-06,
      "loss": 21.3897,
      "step": 1491
    },
    {
      "epoch": 0.7902542372881356,
      "grad_norm": 12.801725387573242,
      "learning_rate": 2.0974576271186443e-06,
      "loss": 21.4123,
      "step": 1492
    },
    {
      "epoch": 0.7907838983050848,
      "grad_norm": 11.5734224319458,
      "learning_rate": 2.0921610169491526e-06,
      "loss": 22.2714,
      "step": 1493
    },
    {
      "epoch": 0.7913135593220338,
      "grad_norm": 11.80034351348877,
      "learning_rate": 2.0868644067796614e-06,
      "loss": 22.3617,
      "step": 1494
    },
    {
      "epoch": 0.791843220338983,
      "grad_norm": 231.23033142089844,
      "learning_rate": 2.0815677966101697e-06,
      "loss": 22.3248,
      "step": 1495
    },
    {
      "epoch": 0.7923728813559322,
      "grad_norm": 9.839229583740234,
      "learning_rate": 2.076271186440678e-06,
      "loss": 22.2507,
      "step": 1496
    },
    {
      "epoch": 0.7929025423728814,
      "grad_norm": 14.404692649841309,
      "learning_rate": 2.0709745762711868e-06,
      "loss": 21.7462,
      "step": 1497
    },
    {
      "epoch": 0.7934322033898306,
      "grad_norm": 10.274415969848633,
      "learning_rate": 2.065677966101695e-06,
      "loss": 22.416,
      "step": 1498
    },
    {
      "epoch": 0.7939618644067796,
      "grad_norm": 10.946431159973145,
      "learning_rate": 2.060381355932204e-06,
      "loss": 23.0237,
      "step": 1499
    },
    {
      "epoch": 0.7944915254237288,
      "grad_norm": 165.60311889648438,
      "learning_rate": 2.055084745762712e-06,
      "loss": 23.9758,
      "step": 1500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1888,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4124851568640000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
