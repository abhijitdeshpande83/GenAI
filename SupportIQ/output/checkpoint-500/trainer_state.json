{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.2648305084745763,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0005296610169491525,
      "grad_norm": 9.824673652648926,
      "learning_rate": 9.99470338983051e-06,
      "loss": 49.112,
      "step": 1
    },
    {
      "epoch": 0.001059322033898305,
      "grad_norm": 9.114420890808105,
      "learning_rate": 9.989406779661017e-06,
      "loss": 45.4029,
      "step": 2
    },
    {
      "epoch": 0.0015889830508474577,
      "grad_norm": 7.220670700073242,
      "learning_rate": 9.984110169491527e-06,
      "loss": 44.7267,
      "step": 3
    },
    {
      "epoch": 0.00211864406779661,
      "grad_norm": 8.346141815185547,
      "learning_rate": 9.978813559322034e-06,
      "loss": 46.5697,
      "step": 4
    },
    {
      "epoch": 0.0026483050847457626,
      "grad_norm": 8.710979461669922,
      "learning_rate": 9.973516949152543e-06,
      "loss": 46.0647,
      "step": 5
    },
    {
      "epoch": 0.0031779661016949155,
      "grad_norm": 10.219281196594238,
      "learning_rate": 9.968220338983052e-06,
      "loss": 47.7918,
      "step": 6
    },
    {
      "epoch": 0.003707627118644068,
      "grad_norm": 7.452911376953125,
      "learning_rate": 9.96292372881356e-06,
      "loss": 43.4166,
      "step": 7
    },
    {
      "epoch": 0.00423728813559322,
      "grad_norm": 10.418255805969238,
      "learning_rate": 9.957627118644069e-06,
      "loss": 48.958,
      "step": 8
    },
    {
      "epoch": 0.004766949152542373,
      "grad_norm": 7.719862937927246,
      "learning_rate": 9.952330508474576e-06,
      "loss": 45.2572,
      "step": 9
    },
    {
      "epoch": 0.005296610169491525,
      "grad_norm": 8.043261528015137,
      "learning_rate": 9.947033898305085e-06,
      "loss": 46.2131,
      "step": 10
    },
    {
      "epoch": 0.005826271186440678,
      "grad_norm": 10.416719436645508,
      "learning_rate": 9.941737288135594e-06,
      "loss": 46.287,
      "step": 11
    },
    {
      "epoch": 0.006355932203389831,
      "grad_norm": 8.956294059753418,
      "learning_rate": 9.936440677966102e-06,
      "loss": 44.8255,
      "step": 12
    },
    {
      "epoch": 0.006885593220338983,
      "grad_norm": 10.272127151489258,
      "learning_rate": 9.931144067796611e-06,
      "loss": 47.7993,
      "step": 13
    },
    {
      "epoch": 0.007415254237288136,
      "grad_norm": 10.842305183410645,
      "learning_rate": 9.92584745762712e-06,
      "loss": 49.068,
      "step": 14
    },
    {
      "epoch": 0.007944915254237288,
      "grad_norm": 8.35157585144043,
      "learning_rate": 9.920550847457629e-06,
      "loss": 45.1803,
      "step": 15
    },
    {
      "epoch": 0.00847457627118644,
      "grad_norm": 9.607563018798828,
      "learning_rate": 9.915254237288137e-06,
      "loss": 46.5756,
      "step": 16
    },
    {
      "epoch": 0.009004237288135594,
      "grad_norm": 9.83224868774414,
      "learning_rate": 9.909957627118644e-06,
      "loss": 46.5982,
      "step": 17
    },
    {
      "epoch": 0.009533898305084746,
      "grad_norm": 9.431659698486328,
      "learning_rate": 9.904661016949153e-06,
      "loss": 46.8067,
      "step": 18
    },
    {
      "epoch": 0.010063559322033898,
      "grad_norm": 10.952531814575195,
      "learning_rate": 9.899364406779662e-06,
      "loss": 49.3123,
      "step": 19
    },
    {
      "epoch": 0.01059322033898305,
      "grad_norm": 8.882469177246094,
      "learning_rate": 9.89406779661017e-06,
      "loss": 46.7754,
      "step": 20
    },
    {
      "epoch": 0.011122881355932203,
      "grad_norm": 10.91796588897705,
      "learning_rate": 9.88877118644068e-06,
      "loss": 49.2673,
      "step": 21
    },
    {
      "epoch": 0.011652542372881356,
      "grad_norm": 8.245124816894531,
      "learning_rate": 9.883474576271186e-06,
      "loss": 47.0233,
      "step": 22
    },
    {
      "epoch": 0.012182203389830509,
      "grad_norm": 11.488899230957031,
      "learning_rate": 9.878177966101695e-06,
      "loss": 46.9983,
      "step": 23
    },
    {
      "epoch": 0.012711864406779662,
      "grad_norm": 9.0787992477417,
      "learning_rate": 9.872881355932204e-06,
      "loss": 47.0618,
      "step": 24
    },
    {
      "epoch": 0.013241525423728813,
      "grad_norm": 10.228734970092773,
      "learning_rate": 9.867584745762713e-06,
      "loss": 48.6781,
      "step": 25
    },
    {
      "epoch": 0.013771186440677966,
      "grad_norm": 9.935210227966309,
      "learning_rate": 9.862288135593221e-06,
      "loss": 45.1185,
      "step": 26
    },
    {
      "epoch": 0.014300847457627119,
      "grad_norm": 10.917930603027344,
      "learning_rate": 9.856991525423729e-06,
      "loss": 45.1888,
      "step": 27
    },
    {
      "epoch": 0.014830508474576272,
      "grad_norm": 7.623673439025879,
      "learning_rate": 9.851694915254239e-06,
      "loss": 45.5157,
      "step": 28
    },
    {
      "epoch": 0.015360169491525424,
      "grad_norm": 8.786433219909668,
      "learning_rate": 9.846398305084746e-06,
      "loss": 47.8806,
      "step": 29
    },
    {
      "epoch": 0.015889830508474576,
      "grad_norm": 12.938705444335938,
      "learning_rate": 9.841101694915255e-06,
      "loss": 47.764,
      "step": 30
    },
    {
      "epoch": 0.01641949152542373,
      "grad_norm": 10.660422325134277,
      "learning_rate": 9.835805084745764e-06,
      "loss": 46.2601,
      "step": 31
    },
    {
      "epoch": 0.01694915254237288,
      "grad_norm": 9.035418510437012,
      "learning_rate": 9.830508474576272e-06,
      "loss": 44.5004,
      "step": 32
    },
    {
      "epoch": 0.017478813559322032,
      "grad_norm": 10.623464584350586,
      "learning_rate": 9.825211864406781e-06,
      "loss": 45.7476,
      "step": 33
    },
    {
      "epoch": 0.018008474576271187,
      "grad_norm": 9.58043384552002,
      "learning_rate": 9.819915254237288e-06,
      "loss": 47.5666,
      "step": 34
    },
    {
      "epoch": 0.018538135593220338,
      "grad_norm": 8.9461088180542,
      "learning_rate": 9.814618644067797e-06,
      "loss": 46.0464,
      "step": 35
    },
    {
      "epoch": 0.019067796610169493,
      "grad_norm": 9.794960021972656,
      "learning_rate": 9.809322033898306e-06,
      "loss": 47.527,
      "step": 36
    },
    {
      "epoch": 0.019597457627118644,
      "grad_norm": 10.666226387023926,
      "learning_rate": 9.804025423728814e-06,
      "loss": 47.016,
      "step": 37
    },
    {
      "epoch": 0.020127118644067795,
      "grad_norm": 9.922653198242188,
      "learning_rate": 9.798728813559323e-06,
      "loss": 47.1683,
      "step": 38
    },
    {
      "epoch": 0.02065677966101695,
      "grad_norm": 11.202757835388184,
      "learning_rate": 9.793432203389832e-06,
      "loss": 45.9637,
      "step": 39
    },
    {
      "epoch": 0.0211864406779661,
      "grad_norm": 10.992786407470703,
      "learning_rate": 9.788135593220339e-06,
      "loss": 44.7269,
      "step": 40
    },
    {
      "epoch": 0.021716101694915255,
      "grad_norm": 9.525754928588867,
      "learning_rate": 9.78283898305085e-06,
      "loss": 47.6572,
      "step": 41
    },
    {
      "epoch": 0.022245762711864406,
      "grad_norm": 10.104632377624512,
      "learning_rate": 9.777542372881356e-06,
      "loss": 48.5083,
      "step": 42
    },
    {
      "epoch": 0.022775423728813558,
      "grad_norm": 10.875740051269531,
      "learning_rate": 9.772245762711865e-06,
      "loss": 47.3372,
      "step": 43
    },
    {
      "epoch": 0.023305084745762712,
      "grad_norm": 11.132129669189453,
      "learning_rate": 9.766949152542374e-06,
      "loss": 45.3571,
      "step": 44
    },
    {
      "epoch": 0.023834745762711863,
      "grad_norm": 9.954155921936035,
      "learning_rate": 9.761652542372883e-06,
      "loss": 47.5347,
      "step": 45
    },
    {
      "epoch": 0.024364406779661018,
      "grad_norm": 11.31274700164795,
      "learning_rate": 9.756355932203391e-06,
      "loss": 44.5112,
      "step": 46
    },
    {
      "epoch": 0.02489406779661017,
      "grad_norm": 12.590404510498047,
      "learning_rate": 9.751059322033898e-06,
      "loss": 47.6753,
      "step": 47
    },
    {
      "epoch": 0.025423728813559324,
      "grad_norm": 13.069845199584961,
      "learning_rate": 9.745762711864407e-06,
      "loss": 47.4901,
      "step": 48
    },
    {
      "epoch": 0.025953389830508475,
      "grad_norm": 11.670619010925293,
      "learning_rate": 9.740466101694916e-06,
      "loss": 48.0667,
      "step": 49
    },
    {
      "epoch": 0.026483050847457626,
      "grad_norm": 13.390597343444824,
      "learning_rate": 9.735169491525425e-06,
      "loss": 46.8159,
      "step": 50
    },
    {
      "epoch": 0.02701271186440678,
      "grad_norm": 12.925251960754395,
      "learning_rate": 9.729872881355933e-06,
      "loss": 49.3867,
      "step": 51
    },
    {
      "epoch": 0.02754237288135593,
      "grad_norm": 10.754331588745117,
      "learning_rate": 9.72457627118644e-06,
      "loss": 46.4268,
      "step": 52
    },
    {
      "epoch": 0.028072033898305086,
      "grad_norm": 10.62302017211914,
      "learning_rate": 9.719279661016951e-06,
      "loss": 47.3283,
      "step": 53
    },
    {
      "epoch": 0.028601694915254237,
      "grad_norm": 13.34897518157959,
      "learning_rate": 9.713983050847458e-06,
      "loss": 46.3467,
      "step": 54
    },
    {
      "epoch": 0.02913135593220339,
      "grad_norm": 10.99782657623291,
      "learning_rate": 9.708686440677967e-06,
      "loss": 47.2685,
      "step": 55
    },
    {
      "epoch": 0.029661016949152543,
      "grad_norm": 10.190768241882324,
      "learning_rate": 9.703389830508475e-06,
      "loss": 43.3058,
      "step": 56
    },
    {
      "epoch": 0.030190677966101694,
      "grad_norm": 14.833115577697754,
      "learning_rate": 9.698093220338984e-06,
      "loss": 48.7503,
      "step": 57
    },
    {
      "epoch": 0.03072033898305085,
      "grad_norm": 14.009073257446289,
      "learning_rate": 9.692796610169493e-06,
      "loss": 48.5087,
      "step": 58
    },
    {
      "epoch": 0.03125,
      "grad_norm": 9.995011329650879,
      "learning_rate": 9.6875e-06,
      "loss": 43.5146,
      "step": 59
    },
    {
      "epoch": 0.03177966101694915,
      "grad_norm": 11.539137840270996,
      "learning_rate": 9.682203389830509e-06,
      "loss": 45.2129,
      "step": 60
    },
    {
      "epoch": 0.0323093220338983,
      "grad_norm": 11.501319885253906,
      "learning_rate": 9.676906779661017e-06,
      "loss": 48.0151,
      "step": 61
    },
    {
      "epoch": 0.03283898305084746,
      "grad_norm": 12.755085945129395,
      "learning_rate": 9.671610169491526e-06,
      "loss": 47.2875,
      "step": 62
    },
    {
      "epoch": 0.03336864406779661,
      "grad_norm": 12.369650840759277,
      "learning_rate": 9.666313559322035e-06,
      "loss": 49.4209,
      "step": 63
    },
    {
      "epoch": 0.03389830508474576,
      "grad_norm": 12.440911293029785,
      "learning_rate": 9.661016949152544e-06,
      "loss": 45.788,
      "step": 64
    },
    {
      "epoch": 0.034427966101694914,
      "grad_norm": 10.421191215515137,
      "learning_rate": 9.65572033898305e-06,
      "loss": 45.5261,
      "step": 65
    },
    {
      "epoch": 0.034957627118644065,
      "grad_norm": 11.502129554748535,
      "learning_rate": 9.650423728813561e-06,
      "loss": 46.3147,
      "step": 66
    },
    {
      "epoch": 0.03548728813559322,
      "grad_norm": 10.831786155700684,
      "learning_rate": 9.645127118644068e-06,
      "loss": 45.5018,
      "step": 67
    },
    {
      "epoch": 0.036016949152542374,
      "grad_norm": 11.678831100463867,
      "learning_rate": 9.639830508474577e-06,
      "loss": 48.9308,
      "step": 68
    },
    {
      "epoch": 0.036546610169491525,
      "grad_norm": 11.768855094909668,
      "learning_rate": 9.634533898305086e-06,
      "loss": 45.957,
      "step": 69
    },
    {
      "epoch": 0.037076271186440676,
      "grad_norm": 11.676040649414062,
      "learning_rate": 9.629237288135595e-06,
      "loss": 46.3126,
      "step": 70
    },
    {
      "epoch": 0.03760593220338983,
      "grad_norm": 11.123186111450195,
      "learning_rate": 9.623940677966103e-06,
      "loss": 45.2067,
      "step": 71
    },
    {
      "epoch": 0.038135593220338986,
      "grad_norm": 16.158811569213867,
      "learning_rate": 9.61864406779661e-06,
      "loss": 48.8853,
      "step": 72
    },
    {
      "epoch": 0.03866525423728814,
      "grad_norm": 12.688535690307617,
      "learning_rate": 9.613347457627119e-06,
      "loss": 48.8538,
      "step": 73
    },
    {
      "epoch": 0.03919491525423729,
      "grad_norm": 12.348453521728516,
      "learning_rate": 9.608050847457628e-06,
      "loss": 48.2938,
      "step": 74
    },
    {
      "epoch": 0.03972457627118644,
      "grad_norm": 10.481280326843262,
      "learning_rate": 9.602754237288137e-06,
      "loss": 44.2207,
      "step": 75
    },
    {
      "epoch": 0.04025423728813559,
      "grad_norm": 12.226302146911621,
      "learning_rate": 9.597457627118645e-06,
      "loss": 45.945,
      "step": 76
    },
    {
      "epoch": 0.04078389830508475,
      "grad_norm": 12.43635368347168,
      "learning_rate": 9.592161016949152e-06,
      "loss": 47.7524,
      "step": 77
    },
    {
      "epoch": 0.0413135593220339,
      "grad_norm": 10.253814697265625,
      "learning_rate": 9.586864406779663e-06,
      "loss": 45.0262,
      "step": 78
    },
    {
      "epoch": 0.04184322033898305,
      "grad_norm": 13.549278259277344,
      "learning_rate": 9.58156779661017e-06,
      "loss": 47.7906,
      "step": 79
    },
    {
      "epoch": 0.0423728813559322,
      "grad_norm": 11.23200511932373,
      "learning_rate": 9.576271186440679e-06,
      "loss": 43.7926,
      "step": 80
    },
    {
      "epoch": 0.04290254237288135,
      "grad_norm": 13.726470947265625,
      "learning_rate": 9.570974576271187e-06,
      "loss": 44.0771,
      "step": 81
    },
    {
      "epoch": 0.04343220338983051,
      "grad_norm": 12.592391967773438,
      "learning_rate": 9.565677966101694e-06,
      "loss": 47.132,
      "step": 82
    },
    {
      "epoch": 0.04396186440677966,
      "grad_norm": 8.935877799987793,
      "learning_rate": 9.560381355932205e-06,
      "loss": 44.6114,
      "step": 83
    },
    {
      "epoch": 0.04449152542372881,
      "grad_norm": 11.312682151794434,
      "learning_rate": 9.555084745762712e-06,
      "loss": 45.9913,
      "step": 84
    },
    {
      "epoch": 0.045021186440677964,
      "grad_norm": 11.910426139831543,
      "learning_rate": 9.54978813559322e-06,
      "loss": 45.2182,
      "step": 85
    },
    {
      "epoch": 0.045550847457627115,
      "grad_norm": 14.938702583312988,
      "learning_rate": 9.54449152542373e-06,
      "loss": 46.6207,
      "step": 86
    },
    {
      "epoch": 0.04608050847457627,
      "grad_norm": 10.8577880859375,
      "learning_rate": 9.539194915254238e-06,
      "loss": 44.3205,
      "step": 87
    },
    {
      "epoch": 0.046610169491525424,
      "grad_norm": 10.709208488464355,
      "learning_rate": 9.533898305084747e-06,
      "loss": 44.0943,
      "step": 88
    },
    {
      "epoch": 0.047139830508474576,
      "grad_norm": 12.558162689208984,
      "learning_rate": 9.528601694915256e-06,
      "loss": 44.6588,
      "step": 89
    },
    {
      "epoch": 0.04766949152542373,
      "grad_norm": 11.565126419067383,
      "learning_rate": 9.523305084745763e-06,
      "loss": 44.9817,
      "step": 90
    },
    {
      "epoch": 0.048199152542372885,
      "grad_norm": 12.934249877929688,
      "learning_rate": 9.518008474576273e-06,
      "loss": 45.7716,
      "step": 91
    },
    {
      "epoch": 0.048728813559322036,
      "grad_norm": 8.9599609375,
      "learning_rate": 9.51271186440678e-06,
      "loss": 44.4895,
      "step": 92
    },
    {
      "epoch": 0.04925847457627119,
      "grad_norm": 12.892339706420898,
      "learning_rate": 9.507415254237289e-06,
      "loss": 45.6362,
      "step": 93
    },
    {
      "epoch": 0.04978813559322034,
      "grad_norm": 13.398110389709473,
      "learning_rate": 9.502118644067798e-06,
      "loss": 46.4862,
      "step": 94
    },
    {
      "epoch": 0.05031779661016949,
      "grad_norm": 13.660088539123535,
      "learning_rate": 9.496822033898306e-06,
      "loss": 46.34,
      "step": 95
    },
    {
      "epoch": 0.05084745762711865,
      "grad_norm": 16.327390670776367,
      "learning_rate": 9.491525423728815e-06,
      "loss": 48.711,
      "step": 96
    },
    {
      "epoch": 0.0513771186440678,
      "grad_norm": 10.168864250183105,
      "learning_rate": 9.486228813559322e-06,
      "loss": 44.7249,
      "step": 97
    },
    {
      "epoch": 0.05190677966101695,
      "grad_norm": 11.677518844604492,
      "learning_rate": 9.480932203389831e-06,
      "loss": 45.6954,
      "step": 98
    },
    {
      "epoch": 0.0524364406779661,
      "grad_norm": 12.626070022583008,
      "learning_rate": 9.47563559322034e-06,
      "loss": 44.8583,
      "step": 99
    },
    {
      "epoch": 0.05296610169491525,
      "grad_norm": 11.655086517333984,
      "learning_rate": 9.470338983050848e-06,
      "loss": 46.5279,
      "step": 100
    },
    {
      "epoch": 0.05349576271186441,
      "grad_norm": 13.484797477722168,
      "learning_rate": 9.465042372881357e-06,
      "loss": 45.0749,
      "step": 101
    },
    {
      "epoch": 0.05402542372881356,
      "grad_norm": 10.462074279785156,
      "learning_rate": 9.459745762711864e-06,
      "loss": 45.4183,
      "step": 102
    },
    {
      "epoch": 0.05455508474576271,
      "grad_norm": 17.580568313598633,
      "learning_rate": 9.454449152542373e-06,
      "loss": 48.3493,
      "step": 103
    },
    {
      "epoch": 0.05508474576271186,
      "grad_norm": 12.802325248718262,
      "learning_rate": 9.449152542372882e-06,
      "loss": 43.8834,
      "step": 104
    },
    {
      "epoch": 0.055614406779661014,
      "grad_norm": 14.19803524017334,
      "learning_rate": 9.44385593220339e-06,
      "loss": 46.2765,
      "step": 105
    },
    {
      "epoch": 0.05614406779661017,
      "grad_norm": 12.183131217956543,
      "learning_rate": 9.4385593220339e-06,
      "loss": 44.7768,
      "step": 106
    },
    {
      "epoch": 0.056673728813559324,
      "grad_norm": 13.098036766052246,
      "learning_rate": 9.433262711864406e-06,
      "loss": 45.3535,
      "step": 107
    },
    {
      "epoch": 0.057203389830508475,
      "grad_norm": 16.667449951171875,
      "learning_rate": 9.427966101694917e-06,
      "loss": 46.3868,
      "step": 108
    },
    {
      "epoch": 0.057733050847457626,
      "grad_norm": 11.712324142456055,
      "learning_rate": 9.422669491525424e-06,
      "loss": 43.7138,
      "step": 109
    },
    {
      "epoch": 0.05826271186440678,
      "grad_norm": 14.999502182006836,
      "learning_rate": 9.417372881355933e-06,
      "loss": 45.4052,
      "step": 110
    },
    {
      "epoch": 0.058792372881355935,
      "grad_norm": 14.092772483825684,
      "learning_rate": 9.412076271186441e-06,
      "loss": 45.6388,
      "step": 111
    },
    {
      "epoch": 0.059322033898305086,
      "grad_norm": 12.757980346679688,
      "learning_rate": 9.40677966101695e-06,
      "loss": 43.3002,
      "step": 112
    },
    {
      "epoch": 0.05985169491525424,
      "grad_norm": 14.760502815246582,
      "learning_rate": 9.401483050847459e-06,
      "loss": 45.9261,
      "step": 113
    },
    {
      "epoch": 0.06038135593220339,
      "grad_norm": 11.731225967407227,
      "learning_rate": 9.396186440677968e-06,
      "loss": 44.64,
      "step": 114
    },
    {
      "epoch": 0.06091101694915254,
      "grad_norm": 14.624043464660645,
      "learning_rate": 9.390889830508475e-06,
      "loss": 46.0926,
      "step": 115
    },
    {
      "epoch": 0.0614406779661017,
      "grad_norm": 11.877707481384277,
      "learning_rate": 9.385593220338985e-06,
      "loss": 44.3539,
      "step": 116
    },
    {
      "epoch": 0.06197033898305085,
      "grad_norm": 12.445934295654297,
      "learning_rate": 9.380296610169492e-06,
      "loss": 45.185,
      "step": 117
    },
    {
      "epoch": 0.0625,
      "grad_norm": 14.30274772644043,
      "learning_rate": 9.375000000000001e-06,
      "loss": 46.8473,
      "step": 118
    },
    {
      "epoch": 0.06302966101694915,
      "grad_norm": 12.959136009216309,
      "learning_rate": 9.36970338983051e-06,
      "loss": 43.2377,
      "step": 119
    },
    {
      "epoch": 0.0635593220338983,
      "grad_norm": 17.73836326599121,
      "learning_rate": 9.364406779661017e-06,
      "loss": 46.01,
      "step": 120
    },
    {
      "epoch": 0.06408898305084745,
      "grad_norm": 13.67056655883789,
      "learning_rate": 9.359110169491527e-06,
      "loss": 45.9014,
      "step": 121
    },
    {
      "epoch": 0.0646186440677966,
      "grad_norm": 15.143765449523926,
      "learning_rate": 9.353813559322034e-06,
      "loss": 45.6664,
      "step": 122
    },
    {
      "epoch": 0.06514830508474577,
      "grad_norm": 12.251914024353027,
      "learning_rate": 9.348516949152543e-06,
      "loss": 45.8862,
      "step": 123
    },
    {
      "epoch": 0.06567796610169492,
      "grad_norm": 11.660740852355957,
      "learning_rate": 9.343220338983052e-06,
      "loss": 43.7876,
      "step": 124
    },
    {
      "epoch": 0.06620762711864407,
      "grad_norm": 12.778264999389648,
      "learning_rate": 9.33792372881356e-06,
      "loss": 44.4956,
      "step": 125
    },
    {
      "epoch": 0.06673728813559322,
      "grad_norm": 13.168693542480469,
      "learning_rate": 9.33262711864407e-06,
      "loss": 44.3098,
      "step": 126
    },
    {
      "epoch": 0.06726694915254237,
      "grad_norm": 12.793709754943848,
      "learning_rate": 9.327330508474576e-06,
      "loss": 46.5531,
      "step": 127
    },
    {
      "epoch": 0.06779661016949153,
      "grad_norm": 17.414196014404297,
      "learning_rate": 9.322033898305085e-06,
      "loss": 46.2886,
      "step": 128
    },
    {
      "epoch": 0.06832627118644068,
      "grad_norm": 13.372808456420898,
      "learning_rate": 9.316737288135594e-06,
      "loss": 44.8482,
      "step": 129
    },
    {
      "epoch": 0.06885593220338983,
      "grad_norm": 14.688763618469238,
      "learning_rate": 9.311440677966102e-06,
      "loss": 46.9127,
      "step": 130
    },
    {
      "epoch": 0.06938559322033898,
      "grad_norm": 13.017791748046875,
      "learning_rate": 9.306144067796611e-06,
      "loss": 45.6465,
      "step": 131
    },
    {
      "epoch": 0.06991525423728813,
      "grad_norm": 18.00020980834961,
      "learning_rate": 9.300847457627118e-06,
      "loss": 51.6891,
      "step": 132
    },
    {
      "epoch": 0.0704449152542373,
      "grad_norm": 14.6775541305542,
      "learning_rate": 9.295550847457629e-06,
      "loss": 43.979,
      "step": 133
    },
    {
      "epoch": 0.07097457627118645,
      "grad_norm": 14.837903022766113,
      "learning_rate": 9.290254237288136e-06,
      "loss": 46.4973,
      "step": 134
    },
    {
      "epoch": 0.0715042372881356,
      "grad_norm": 13.911921501159668,
      "learning_rate": 9.284957627118645e-06,
      "loss": 44.6305,
      "step": 135
    },
    {
      "epoch": 0.07203389830508475,
      "grad_norm": 14.227741241455078,
      "learning_rate": 9.279661016949153e-06,
      "loss": 44.9326,
      "step": 136
    },
    {
      "epoch": 0.0725635593220339,
      "grad_norm": 13.136384963989258,
      "learning_rate": 9.274364406779662e-06,
      "loss": 44.2295,
      "step": 137
    },
    {
      "epoch": 0.07309322033898305,
      "grad_norm": 12.225574493408203,
      "learning_rate": 9.26906779661017e-06,
      "loss": 45.3111,
      "step": 138
    },
    {
      "epoch": 0.0736228813559322,
      "grad_norm": 14.368593215942383,
      "learning_rate": 9.26377118644068e-06,
      "loss": 43.9123,
      "step": 139
    },
    {
      "epoch": 0.07415254237288135,
      "grad_norm": 13.189476013183594,
      "learning_rate": 9.258474576271187e-06,
      "loss": 44.086,
      "step": 140
    },
    {
      "epoch": 0.0746822033898305,
      "grad_norm": 13.54965877532959,
      "learning_rate": 9.253177966101695e-06,
      "loss": 41.8549,
      "step": 141
    },
    {
      "epoch": 0.07521186440677965,
      "grad_norm": 15.71441650390625,
      "learning_rate": 9.247881355932204e-06,
      "loss": 44.6146,
      "step": 142
    },
    {
      "epoch": 0.07574152542372882,
      "grad_norm": 14.36960506439209,
      "learning_rate": 9.242584745762713e-06,
      "loss": 45.6127,
      "step": 143
    },
    {
      "epoch": 0.07627118644067797,
      "grad_norm": 15.577178955078125,
      "learning_rate": 9.237288135593222e-06,
      "loss": 42.1889,
      "step": 144
    },
    {
      "epoch": 0.07680084745762712,
      "grad_norm": 19.175662994384766,
      "learning_rate": 9.231991525423729e-06,
      "loss": 45.8298,
      "step": 145
    },
    {
      "epoch": 0.07733050847457627,
      "grad_norm": 15.818450927734375,
      "learning_rate": 9.226694915254239e-06,
      "loss": 44.479,
      "step": 146
    },
    {
      "epoch": 0.07786016949152542,
      "grad_norm": 16.356101989746094,
      "learning_rate": 9.221398305084746e-06,
      "loss": 44.9392,
      "step": 147
    },
    {
      "epoch": 0.07838983050847458,
      "grad_norm": 17.910112380981445,
      "learning_rate": 9.216101694915255e-06,
      "loss": 44.5173,
      "step": 148
    },
    {
      "epoch": 0.07891949152542373,
      "grad_norm": 11.76662826538086,
      "learning_rate": 9.210805084745764e-06,
      "loss": 42.7223,
      "step": 149
    },
    {
      "epoch": 0.07944915254237288,
      "grad_norm": 15.474702835083008,
      "learning_rate": 9.205508474576272e-06,
      "loss": 46.411,
      "step": 150
    },
    {
      "epoch": 0.07997881355932203,
      "grad_norm": 12.965540885925293,
      "learning_rate": 9.200211864406781e-06,
      "loss": 43.8627,
      "step": 151
    },
    {
      "epoch": 0.08050847457627118,
      "grad_norm": 16.641456604003906,
      "learning_rate": 9.194915254237288e-06,
      "loss": 44.25,
      "step": 152
    },
    {
      "epoch": 0.08103813559322035,
      "grad_norm": 14.435934066772461,
      "learning_rate": 9.189618644067797e-06,
      "loss": 43.3545,
      "step": 153
    },
    {
      "epoch": 0.0815677966101695,
      "grad_norm": 15.837759017944336,
      "learning_rate": 9.184322033898306e-06,
      "loss": 44.3124,
      "step": 154
    },
    {
      "epoch": 0.08209745762711865,
      "grad_norm": 16.089370727539062,
      "learning_rate": 9.179025423728814e-06,
      "loss": 45.6744,
      "step": 155
    },
    {
      "epoch": 0.0826271186440678,
      "grad_norm": 14.413983345031738,
      "learning_rate": 9.173728813559323e-06,
      "loss": 44.0566,
      "step": 156
    },
    {
      "epoch": 0.08315677966101695,
      "grad_norm": 12.107999801635742,
      "learning_rate": 9.16843220338983e-06,
      "loss": 43.1977,
      "step": 157
    },
    {
      "epoch": 0.0836864406779661,
      "grad_norm": 14.233626365661621,
      "learning_rate": 9.163135593220339e-06,
      "loss": 43.8167,
      "step": 158
    },
    {
      "epoch": 0.08421610169491525,
      "grad_norm": 12.994474411010742,
      "learning_rate": 9.157838983050848e-06,
      "loss": 43.649,
      "step": 159
    },
    {
      "epoch": 0.0847457627118644,
      "grad_norm": 14.202582359313965,
      "learning_rate": 9.152542372881356e-06,
      "loss": 42.5304,
      "step": 160
    },
    {
      "epoch": 0.08527542372881355,
      "grad_norm": 13.463746070861816,
      "learning_rate": 9.147245762711865e-06,
      "loss": 45.5199,
      "step": 161
    },
    {
      "epoch": 0.0858050847457627,
      "grad_norm": 15.107317924499512,
      "learning_rate": 9.141949152542374e-06,
      "loss": 44.0625,
      "step": 162
    },
    {
      "epoch": 0.08633474576271187,
      "grad_norm": 13.02063274383545,
      "learning_rate": 9.136652542372883e-06,
      "loss": 44.7377,
      "step": 163
    },
    {
      "epoch": 0.08686440677966102,
      "grad_norm": 15.182476043701172,
      "learning_rate": 9.131355932203391e-06,
      "loss": 44.689,
      "step": 164
    },
    {
      "epoch": 0.08739406779661017,
      "grad_norm": 14.193438529968262,
      "learning_rate": 9.126059322033898e-06,
      "loss": 43.9248,
      "step": 165
    },
    {
      "epoch": 0.08792372881355932,
      "grad_norm": 12.928723335266113,
      "learning_rate": 9.120762711864407e-06,
      "loss": 42.0982,
      "step": 166
    },
    {
      "epoch": 0.08845338983050847,
      "grad_norm": 17.051420211791992,
      "learning_rate": 9.115466101694916e-06,
      "loss": 45.8974,
      "step": 167
    },
    {
      "epoch": 0.08898305084745763,
      "grad_norm": 14.144514083862305,
      "learning_rate": 9.110169491525425e-06,
      "loss": 43.093,
      "step": 168
    },
    {
      "epoch": 0.08951271186440678,
      "grad_norm": 15.755730628967285,
      "learning_rate": 9.104872881355933e-06,
      "loss": 46.8026,
      "step": 169
    },
    {
      "epoch": 0.09004237288135593,
      "grad_norm": 15.752670288085938,
      "learning_rate": 9.09957627118644e-06,
      "loss": 42.942,
      "step": 170
    },
    {
      "epoch": 0.09057203389830508,
      "grad_norm": 17.557292938232422,
      "learning_rate": 9.094279661016951e-06,
      "loss": 45.1299,
      "step": 171
    },
    {
      "epoch": 0.09110169491525423,
      "grad_norm": 17.129390716552734,
      "learning_rate": 9.088983050847458e-06,
      "loss": 44.5567,
      "step": 172
    },
    {
      "epoch": 0.0916313559322034,
      "grad_norm": 13.517969131469727,
      "learning_rate": 9.083686440677967e-06,
      "loss": 45.0957,
      "step": 173
    },
    {
      "epoch": 0.09216101694915255,
      "grad_norm": 20.450660705566406,
      "learning_rate": 9.078389830508476e-06,
      "loss": 44.2672,
      "step": 174
    },
    {
      "epoch": 0.0926906779661017,
      "grad_norm": 15.442261695861816,
      "learning_rate": 9.073093220338984e-06,
      "loss": 44.9288,
      "step": 175
    },
    {
      "epoch": 0.09322033898305085,
      "grad_norm": 18.71314811706543,
      "learning_rate": 9.067796610169493e-06,
      "loss": 44.1856,
      "step": 176
    },
    {
      "epoch": 0.09375,
      "grad_norm": 14.060361862182617,
      "learning_rate": 9.0625e-06,
      "loss": 43.5869,
      "step": 177
    },
    {
      "epoch": 0.09427966101694915,
      "grad_norm": 16.31285285949707,
      "learning_rate": 9.057203389830509e-06,
      "loss": 42.7866,
      "step": 178
    },
    {
      "epoch": 0.0948093220338983,
      "grad_norm": 15.066601753234863,
      "learning_rate": 9.051906779661018e-06,
      "loss": 44.7566,
      "step": 179
    },
    {
      "epoch": 0.09533898305084745,
      "grad_norm": 13.873777389526367,
      "learning_rate": 9.046610169491526e-06,
      "loss": 43.5197,
      "step": 180
    },
    {
      "epoch": 0.0958686440677966,
      "grad_norm": 16.273666381835938,
      "learning_rate": 9.041313559322035e-06,
      "loss": 41.174,
      "step": 181
    },
    {
      "epoch": 0.09639830508474577,
      "grad_norm": 16.049724578857422,
      "learning_rate": 9.036016949152542e-06,
      "loss": 42.0004,
      "step": 182
    },
    {
      "epoch": 0.09692796610169492,
      "grad_norm": 19.54194450378418,
      "learning_rate": 9.03072033898305e-06,
      "loss": 45.6732,
      "step": 183
    },
    {
      "epoch": 0.09745762711864407,
      "grad_norm": 18.0690860748291,
      "learning_rate": 9.02542372881356e-06,
      "loss": 44.5665,
      "step": 184
    },
    {
      "epoch": 0.09798728813559322,
      "grad_norm": 14.092813491821289,
      "learning_rate": 9.020127118644068e-06,
      "loss": 44.908,
      "step": 185
    },
    {
      "epoch": 0.09851694915254237,
      "grad_norm": 15.447066307067871,
      "learning_rate": 9.014830508474577e-06,
      "loss": 42.7003,
      "step": 186
    },
    {
      "epoch": 0.09904661016949153,
      "grad_norm": 16.527538299560547,
      "learning_rate": 9.009533898305086e-06,
      "loss": 45.2748,
      "step": 187
    },
    {
      "epoch": 0.09957627118644068,
      "grad_norm": 18.911373138427734,
      "learning_rate": 9.004237288135595e-06,
      "loss": 45.126,
      "step": 188
    },
    {
      "epoch": 0.10010593220338983,
      "grad_norm": 15.269401550292969,
      "learning_rate": 8.998940677966103e-06,
      "loss": 43.053,
      "step": 189
    },
    {
      "epoch": 0.10063559322033898,
      "grad_norm": 14.951906204223633,
      "learning_rate": 8.99364406779661e-06,
      "loss": 42.9235,
      "step": 190
    },
    {
      "epoch": 0.10116525423728813,
      "grad_norm": 14.439262390136719,
      "learning_rate": 8.988347457627119e-06,
      "loss": 43.185,
      "step": 191
    },
    {
      "epoch": 0.1016949152542373,
      "grad_norm": 22.62448501586914,
      "learning_rate": 8.983050847457628e-06,
      "loss": 44.5611,
      "step": 192
    },
    {
      "epoch": 0.10222457627118645,
      "grad_norm": 15.886744499206543,
      "learning_rate": 8.977754237288137e-06,
      "loss": 43.0422,
      "step": 193
    },
    {
      "epoch": 0.1027542372881356,
      "grad_norm": 16.342416763305664,
      "learning_rate": 8.972457627118645e-06,
      "loss": 43.4564,
      "step": 194
    },
    {
      "epoch": 0.10328389830508475,
      "grad_norm": 14.021316528320312,
      "learning_rate": 8.967161016949152e-06,
      "loss": 41.3918,
      "step": 195
    },
    {
      "epoch": 0.1038135593220339,
      "grad_norm": 15.82027530670166,
      "learning_rate": 8.961864406779663e-06,
      "loss": 42.5995,
      "step": 196
    },
    {
      "epoch": 0.10434322033898305,
      "grad_norm": 15.34825325012207,
      "learning_rate": 8.95656779661017e-06,
      "loss": 43.771,
      "step": 197
    },
    {
      "epoch": 0.1048728813559322,
      "grad_norm": 15.0744047164917,
      "learning_rate": 8.951271186440679e-06,
      "loss": 42.5319,
      "step": 198
    },
    {
      "epoch": 0.10540254237288135,
      "grad_norm": 17.18386459350586,
      "learning_rate": 8.945974576271187e-06,
      "loss": 43.7582,
      "step": 199
    },
    {
      "epoch": 0.1059322033898305,
      "grad_norm": 15.976750373840332,
      "learning_rate": 8.940677966101694e-06,
      "loss": 43.1739,
      "step": 200
    },
    {
      "epoch": 0.10646186440677965,
      "grad_norm": 13.061029434204102,
      "learning_rate": 8.935381355932205e-06,
      "loss": 41.4326,
      "step": 201
    },
    {
      "epoch": 0.10699152542372882,
      "grad_norm": 13.137149810791016,
      "learning_rate": 8.930084745762712e-06,
      "loss": 40.966,
      "step": 202
    },
    {
      "epoch": 0.10752118644067797,
      "grad_norm": 12.398611068725586,
      "learning_rate": 8.92478813559322e-06,
      "loss": 41.9912,
      "step": 203
    },
    {
      "epoch": 0.10805084745762712,
      "grad_norm": 14.261005401611328,
      "learning_rate": 8.91949152542373e-06,
      "loss": 39.9012,
      "step": 204
    },
    {
      "epoch": 0.10858050847457627,
      "grad_norm": 15.67331600189209,
      "learning_rate": 8.914194915254238e-06,
      "loss": 42.8413,
      "step": 205
    },
    {
      "epoch": 0.10911016949152542,
      "grad_norm": 16.921180725097656,
      "learning_rate": 8.908898305084747e-06,
      "loss": 43.4836,
      "step": 206
    },
    {
      "epoch": 0.10963983050847458,
      "grad_norm": 15.534255981445312,
      "learning_rate": 8.903601694915254e-06,
      "loss": 41.3274,
      "step": 207
    },
    {
      "epoch": 0.11016949152542373,
      "grad_norm": 16.455135345458984,
      "learning_rate": 8.898305084745763e-06,
      "loss": 44.1427,
      "step": 208
    },
    {
      "epoch": 0.11069915254237288,
      "grad_norm": 14.592940330505371,
      "learning_rate": 8.893008474576273e-06,
      "loss": 42.9283,
      "step": 209
    },
    {
      "epoch": 0.11122881355932203,
      "grad_norm": 25.936908721923828,
      "learning_rate": 8.88771186440678e-06,
      "loss": 44.6859,
      "step": 210
    },
    {
      "epoch": 0.11175847457627118,
      "grad_norm": 15.597710609436035,
      "learning_rate": 8.882415254237289e-06,
      "loss": 43.3808,
      "step": 211
    },
    {
      "epoch": 0.11228813559322035,
      "grad_norm": 11.907612800598145,
      "learning_rate": 8.877118644067798e-06,
      "loss": 39.6498,
      "step": 212
    },
    {
      "epoch": 0.1128177966101695,
      "grad_norm": 15.054706573486328,
      "learning_rate": 8.871822033898307e-06,
      "loss": 41.3892,
      "step": 213
    },
    {
      "epoch": 0.11334745762711865,
      "grad_norm": 16.06768226623535,
      "learning_rate": 8.866525423728815e-06,
      "loss": 42.1037,
      "step": 214
    },
    {
      "epoch": 0.1138771186440678,
      "grad_norm": 16.24547576904297,
      "learning_rate": 8.861228813559322e-06,
      "loss": 43.2085,
      "step": 215
    },
    {
      "epoch": 0.11440677966101695,
      "grad_norm": 13.186929702758789,
      "learning_rate": 8.855932203389831e-06,
      "loss": 41.0925,
      "step": 216
    },
    {
      "epoch": 0.1149364406779661,
      "grad_norm": 15.12317180633545,
      "learning_rate": 8.85063559322034e-06,
      "loss": 42.2887,
      "step": 217
    },
    {
      "epoch": 0.11546610169491525,
      "grad_norm": 18.256319046020508,
      "learning_rate": 8.845338983050849e-06,
      "loss": 43.8898,
      "step": 218
    },
    {
      "epoch": 0.1159957627118644,
      "grad_norm": 19.571659088134766,
      "learning_rate": 8.840042372881357e-06,
      "loss": 43.4883,
      "step": 219
    },
    {
      "epoch": 0.11652542372881355,
      "grad_norm": 17.962724685668945,
      "learning_rate": 8.834745762711864e-06,
      "loss": 42.0768,
      "step": 220
    },
    {
      "epoch": 0.1170550847457627,
      "grad_norm": 14.048063278198242,
      "learning_rate": 8.829449152542373e-06,
      "loss": 41.6314,
      "step": 221
    },
    {
      "epoch": 0.11758474576271187,
      "grad_norm": 18.41710662841797,
      "learning_rate": 8.824152542372882e-06,
      "loss": 42.9001,
      "step": 222
    },
    {
      "epoch": 0.11811440677966102,
      "grad_norm": 13.716641426086426,
      "learning_rate": 8.81885593220339e-06,
      "loss": 40.0575,
      "step": 223
    },
    {
      "epoch": 0.11864406779661017,
      "grad_norm": 15.040282249450684,
      "learning_rate": 8.8135593220339e-06,
      "loss": 40.8923,
      "step": 224
    },
    {
      "epoch": 0.11917372881355932,
      "grad_norm": 16.52912139892578,
      "learning_rate": 8.808262711864406e-06,
      "loss": 42.1481,
      "step": 225
    },
    {
      "epoch": 0.11970338983050847,
      "grad_norm": 13.509419441223145,
      "learning_rate": 8.802966101694917e-06,
      "loss": 39.7799,
      "step": 226
    },
    {
      "epoch": 0.12023305084745763,
      "grad_norm": 17.121986389160156,
      "learning_rate": 8.797669491525424e-06,
      "loss": 42.044,
      "step": 227
    },
    {
      "epoch": 0.12076271186440678,
      "grad_norm": 15.027372360229492,
      "learning_rate": 8.792372881355933e-06,
      "loss": 39.8918,
      "step": 228
    },
    {
      "epoch": 0.12129237288135593,
      "grad_norm": 14.092366218566895,
      "learning_rate": 8.787076271186441e-06,
      "loss": 42.3522,
      "step": 229
    },
    {
      "epoch": 0.12182203389830508,
      "grad_norm": 24.92057991027832,
      "learning_rate": 8.78177966101695e-06,
      "loss": 43.036,
      "step": 230
    },
    {
      "epoch": 0.12235169491525423,
      "grad_norm": 15.43857479095459,
      "learning_rate": 8.776483050847459e-06,
      "loss": 43.7397,
      "step": 231
    },
    {
      "epoch": 0.1228813559322034,
      "grad_norm": 19.818382263183594,
      "learning_rate": 8.771186440677966e-06,
      "loss": 43.5315,
      "step": 232
    },
    {
      "epoch": 0.12341101694915255,
      "grad_norm": 14.703324317932129,
      "learning_rate": 8.765889830508475e-06,
      "loss": 41.1778,
      "step": 233
    },
    {
      "epoch": 0.1239406779661017,
      "grad_norm": 14.151469230651855,
      "learning_rate": 8.760593220338985e-06,
      "loss": 39.993,
      "step": 234
    },
    {
      "epoch": 0.12447033898305085,
      "grad_norm": 12.474395751953125,
      "learning_rate": 8.755296610169492e-06,
      "loss": 39.9198,
      "step": 235
    },
    {
      "epoch": 0.125,
      "grad_norm": 16.170940399169922,
      "learning_rate": 8.750000000000001e-06,
      "loss": 44.5931,
      "step": 236
    },
    {
      "epoch": 0.12552966101694915,
      "grad_norm": 14.413511276245117,
      "learning_rate": 8.74470338983051e-06,
      "loss": 41.0592,
      "step": 237
    },
    {
      "epoch": 0.1260593220338983,
      "grad_norm": 14.989477157592773,
      "learning_rate": 8.739406779661017e-06,
      "loss": 40.9657,
      "step": 238
    },
    {
      "epoch": 0.12658898305084745,
      "grad_norm": 16.371431350708008,
      "learning_rate": 8.734110169491527e-06,
      "loss": 41.3418,
      "step": 239
    },
    {
      "epoch": 0.1271186440677966,
      "grad_norm": 14.802279472351074,
      "learning_rate": 8.728813559322034e-06,
      "loss": 42.5882,
      "step": 240
    },
    {
      "epoch": 0.12764830508474576,
      "grad_norm": 20.02827262878418,
      "learning_rate": 8.723516949152543e-06,
      "loss": 42.8217,
      "step": 241
    },
    {
      "epoch": 0.1281779661016949,
      "grad_norm": 11.373907089233398,
      "learning_rate": 8.718220338983052e-06,
      "loss": 39.8341,
      "step": 242
    },
    {
      "epoch": 0.12870762711864406,
      "grad_norm": 14.20452880859375,
      "learning_rate": 8.71292372881356e-06,
      "loss": 40.0912,
      "step": 243
    },
    {
      "epoch": 0.1292372881355932,
      "grad_norm": 14.933534622192383,
      "learning_rate": 8.70762711864407e-06,
      "loss": 40.9392,
      "step": 244
    },
    {
      "epoch": 0.12976694915254236,
      "grad_norm": 14.053047180175781,
      "learning_rate": 8.702330508474576e-06,
      "loss": 39.7796,
      "step": 245
    },
    {
      "epoch": 0.13029661016949154,
      "grad_norm": 17.01775550842285,
      "learning_rate": 8.697033898305085e-06,
      "loss": 41.7197,
      "step": 246
    },
    {
      "epoch": 0.1308262711864407,
      "grad_norm": 18.08636474609375,
      "learning_rate": 8.691737288135594e-06,
      "loss": 42.9922,
      "step": 247
    },
    {
      "epoch": 0.13135593220338984,
      "grad_norm": 17.077959060668945,
      "learning_rate": 8.686440677966103e-06,
      "loss": 40.7556,
      "step": 248
    },
    {
      "epoch": 0.131885593220339,
      "grad_norm": 15.379338264465332,
      "learning_rate": 8.681144067796611e-06,
      "loss": 40.314,
      "step": 249
    },
    {
      "epoch": 0.13241525423728814,
      "grad_norm": 15.305740356445312,
      "learning_rate": 8.675847457627118e-06,
      "loss": 41.3682,
      "step": 250
    },
    {
      "epoch": 0.1329449152542373,
      "grad_norm": 17.093536376953125,
      "learning_rate": 8.670550847457629e-06,
      "loss": 41.907,
      "step": 251
    },
    {
      "epoch": 0.13347457627118645,
      "grad_norm": 16.537490844726562,
      "learning_rate": 8.665254237288136e-06,
      "loss": 41.7812,
      "step": 252
    },
    {
      "epoch": 0.1340042372881356,
      "grad_norm": 16.04497528076172,
      "learning_rate": 8.659957627118645e-06,
      "loss": 40.169,
      "step": 253
    },
    {
      "epoch": 0.13453389830508475,
      "grad_norm": 19.32991600036621,
      "learning_rate": 8.654661016949153e-06,
      "loss": 42.1785,
      "step": 254
    },
    {
      "epoch": 0.1350635593220339,
      "grad_norm": 17.907474517822266,
      "learning_rate": 8.649364406779662e-06,
      "loss": 43.8972,
      "step": 255
    },
    {
      "epoch": 0.13559322033898305,
      "grad_norm": 17.56587028503418,
      "learning_rate": 8.64406779661017e-06,
      "loss": 39.6362,
      "step": 256
    },
    {
      "epoch": 0.1361228813559322,
      "grad_norm": 17.579334259033203,
      "learning_rate": 8.638771186440678e-06,
      "loss": 42.3221,
      "step": 257
    },
    {
      "epoch": 0.13665254237288135,
      "grad_norm": 13.46346664428711,
      "learning_rate": 8.633474576271187e-06,
      "loss": 40.4565,
      "step": 258
    },
    {
      "epoch": 0.1371822033898305,
      "grad_norm": 15.181448936462402,
      "learning_rate": 8.628177966101695e-06,
      "loss": 40.8789,
      "step": 259
    },
    {
      "epoch": 0.13771186440677965,
      "grad_norm": 20.460641860961914,
      "learning_rate": 8.622881355932204e-06,
      "loss": 42.2112,
      "step": 260
    },
    {
      "epoch": 0.1382415254237288,
      "grad_norm": 14.17595386505127,
      "learning_rate": 8.617584745762713e-06,
      "loss": 39.8262,
      "step": 261
    },
    {
      "epoch": 0.13877118644067796,
      "grad_norm": 17.116777420043945,
      "learning_rate": 8.612288135593222e-06,
      "loss": 40.5341,
      "step": 262
    },
    {
      "epoch": 0.1393008474576271,
      "grad_norm": 16.0313720703125,
      "learning_rate": 8.606991525423729e-06,
      "loss": 40.3477,
      "step": 263
    },
    {
      "epoch": 0.13983050847457626,
      "grad_norm": 14.411069869995117,
      "learning_rate": 8.601694915254239e-06,
      "loss": 41.7851,
      "step": 264
    },
    {
      "epoch": 0.1403601694915254,
      "grad_norm": 14.982532501220703,
      "learning_rate": 8.596398305084746e-06,
      "loss": 38.2329,
      "step": 265
    },
    {
      "epoch": 0.1408898305084746,
      "grad_norm": 18.28425407409668,
      "learning_rate": 8.591101694915255e-06,
      "loss": 42.4722,
      "step": 266
    },
    {
      "epoch": 0.14141949152542374,
      "grad_norm": 12.853660583496094,
      "learning_rate": 8.585805084745764e-06,
      "loss": 39.2305,
      "step": 267
    },
    {
      "epoch": 0.1419491525423729,
      "grad_norm": 21.43488121032715,
      "learning_rate": 8.580508474576272e-06,
      "loss": 42.7929,
      "step": 268
    },
    {
      "epoch": 0.14247881355932204,
      "grad_norm": 16.01351547241211,
      "learning_rate": 8.575211864406781e-06,
      "loss": 42.2796,
      "step": 269
    },
    {
      "epoch": 0.1430084745762712,
      "grad_norm": 14.995550155639648,
      "learning_rate": 8.569915254237288e-06,
      "loss": 40.5965,
      "step": 270
    },
    {
      "epoch": 0.14353813559322035,
      "grad_norm": 15.351802825927734,
      "learning_rate": 8.564618644067797e-06,
      "loss": 39.1034,
      "step": 271
    },
    {
      "epoch": 0.1440677966101695,
      "grad_norm": 14.207134246826172,
      "learning_rate": 8.559322033898306e-06,
      "loss": 39.0523,
      "step": 272
    },
    {
      "epoch": 0.14459745762711865,
      "grad_norm": 18.96088981628418,
      "learning_rate": 8.554025423728814e-06,
      "loss": 40.9693,
      "step": 273
    },
    {
      "epoch": 0.1451271186440678,
      "grad_norm": 16.84577178955078,
      "learning_rate": 8.548728813559323e-06,
      "loss": 40.266,
      "step": 274
    },
    {
      "epoch": 0.14565677966101695,
      "grad_norm": 18.967397689819336,
      "learning_rate": 8.54343220338983e-06,
      "loss": 40.45,
      "step": 275
    },
    {
      "epoch": 0.1461864406779661,
      "grad_norm": 15.440354347229004,
      "learning_rate": 8.538135593220339e-06,
      "loss": 41.4072,
      "step": 276
    },
    {
      "epoch": 0.14671610169491525,
      "grad_norm": 15.704645156860352,
      "learning_rate": 8.532838983050848e-06,
      "loss": 40.4637,
      "step": 277
    },
    {
      "epoch": 0.1472457627118644,
      "grad_norm": 15.307123184204102,
      "learning_rate": 8.527542372881356e-06,
      "loss": 39.0186,
      "step": 278
    },
    {
      "epoch": 0.14777542372881355,
      "grad_norm": 12.578743934631348,
      "learning_rate": 8.522245762711865e-06,
      "loss": 40.2655,
      "step": 279
    },
    {
      "epoch": 0.1483050847457627,
      "grad_norm": 16.67361068725586,
      "learning_rate": 8.516949152542372e-06,
      "loss": 40.8645,
      "step": 280
    },
    {
      "epoch": 0.14883474576271186,
      "grad_norm": 17.23623275756836,
      "learning_rate": 8.511652542372883e-06,
      "loss": 41.1118,
      "step": 281
    },
    {
      "epoch": 0.149364406779661,
      "grad_norm": 13.876092910766602,
      "learning_rate": 8.50635593220339e-06,
      "loss": 38.5299,
      "step": 282
    },
    {
      "epoch": 0.14989406779661016,
      "grad_norm": 17.075061798095703,
      "learning_rate": 8.501059322033899e-06,
      "loss": 40.8503,
      "step": 283
    },
    {
      "epoch": 0.1504237288135593,
      "grad_norm": 16.618202209472656,
      "learning_rate": 8.495762711864407e-06,
      "loss": 38.7768,
      "step": 284
    },
    {
      "epoch": 0.15095338983050846,
      "grad_norm": 20.09058380126953,
      "learning_rate": 8.490466101694916e-06,
      "loss": 43.2695,
      "step": 285
    },
    {
      "epoch": 0.15148305084745764,
      "grad_norm": 16.622262954711914,
      "learning_rate": 8.485169491525425e-06,
      "loss": 38.7524,
      "step": 286
    },
    {
      "epoch": 0.1520127118644068,
      "grad_norm": 20.795183181762695,
      "learning_rate": 8.479872881355934e-06,
      "loss": 42.7362,
      "step": 287
    },
    {
      "epoch": 0.15254237288135594,
      "grad_norm": 15.230008125305176,
      "learning_rate": 8.47457627118644e-06,
      "loss": 38.6891,
      "step": 288
    },
    {
      "epoch": 0.1530720338983051,
      "grad_norm": 17.610427856445312,
      "learning_rate": 8.469279661016951e-06,
      "loss": 39.1889,
      "step": 289
    },
    {
      "epoch": 0.15360169491525424,
      "grad_norm": 17.195369720458984,
      "learning_rate": 8.463983050847458e-06,
      "loss": 38.4673,
      "step": 290
    },
    {
      "epoch": 0.1541313559322034,
      "grad_norm": 14.823731422424316,
      "learning_rate": 8.458686440677967e-06,
      "loss": 41.5085,
      "step": 291
    },
    {
      "epoch": 0.15466101694915255,
      "grad_norm": 16.83893585205078,
      "learning_rate": 8.453389830508476e-06,
      "loss": 40.0534,
      "step": 292
    },
    {
      "epoch": 0.1551906779661017,
      "grad_norm": 16.137813568115234,
      "learning_rate": 8.448093220338984e-06,
      "loss": 41.3277,
      "step": 293
    },
    {
      "epoch": 0.15572033898305085,
      "grad_norm": 17.45763397216797,
      "learning_rate": 8.442796610169493e-06,
      "loss": 40.7325,
      "step": 294
    },
    {
      "epoch": 0.15625,
      "grad_norm": 13.673721313476562,
      "learning_rate": 8.4375e-06,
      "loss": 40.7895,
      "step": 295
    },
    {
      "epoch": 0.15677966101694915,
      "grad_norm": 18.841793060302734,
      "learning_rate": 8.432203389830509e-06,
      "loss": 39.9472,
      "step": 296
    },
    {
      "epoch": 0.1573093220338983,
      "grad_norm": 14.98412799835205,
      "learning_rate": 8.426906779661018e-06,
      "loss": 41.7587,
      "step": 297
    },
    {
      "epoch": 0.15783898305084745,
      "grad_norm": 13.28298568725586,
      "learning_rate": 8.421610169491526e-06,
      "loss": 39.1576,
      "step": 298
    },
    {
      "epoch": 0.1583686440677966,
      "grad_norm": 14.122819900512695,
      "learning_rate": 8.416313559322035e-06,
      "loss": 38.6992,
      "step": 299
    },
    {
      "epoch": 0.15889830508474576,
      "grad_norm": 15.145340919494629,
      "learning_rate": 8.411016949152542e-06,
      "loss": 40.0498,
      "step": 300
    },
    {
      "epoch": 0.1594279661016949,
      "grad_norm": 19.84711265563965,
      "learning_rate": 8.405720338983051e-06,
      "loss": 40.457,
      "step": 301
    },
    {
      "epoch": 0.15995762711864406,
      "grad_norm": 13.758794784545898,
      "learning_rate": 8.40042372881356e-06,
      "loss": 40.1351,
      "step": 302
    },
    {
      "epoch": 0.1604872881355932,
      "grad_norm": 13.423484802246094,
      "learning_rate": 8.395127118644068e-06,
      "loss": 38.8103,
      "step": 303
    },
    {
      "epoch": 0.16101694915254236,
      "grad_norm": 15.985000610351562,
      "learning_rate": 8.389830508474577e-06,
      "loss": 40.3776,
      "step": 304
    },
    {
      "epoch": 0.16154661016949154,
      "grad_norm": 19.32046127319336,
      "learning_rate": 8.384533898305084e-06,
      "loss": 39.9468,
      "step": 305
    },
    {
      "epoch": 0.1620762711864407,
      "grad_norm": 15.089706420898438,
      "learning_rate": 8.379237288135595e-06,
      "loss": 38.7383,
      "step": 306
    },
    {
      "epoch": 0.16260593220338984,
      "grad_norm": 17.46820831298828,
      "learning_rate": 8.373940677966103e-06,
      "loss": 39.6276,
      "step": 307
    },
    {
      "epoch": 0.163135593220339,
      "grad_norm": 18.25245475769043,
      "learning_rate": 8.36864406779661e-06,
      "loss": 39.8369,
      "step": 308
    },
    {
      "epoch": 0.16366525423728814,
      "grad_norm": 18.659257888793945,
      "learning_rate": 8.36334745762712e-06,
      "loss": 41.0589,
      "step": 309
    },
    {
      "epoch": 0.1641949152542373,
      "grad_norm": 18.425472259521484,
      "learning_rate": 8.358050847457628e-06,
      "loss": 40.1189,
      "step": 310
    },
    {
      "epoch": 0.16472457627118645,
      "grad_norm": 12.341487884521484,
      "learning_rate": 8.352754237288137e-06,
      "loss": 36.8067,
      "step": 311
    },
    {
      "epoch": 0.1652542372881356,
      "grad_norm": 17.14761734008789,
      "learning_rate": 8.347457627118645e-06,
      "loss": 39.2866,
      "step": 312
    },
    {
      "epoch": 0.16578389830508475,
      "grad_norm": 17.003276824951172,
      "learning_rate": 8.342161016949152e-06,
      "loss": 39.306,
      "step": 313
    },
    {
      "epoch": 0.1663135593220339,
      "grad_norm": 13.784782409667969,
      "learning_rate": 8.336864406779663e-06,
      "loss": 39.112,
      "step": 314
    },
    {
      "epoch": 0.16684322033898305,
      "grad_norm": 20.50088882446289,
      "learning_rate": 8.33156779661017e-06,
      "loss": 40.6695,
      "step": 315
    },
    {
      "epoch": 0.1673728813559322,
      "grad_norm": 16.98780059814453,
      "learning_rate": 8.326271186440679e-06,
      "loss": 38.9496,
      "step": 316
    },
    {
      "epoch": 0.16790254237288135,
      "grad_norm": 17.2318058013916,
      "learning_rate": 8.320974576271187e-06,
      "loss": 38.484,
      "step": 317
    },
    {
      "epoch": 0.1684322033898305,
      "grad_norm": 16.785367965698242,
      "learning_rate": 8.315677966101695e-06,
      "loss": 39.0766,
      "step": 318
    },
    {
      "epoch": 0.16896186440677965,
      "grad_norm": 18.396791458129883,
      "learning_rate": 8.310381355932205e-06,
      "loss": 40.2516,
      "step": 319
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 15.683804512023926,
      "learning_rate": 8.305084745762712e-06,
      "loss": 39.1094,
      "step": 320
    },
    {
      "epoch": 0.17002118644067796,
      "grad_norm": 14.648982048034668,
      "learning_rate": 8.29978813559322e-06,
      "loss": 37.7653,
      "step": 321
    },
    {
      "epoch": 0.1705508474576271,
      "grad_norm": 16.9040584564209,
      "learning_rate": 8.29449152542373e-06,
      "loss": 40.0507,
      "step": 322
    },
    {
      "epoch": 0.17108050847457626,
      "grad_norm": 15.456400871276855,
      "learning_rate": 8.289194915254238e-06,
      "loss": 38.262,
      "step": 323
    },
    {
      "epoch": 0.1716101694915254,
      "grad_norm": 10.87707805633545,
      "learning_rate": 8.283898305084747e-06,
      "loss": 37.0066,
      "step": 324
    },
    {
      "epoch": 0.1721398305084746,
      "grad_norm": 14.249225616455078,
      "learning_rate": 8.278601694915254e-06,
      "loss": 38.8624,
      "step": 325
    },
    {
      "epoch": 0.17266949152542374,
      "grad_norm": 15.313895225524902,
      "learning_rate": 8.273305084745763e-06,
      "loss": 37.758,
      "step": 326
    },
    {
      "epoch": 0.1731991525423729,
      "grad_norm": 16.406631469726562,
      "learning_rate": 8.268008474576272e-06,
      "loss": 38.9996,
      "step": 327
    },
    {
      "epoch": 0.17372881355932204,
      "grad_norm": 15.323809623718262,
      "learning_rate": 8.26271186440678e-06,
      "loss": 36.711,
      "step": 328
    },
    {
      "epoch": 0.1742584745762712,
      "grad_norm": 14.668671607971191,
      "learning_rate": 8.257415254237289e-06,
      "loss": 37.4566,
      "step": 329
    },
    {
      "epoch": 0.17478813559322035,
      "grad_norm": 14.281198501586914,
      "learning_rate": 8.252118644067796e-06,
      "loss": 39.0694,
      "step": 330
    },
    {
      "epoch": 0.1753177966101695,
      "grad_norm": 14.440709114074707,
      "learning_rate": 8.246822033898307e-06,
      "loss": 39.3345,
      "step": 331
    },
    {
      "epoch": 0.17584745762711865,
      "grad_norm": 13.759039878845215,
      "learning_rate": 8.241525423728815e-06,
      "loss": 38.3043,
      "step": 332
    },
    {
      "epoch": 0.1763771186440678,
      "grad_norm": 17.43242835998535,
      "learning_rate": 8.236228813559322e-06,
      "loss": 38.4436,
      "step": 333
    },
    {
      "epoch": 0.17690677966101695,
      "grad_norm": 13.69066333770752,
      "learning_rate": 8.230932203389831e-06,
      "loss": 38.5703,
      "step": 334
    },
    {
      "epoch": 0.1774364406779661,
      "grad_norm": 16.85468101501465,
      "learning_rate": 8.22563559322034e-06,
      "loss": 39.8044,
      "step": 335
    },
    {
      "epoch": 0.17796610169491525,
      "grad_norm": 16.32362174987793,
      "learning_rate": 8.220338983050849e-06,
      "loss": 40.2409,
      "step": 336
    },
    {
      "epoch": 0.1784957627118644,
      "grad_norm": 18.045198440551758,
      "learning_rate": 8.215042372881357e-06,
      "loss": 39.6905,
      "step": 337
    },
    {
      "epoch": 0.17902542372881355,
      "grad_norm": 16.43853759765625,
      "learning_rate": 8.209745762711864e-06,
      "loss": 41.5248,
      "step": 338
    },
    {
      "epoch": 0.1795550847457627,
      "grad_norm": 17.157817840576172,
      "learning_rate": 8.204449152542373e-06,
      "loss": 37.9985,
      "step": 339
    },
    {
      "epoch": 0.18008474576271186,
      "grad_norm": 15.721170425415039,
      "learning_rate": 8.199152542372882e-06,
      "loss": 37.3005,
      "step": 340
    },
    {
      "epoch": 0.180614406779661,
      "grad_norm": 16.326520919799805,
      "learning_rate": 8.19385593220339e-06,
      "loss": 38.3742,
      "step": 341
    },
    {
      "epoch": 0.18114406779661016,
      "grad_norm": 17.589359283447266,
      "learning_rate": 8.1885593220339e-06,
      "loss": 39.0762,
      "step": 342
    },
    {
      "epoch": 0.1816737288135593,
      "grad_norm": 14.245378494262695,
      "learning_rate": 8.183262711864406e-06,
      "loss": 38.5974,
      "step": 343
    },
    {
      "epoch": 0.18220338983050846,
      "grad_norm": 20.359603881835938,
      "learning_rate": 8.177966101694917e-06,
      "loss": 38.755,
      "step": 344
    },
    {
      "epoch": 0.18273305084745764,
      "grad_norm": 15.341033935546875,
      "learning_rate": 8.172669491525424e-06,
      "loss": 38.2094,
      "step": 345
    },
    {
      "epoch": 0.1832627118644068,
      "grad_norm": 10.67183780670166,
      "learning_rate": 8.167372881355933e-06,
      "loss": 36.6774,
      "step": 346
    },
    {
      "epoch": 0.18379237288135594,
      "grad_norm": 16.99692153930664,
      "learning_rate": 8.162076271186441e-06,
      "loss": 38.24,
      "step": 347
    },
    {
      "epoch": 0.1843220338983051,
      "grad_norm": 13.437159538269043,
      "learning_rate": 8.15677966101695e-06,
      "loss": 38.2525,
      "step": 348
    },
    {
      "epoch": 0.18485169491525424,
      "grad_norm": 14.487502098083496,
      "learning_rate": 8.151483050847459e-06,
      "loss": 38.2376,
      "step": 349
    },
    {
      "epoch": 0.1853813559322034,
      "grad_norm": 17.539766311645508,
      "learning_rate": 8.146186440677966e-06,
      "loss": 41.1735,
      "step": 350
    },
    {
      "epoch": 0.18591101694915255,
      "grad_norm": 14.253143310546875,
      "learning_rate": 8.140889830508475e-06,
      "loss": 38.3221,
      "step": 351
    },
    {
      "epoch": 0.1864406779661017,
      "grad_norm": 14.686485290527344,
      "learning_rate": 8.135593220338983e-06,
      "loss": 37.7646,
      "step": 352
    },
    {
      "epoch": 0.18697033898305085,
      "grad_norm": 14.070266723632812,
      "learning_rate": 8.130296610169492e-06,
      "loss": 36.7814,
      "step": 353
    },
    {
      "epoch": 0.1875,
      "grad_norm": 14.59591293334961,
      "learning_rate": 8.125000000000001e-06,
      "loss": 37.3094,
      "step": 354
    },
    {
      "epoch": 0.18802966101694915,
      "grad_norm": 16.729726791381836,
      "learning_rate": 8.119703389830508e-06,
      "loss": 37.6762,
      "step": 355
    },
    {
      "epoch": 0.1885593220338983,
      "grad_norm": 14.66527271270752,
      "learning_rate": 8.114406779661017e-06,
      "loss": 37.127,
      "step": 356
    },
    {
      "epoch": 0.18908898305084745,
      "grad_norm": 14.61306095123291,
      "learning_rate": 8.109110169491527e-06,
      "loss": 38.8526,
      "step": 357
    },
    {
      "epoch": 0.1896186440677966,
      "grad_norm": 15.703363418579102,
      "learning_rate": 8.103813559322034e-06,
      "loss": 38.5697,
      "step": 358
    },
    {
      "epoch": 0.19014830508474576,
      "grad_norm": 15.312939643859863,
      "learning_rate": 8.098516949152543e-06,
      "loss": 37.8277,
      "step": 359
    },
    {
      "epoch": 0.1906779661016949,
      "grad_norm": 13.705245971679688,
      "learning_rate": 8.093220338983052e-06,
      "loss": 37.7011,
      "step": 360
    },
    {
      "epoch": 0.19120762711864406,
      "grad_norm": 17.470590591430664,
      "learning_rate": 8.08792372881356e-06,
      "loss": 37.789,
      "step": 361
    },
    {
      "epoch": 0.1917372881355932,
      "grad_norm": 16.023317337036133,
      "learning_rate": 8.08262711864407e-06,
      "loss": 38.0558,
      "step": 362
    },
    {
      "epoch": 0.19226694915254236,
      "grad_norm": 15.067134857177734,
      "learning_rate": 8.077330508474576e-06,
      "loss": 38.6135,
      "step": 363
    },
    {
      "epoch": 0.19279661016949154,
      "grad_norm": 16.421798706054688,
      "learning_rate": 8.072033898305085e-06,
      "loss": 39.6641,
      "step": 364
    },
    {
      "epoch": 0.1933262711864407,
      "grad_norm": 13.289566993713379,
      "learning_rate": 8.066737288135594e-06,
      "loss": 36.7563,
      "step": 365
    },
    {
      "epoch": 0.19385593220338984,
      "grad_norm": 12.283499717712402,
      "learning_rate": 8.061440677966103e-06,
      "loss": 36.4315,
      "step": 366
    },
    {
      "epoch": 0.194385593220339,
      "grad_norm": 14.71926212310791,
      "learning_rate": 8.056144067796611e-06,
      "loss": 38.8644,
      "step": 367
    },
    {
      "epoch": 0.19491525423728814,
      "grad_norm": 13.94066047668457,
      "learning_rate": 8.050847457627118e-06,
      "loss": 38.6081,
      "step": 368
    },
    {
      "epoch": 0.1954449152542373,
      "grad_norm": 16.23077392578125,
      "learning_rate": 8.045550847457629e-06,
      "loss": 39.2155,
      "step": 369
    },
    {
      "epoch": 0.19597457627118645,
      "grad_norm": 12.926231384277344,
      "learning_rate": 8.040254237288136e-06,
      "loss": 36.7572,
      "step": 370
    },
    {
      "epoch": 0.1965042372881356,
      "grad_norm": 15.421478271484375,
      "learning_rate": 8.034957627118645e-06,
      "loss": 36.1574,
      "step": 371
    },
    {
      "epoch": 0.19703389830508475,
      "grad_norm": 16.24416732788086,
      "learning_rate": 8.029661016949153e-06,
      "loss": 37.1773,
      "step": 372
    },
    {
      "epoch": 0.1975635593220339,
      "grad_norm": 12.23160457611084,
      "learning_rate": 8.024364406779662e-06,
      "loss": 36.2246,
      "step": 373
    },
    {
      "epoch": 0.19809322033898305,
      "grad_norm": 12.668546676635742,
      "learning_rate": 8.019067796610171e-06,
      "loss": 35.3108,
      "step": 374
    },
    {
      "epoch": 0.1986228813559322,
      "grad_norm": 16.283843994140625,
      "learning_rate": 8.013771186440678e-06,
      "loss": 36.7857,
      "step": 375
    },
    {
      "epoch": 0.19915254237288135,
      "grad_norm": 15.66490650177002,
      "learning_rate": 8.008474576271187e-06,
      "loss": 39.2099,
      "step": 376
    },
    {
      "epoch": 0.1996822033898305,
      "grad_norm": 15.377089500427246,
      "learning_rate": 8.003177966101695e-06,
      "loss": 37.9667,
      "step": 377
    },
    {
      "epoch": 0.20021186440677965,
      "grad_norm": 13.770482063293457,
      "learning_rate": 7.997881355932204e-06,
      "loss": 37.0291,
      "step": 378
    },
    {
      "epoch": 0.2007415254237288,
      "grad_norm": 13.117777824401855,
      "learning_rate": 7.992584745762713e-06,
      "loss": 37.7153,
      "step": 379
    },
    {
      "epoch": 0.20127118644067796,
      "grad_norm": 13.280060768127441,
      "learning_rate": 7.987288135593222e-06,
      "loss": 38.2426,
      "step": 380
    },
    {
      "epoch": 0.2018008474576271,
      "grad_norm": 14.14598274230957,
      "learning_rate": 7.981991525423729e-06,
      "loss": 37.8936,
      "step": 381
    },
    {
      "epoch": 0.20233050847457626,
      "grad_norm": 14.772379875183105,
      "learning_rate": 7.976694915254239e-06,
      "loss": 38.5434,
      "step": 382
    },
    {
      "epoch": 0.2028601694915254,
      "grad_norm": 14.686677932739258,
      "learning_rate": 7.971398305084746e-06,
      "loss": 37.4939,
      "step": 383
    },
    {
      "epoch": 0.2033898305084746,
      "grad_norm": 14.272186279296875,
      "learning_rate": 7.966101694915255e-06,
      "loss": 35.7427,
      "step": 384
    },
    {
      "epoch": 0.20391949152542374,
      "grad_norm": 18.501901626586914,
      "learning_rate": 7.960805084745764e-06,
      "loss": 37.4882,
      "step": 385
    },
    {
      "epoch": 0.2044491525423729,
      "grad_norm": 20.163265228271484,
      "learning_rate": 7.955508474576272e-06,
      "loss": 37.8782,
      "step": 386
    },
    {
      "epoch": 0.20497881355932204,
      "grad_norm": 15.181340217590332,
      "learning_rate": 7.950211864406781e-06,
      "loss": 37.7643,
      "step": 387
    },
    {
      "epoch": 0.2055084745762712,
      "grad_norm": 14.075495719909668,
      "learning_rate": 7.944915254237288e-06,
      "loss": 37.6469,
      "step": 388
    },
    {
      "epoch": 0.20603813559322035,
      "grad_norm": 14.970874786376953,
      "learning_rate": 7.939618644067797e-06,
      "loss": 37.969,
      "step": 389
    },
    {
      "epoch": 0.2065677966101695,
      "grad_norm": 12.968758583068848,
      "learning_rate": 7.934322033898306e-06,
      "loss": 35.7424,
      "step": 390
    },
    {
      "epoch": 0.20709745762711865,
      "grad_norm": 16.765079498291016,
      "learning_rate": 7.929025423728814e-06,
      "loss": 37.9629,
      "step": 391
    },
    {
      "epoch": 0.2076271186440678,
      "grad_norm": 14.527029037475586,
      "learning_rate": 7.923728813559323e-06,
      "loss": 36.7617,
      "step": 392
    },
    {
      "epoch": 0.20815677966101695,
      "grad_norm": 14.467700004577637,
      "learning_rate": 7.91843220338983e-06,
      "loss": 36.5531,
      "step": 393
    },
    {
      "epoch": 0.2086864406779661,
      "grad_norm": 14.621841430664062,
      "learning_rate": 7.913135593220339e-06,
      "loss": 38.1907,
      "step": 394
    },
    {
      "epoch": 0.20921610169491525,
      "grad_norm": 14.844414710998535,
      "learning_rate": 7.907838983050848e-06,
      "loss": 36.7918,
      "step": 395
    },
    {
      "epoch": 0.2097457627118644,
      "grad_norm": 12.957280158996582,
      "learning_rate": 7.902542372881357e-06,
      "loss": 36.5903,
      "step": 396
    },
    {
      "epoch": 0.21027542372881355,
      "grad_norm": 13.241612434387207,
      "learning_rate": 7.897245762711865e-06,
      "loss": 36.93,
      "step": 397
    },
    {
      "epoch": 0.2108050847457627,
      "grad_norm": 12.306845664978027,
      "learning_rate": 7.891949152542372e-06,
      "loss": 36.174,
      "step": 398
    },
    {
      "epoch": 0.21133474576271186,
      "grad_norm": 14.577982902526855,
      "learning_rate": 7.886652542372883e-06,
      "loss": 36.7113,
      "step": 399
    },
    {
      "epoch": 0.211864406779661,
      "grad_norm": 13.36111068725586,
      "learning_rate": 7.88135593220339e-06,
      "loss": 36.325,
      "step": 400
    },
    {
      "epoch": 0.21239406779661016,
      "grad_norm": 16.200069427490234,
      "learning_rate": 7.876059322033899e-06,
      "loss": 37.1834,
      "step": 401
    },
    {
      "epoch": 0.2129237288135593,
      "grad_norm": 13.555863380432129,
      "learning_rate": 7.870762711864407e-06,
      "loss": 36.2399,
      "step": 402
    },
    {
      "epoch": 0.21345338983050846,
      "grad_norm": 14.421463012695312,
      "learning_rate": 7.865466101694916e-06,
      "loss": 36.285,
      "step": 403
    },
    {
      "epoch": 0.21398305084745764,
      "grad_norm": 16.564464569091797,
      "learning_rate": 7.860169491525425e-06,
      "loss": 37.0492,
      "step": 404
    },
    {
      "epoch": 0.2145127118644068,
      "grad_norm": 17.40530776977539,
      "learning_rate": 7.854872881355934e-06,
      "loss": 38.223,
      "step": 405
    },
    {
      "epoch": 0.21504237288135594,
      "grad_norm": 17.59159278869629,
      "learning_rate": 7.84957627118644e-06,
      "loss": 37.7988,
      "step": 406
    },
    {
      "epoch": 0.2155720338983051,
      "grad_norm": 14.481407165527344,
      "learning_rate": 7.844279661016951e-06,
      "loss": 37.2099,
      "step": 407
    },
    {
      "epoch": 0.21610169491525424,
      "grad_norm": 14.543986320495605,
      "learning_rate": 7.838983050847458e-06,
      "loss": 36.6947,
      "step": 408
    },
    {
      "epoch": 0.2166313559322034,
      "grad_norm": 15.12159252166748,
      "learning_rate": 7.833686440677967e-06,
      "loss": 34.9774,
      "step": 409
    },
    {
      "epoch": 0.21716101694915255,
      "grad_norm": 15.058058738708496,
      "learning_rate": 7.828389830508476e-06,
      "loss": 37.9512,
      "step": 410
    },
    {
      "epoch": 0.2176906779661017,
      "grad_norm": 12.692740440368652,
      "learning_rate": 7.823093220338984e-06,
      "loss": 37.7849,
      "step": 411
    },
    {
      "epoch": 0.21822033898305085,
      "grad_norm": 14.874739646911621,
      "learning_rate": 7.817796610169493e-06,
      "loss": 36.276,
      "step": 412
    },
    {
      "epoch": 0.21875,
      "grad_norm": 10.866676330566406,
      "learning_rate": 7.8125e-06,
      "loss": 35.9967,
      "step": 413
    },
    {
      "epoch": 0.21927966101694915,
      "grad_norm": 13.37695026397705,
      "learning_rate": 7.807203389830509e-06,
      "loss": 37.7138,
      "step": 414
    },
    {
      "epoch": 0.2198093220338983,
      "grad_norm": 18.56627082824707,
      "learning_rate": 7.801906779661018e-06,
      "loss": 37.5726,
      "step": 415
    },
    {
      "epoch": 0.22033898305084745,
      "grad_norm": 14.760334968566895,
      "learning_rate": 7.796610169491526e-06,
      "loss": 38.6441,
      "step": 416
    },
    {
      "epoch": 0.2208686440677966,
      "grad_norm": 15.370715141296387,
      "learning_rate": 7.791313559322035e-06,
      "loss": 35.7603,
      "step": 417
    },
    {
      "epoch": 0.22139830508474576,
      "grad_norm": 13.887012481689453,
      "learning_rate": 7.786016949152542e-06,
      "loss": 36.0982,
      "step": 418
    },
    {
      "epoch": 0.2219279661016949,
      "grad_norm": 11.730676651000977,
      "learning_rate": 7.780720338983051e-06,
      "loss": 35.7375,
      "step": 419
    },
    {
      "epoch": 0.22245762711864406,
      "grad_norm": 14.909584045410156,
      "learning_rate": 7.77542372881356e-06,
      "loss": 36.8677,
      "step": 420
    },
    {
      "epoch": 0.2229872881355932,
      "grad_norm": 14.385584831237793,
      "learning_rate": 7.770127118644068e-06,
      "loss": 35.6429,
      "step": 421
    },
    {
      "epoch": 0.22351694915254236,
      "grad_norm": 12.89328670501709,
      "learning_rate": 7.764830508474577e-06,
      "loss": 35.0705,
      "step": 422
    },
    {
      "epoch": 0.22404661016949154,
      "grad_norm": 17.612462997436523,
      "learning_rate": 7.759533898305084e-06,
      "loss": 36.4236,
      "step": 423
    },
    {
      "epoch": 0.2245762711864407,
      "grad_norm": 16.163755416870117,
      "learning_rate": 7.754237288135595e-06,
      "loss": 36.9028,
      "step": 424
    },
    {
      "epoch": 0.22510593220338984,
      "grad_norm": 14.078839302062988,
      "learning_rate": 7.748940677966102e-06,
      "loss": 38.0464,
      "step": 425
    },
    {
      "epoch": 0.225635593220339,
      "grad_norm": 12.508034706115723,
      "learning_rate": 7.74364406779661e-06,
      "loss": 35.1789,
      "step": 426
    },
    {
      "epoch": 0.22616525423728814,
      "grad_norm": 14.0387601852417,
      "learning_rate": 7.73834745762712e-06,
      "loss": 36.6567,
      "step": 427
    },
    {
      "epoch": 0.2266949152542373,
      "grad_norm": 12.07306957244873,
      "learning_rate": 7.733050847457628e-06,
      "loss": 35.1888,
      "step": 428
    },
    {
      "epoch": 0.22722457627118645,
      "grad_norm": 12.650291442871094,
      "learning_rate": 7.727754237288137e-06,
      "loss": 34.5795,
      "step": 429
    },
    {
      "epoch": 0.2277542372881356,
      "grad_norm": 15.604769706726074,
      "learning_rate": 7.722457627118645e-06,
      "loss": 37.2186,
      "step": 430
    },
    {
      "epoch": 0.22828389830508475,
      "grad_norm": 13.434661865234375,
      "learning_rate": 7.717161016949153e-06,
      "loss": 36.3673,
      "step": 431
    },
    {
      "epoch": 0.2288135593220339,
      "grad_norm": 14.793424606323242,
      "learning_rate": 7.711864406779663e-06,
      "loss": 37.0799,
      "step": 432
    },
    {
      "epoch": 0.22934322033898305,
      "grad_norm": 11.482912063598633,
      "learning_rate": 7.70656779661017e-06,
      "loss": 35.5861,
      "step": 433
    },
    {
      "epoch": 0.2298728813559322,
      "grad_norm": 14.342568397521973,
      "learning_rate": 7.701271186440679e-06,
      "loss": 35.5337,
      "step": 434
    },
    {
      "epoch": 0.23040254237288135,
      "grad_norm": 15.650469779968262,
      "learning_rate": 7.695974576271188e-06,
      "loss": 36.8128,
      "step": 435
    },
    {
      "epoch": 0.2309322033898305,
      "grad_norm": 15.991424560546875,
      "learning_rate": 7.690677966101695e-06,
      "loss": 37.0812,
      "step": 436
    },
    {
      "epoch": 0.23146186440677965,
      "grad_norm": 13.7311429977417,
      "learning_rate": 7.685381355932205e-06,
      "loss": 36.3043,
      "step": 437
    },
    {
      "epoch": 0.2319915254237288,
      "grad_norm": 13.111760139465332,
      "learning_rate": 7.680084745762712e-06,
      "loss": 35.8702,
      "step": 438
    },
    {
      "epoch": 0.23252118644067796,
      "grad_norm": 12.667045593261719,
      "learning_rate": 7.67478813559322e-06,
      "loss": 35.839,
      "step": 439
    },
    {
      "epoch": 0.2330508474576271,
      "grad_norm": 13.852143287658691,
      "learning_rate": 7.66949152542373e-06,
      "loss": 35.4605,
      "step": 440
    },
    {
      "epoch": 0.23358050847457626,
      "grad_norm": 11.194706916809082,
      "learning_rate": 7.664194915254238e-06,
      "loss": 35.0994,
      "step": 441
    },
    {
      "epoch": 0.2341101694915254,
      "grad_norm": 14.387446403503418,
      "learning_rate": 7.658898305084747e-06,
      "loss": 37.049,
      "step": 442
    },
    {
      "epoch": 0.2346398305084746,
      "grad_norm": 12.108460426330566,
      "learning_rate": 7.653601694915254e-06,
      "loss": 35.222,
      "step": 443
    },
    {
      "epoch": 0.23516949152542374,
      "grad_norm": 13.49776554107666,
      "learning_rate": 7.648305084745763e-06,
      "loss": 37.117,
      "step": 444
    },
    {
      "epoch": 0.2356991525423729,
      "grad_norm": 12.823853492736816,
      "learning_rate": 7.643008474576272e-06,
      "loss": 35.5313,
      "step": 445
    },
    {
      "epoch": 0.23622881355932204,
      "grad_norm": 12.872989654541016,
      "learning_rate": 7.63771186440678e-06,
      "loss": 36.6576,
      "step": 446
    },
    {
      "epoch": 0.2367584745762712,
      "grad_norm": 14.687725067138672,
      "learning_rate": 7.632415254237289e-06,
      "loss": 35.9383,
      "step": 447
    },
    {
      "epoch": 0.23728813559322035,
      "grad_norm": 13.950675010681152,
      "learning_rate": 7.627118644067797e-06,
      "loss": 35.8499,
      "step": 448
    },
    {
      "epoch": 0.2378177966101695,
      "grad_norm": 12.251441955566406,
      "learning_rate": 7.621822033898307e-06,
      "loss": 35.1809,
      "step": 449
    },
    {
      "epoch": 0.23834745762711865,
      "grad_norm": 10.617510795593262,
      "learning_rate": 7.6165254237288145e-06,
      "loss": 34.5212,
      "step": 450
    },
    {
      "epoch": 0.2388771186440678,
      "grad_norm": 12.790823936462402,
      "learning_rate": 7.611228813559322e-06,
      "loss": 35.9684,
      "step": 451
    },
    {
      "epoch": 0.23940677966101695,
      "grad_norm": 14.022799491882324,
      "learning_rate": 7.605932203389831e-06,
      "loss": 35.5855,
      "step": 452
    },
    {
      "epoch": 0.2399364406779661,
      "grad_norm": 16.07508087158203,
      "learning_rate": 7.600635593220339e-06,
      "loss": 37.2118,
      "step": 453
    },
    {
      "epoch": 0.24046610169491525,
      "grad_norm": 15.487733840942383,
      "learning_rate": 7.595338983050849e-06,
      "loss": 36.4803,
      "step": 454
    },
    {
      "epoch": 0.2409957627118644,
      "grad_norm": 12.216702461242676,
      "learning_rate": 7.5900423728813566e-06,
      "loss": 35.4833,
      "step": 455
    },
    {
      "epoch": 0.24152542372881355,
      "grad_norm": 13.599502563476562,
      "learning_rate": 7.5847457627118645e-06,
      "loss": 36.6042,
      "step": 456
    },
    {
      "epoch": 0.2420550847457627,
      "grad_norm": 13.38609504699707,
      "learning_rate": 7.579449152542373e-06,
      "loss": 35.3173,
      "step": 457
    },
    {
      "epoch": 0.24258474576271186,
      "grad_norm": 15.103922843933105,
      "learning_rate": 7.574152542372882e-06,
      "loss": 35.8786,
      "step": 458
    },
    {
      "epoch": 0.243114406779661,
      "grad_norm": 13.98809814453125,
      "learning_rate": 7.568855932203391e-06,
      "loss": 35.12,
      "step": 459
    },
    {
      "epoch": 0.24364406779661016,
      "grad_norm": 12.736790657043457,
      "learning_rate": 7.563559322033899e-06,
      "loss": 36.1708,
      "step": 460
    },
    {
      "epoch": 0.2441737288135593,
      "grad_norm": 13.747405052185059,
      "learning_rate": 7.558262711864407e-06,
      "loss": 36.5864,
      "step": 461
    },
    {
      "epoch": 0.24470338983050846,
      "grad_norm": 13.04160213470459,
      "learning_rate": 7.552966101694916e-06,
      "loss": 35.0197,
      "step": 462
    },
    {
      "epoch": 0.24523305084745764,
      "grad_norm": 12.303763389587402,
      "learning_rate": 7.547669491525425e-06,
      "loss": 35.2411,
      "step": 463
    },
    {
      "epoch": 0.2457627118644068,
      "grad_norm": 19.88754653930664,
      "learning_rate": 7.542372881355933e-06,
      "loss": 37.8196,
      "step": 464
    },
    {
      "epoch": 0.24629237288135594,
      "grad_norm": 12.700716972351074,
      "learning_rate": 7.537076271186441e-06,
      "loss": 34.8035,
      "step": 465
    },
    {
      "epoch": 0.2468220338983051,
      "grad_norm": 13.4606351852417,
      "learning_rate": 7.53177966101695e-06,
      "loss": 35.1558,
      "step": 466
    },
    {
      "epoch": 0.24735169491525424,
      "grad_norm": 13.608341217041016,
      "learning_rate": 7.526483050847458e-06,
      "loss": 35.5202,
      "step": 467
    },
    {
      "epoch": 0.2478813559322034,
      "grad_norm": 12.665416717529297,
      "learning_rate": 7.521186440677967e-06,
      "loss": 35.8215,
      "step": 468
    },
    {
      "epoch": 0.24841101694915255,
      "grad_norm": 11.7763090133667,
      "learning_rate": 7.515889830508475e-06,
      "loss": 34.6889,
      "step": 469
    },
    {
      "epoch": 0.2489406779661017,
      "grad_norm": 15.366496086120605,
      "learning_rate": 7.510593220338984e-06,
      "loss": 36.5941,
      "step": 470
    },
    {
      "epoch": 0.24947033898305085,
      "grad_norm": 17.286909103393555,
      "learning_rate": 7.505296610169492e-06,
      "loss": 37.0892,
      "step": 471
    },
    {
      "epoch": 0.25,
      "grad_norm": 13.034377098083496,
      "learning_rate": 7.500000000000001e-06,
      "loss": 35.7045,
      "step": 472
    },
    {
      "epoch": 0.2505296610169492,
      "grad_norm": 14.313055038452148,
      "learning_rate": 7.494703389830509e-06,
      "loss": 35.7783,
      "step": 473
    },
    {
      "epoch": 0.2510593220338983,
      "grad_norm": 16.570783615112305,
      "learning_rate": 7.489406779661017e-06,
      "loss": 36.2294,
      "step": 474
    },
    {
      "epoch": 0.2515889830508475,
      "grad_norm": 11.87003231048584,
      "learning_rate": 7.4841101694915264e-06,
      "loss": 34.5185,
      "step": 475
    },
    {
      "epoch": 0.2521186440677966,
      "grad_norm": 11.322108268737793,
      "learning_rate": 7.478813559322034e-06,
      "loss": 35.6784,
      "step": 476
    },
    {
      "epoch": 0.2526483050847458,
      "grad_norm": 16.87864875793457,
      "learning_rate": 7.473516949152543e-06,
      "loss": 35.6567,
      "step": 477
    },
    {
      "epoch": 0.2531779661016949,
      "grad_norm": 12.462778091430664,
      "learning_rate": 7.468220338983051e-06,
      "loss": 34.4818,
      "step": 478
    },
    {
      "epoch": 0.2537076271186441,
      "grad_norm": 12.841659545898438,
      "learning_rate": 7.462923728813561e-06,
      "loss": 35.6833,
      "step": 479
    },
    {
      "epoch": 0.2542372881355932,
      "grad_norm": 14.566444396972656,
      "learning_rate": 7.4576271186440685e-06,
      "loss": 35.9223,
      "step": 480
    },
    {
      "epoch": 0.2547669491525424,
      "grad_norm": 15.032759666442871,
      "learning_rate": 7.452330508474576e-06,
      "loss": 35.3623,
      "step": 481
    },
    {
      "epoch": 0.2552966101694915,
      "grad_norm": 13.192596435546875,
      "learning_rate": 7.447033898305085e-06,
      "loss": 34.911,
      "step": 482
    },
    {
      "epoch": 0.2558262711864407,
      "grad_norm": 14.381868362426758,
      "learning_rate": 7.441737288135594e-06,
      "loss": 35.9336,
      "step": 483
    },
    {
      "epoch": 0.2563559322033898,
      "grad_norm": 12.850976943969727,
      "learning_rate": 7.436440677966103e-06,
      "loss": 35.6824,
      "step": 484
    },
    {
      "epoch": 0.256885593220339,
      "grad_norm": 12.497685432434082,
      "learning_rate": 7.4311440677966105e-06,
      "loss": 36.343,
      "step": 485
    },
    {
      "epoch": 0.2574152542372881,
      "grad_norm": 13.908935546875,
      "learning_rate": 7.425847457627119e-06,
      "loss": 35.4696,
      "step": 486
    },
    {
      "epoch": 0.2579449152542373,
      "grad_norm": 13.70351505279541,
      "learning_rate": 7.420550847457628e-06,
      "loss": 35.3874,
      "step": 487
    },
    {
      "epoch": 0.2584745762711864,
      "grad_norm": 13.918442726135254,
      "learning_rate": 7.415254237288137e-06,
      "loss": 35.1164,
      "step": 488
    },
    {
      "epoch": 0.2590042372881356,
      "grad_norm": 12.070303916931152,
      "learning_rate": 7.409957627118645e-06,
      "loss": 34.3897,
      "step": 489
    },
    {
      "epoch": 0.2595338983050847,
      "grad_norm": 11.308910369873047,
      "learning_rate": 7.4046610169491526e-06,
      "loss": 34.2978,
      "step": 490
    },
    {
      "epoch": 0.2600635593220339,
      "grad_norm": 13.477808952331543,
      "learning_rate": 7.399364406779662e-06,
      "loss": 36.438,
      "step": 491
    },
    {
      "epoch": 0.2605932203389831,
      "grad_norm": 16.292407989501953,
      "learning_rate": 7.39406779661017e-06,
      "loss": 35.2849,
      "step": 492
    },
    {
      "epoch": 0.2611228813559322,
      "grad_norm": 13.881239891052246,
      "learning_rate": 7.388771186440679e-06,
      "loss": 34.786,
      "step": 493
    },
    {
      "epoch": 0.2616525423728814,
      "grad_norm": 12.140098571777344,
      "learning_rate": 7.383474576271187e-06,
      "loss": 34.6671,
      "step": 494
    },
    {
      "epoch": 0.2621822033898305,
      "grad_norm": 11.392627716064453,
      "learning_rate": 7.378177966101695e-06,
      "loss": 34.2748,
      "step": 495
    },
    {
      "epoch": 0.2627118644067797,
      "grad_norm": 11.277288436889648,
      "learning_rate": 7.372881355932204e-06,
      "loss": 34.555,
      "step": 496
    },
    {
      "epoch": 0.2632415254237288,
      "grad_norm": 59.96910095214844,
      "learning_rate": 7.367584745762713e-06,
      "loss": 33.764,
      "step": 497
    },
    {
      "epoch": 0.263771186440678,
      "grad_norm": 12.529248237609863,
      "learning_rate": 7.362288135593221e-06,
      "loss": 33.9436,
      "step": 498
    },
    {
      "epoch": 0.2643008474576271,
      "grad_norm": 10.365516662597656,
      "learning_rate": 7.356991525423729e-06,
      "loss": 33.1056,
      "step": 499
    },
    {
      "epoch": 0.2648305084745763,
      "grad_norm": 12.834813117980957,
      "learning_rate": 7.351694915254238e-06,
      "loss": 35.5655,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 1888,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1374950522880000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
